# -*- coding: utf-8 -*-
# emotion_reward_sft.py

import torch
import logging
import os
import sys
import json
import threading
import time
import psutil
from datetime import datetime
from tqdm import tqdm

# ãƒªãƒ¢ãƒ¼ãƒˆæ¥ç¶šã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '3600'  # 1æ™‚é–“
os.environ['REQUESTS_TIMEOUT'] = '3600'  # 1æ™‚é–“
os.environ['HF_HUB_OFFLINE'] = '0'  # ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰ã‚’ç„¡åŠ¹åŒ–
from datasets import Dataset, load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding,
    pipeline
)
# from dataclasses import dataclass  # é€šå¸¸ã®ã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
try:
    from typing import Any, Dict, List, Union
except ImportError:
    # Python 3.5æœªæº€ã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    Any = object
    Dict = dict
    List = list
    Union = object
from peft import LoraConfig, get_peft_model
import torch.nn as nn
import torch.nn.functional as F

# --- ä»–ã®è‡ªä½œãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
from data_processing import load_and_split_dataset, EVALUATION_ITEMS
from turn_segmentation import create_turn_list
# from llm_evaluation import evaluate_conversation_on_items  # å®Ÿéš›ã®æ­£è§£ãƒ©ãƒ™ãƒ«ä½¿ç”¨ã®ãŸã‚ä¸è¦

# --- ãƒ­ã‚°è¨­å®š ---
def setup_logging(log_dir="./logs_sft"):
    os.makedirs(log_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, "sft_training_{}.log".format(timestamp))

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[logging.FileHandler(log_file, encoding='utf-8'), logging.StreamHandler(sys.stdout)]
    )
    logger = logging.getLogger(__name__)
    logger.info("ãƒ­ã‚°ã¯ {} ã«è¨˜éŒ²ã•ã‚Œã¾ã™ã€‚".format(log_file))
    return logger

logger = setup_logging()

# --- å®Ÿé¨“ç®¡ç†ãƒ„ãƒ¼ãƒ« ---
try:
    from experiment_tracker import ExperimentTracker, create_experiment_tracker
    from experiment_config import ExperimentConfig
    EXPERIMENT_TRACKING_AVAILABLE = True
except ImportError as e:
    logger.warning("å®Ÿé¨“ç®¡ç†ãƒ„ãƒ¼ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {}".format(e))
    EXPERIMENT_TRACKING_AVAILABLE = False

# --- ãƒ¡ãƒ¢ãƒªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ  ---
class MemoryGuard:
    def __init__(self, threshold=90, check_interval=30):
        self.threshold = threshold
        self.check_interval = check_interval
        self.monitoring = False
        self.monitor_thread = None
        self.log_file = "memory_usage_{}.csv".format(datetime.now().strftime("%Y%m%d_%H%M%S"))
        self._init_log_file()
    
    def _init_log_file(self):
        """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆæœŸåŒ–"""
        try:
            with open(self.log_file, 'w') as f:
                f.write("timestamp,memory_usage_percent,memory_used_gb,memory_available_gb,swap_usage_percent\n")
            logger.info("ğŸ“Š ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: {}".format(self.log_file))
        except Exception as e:
            logger.warning("ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {}".format(e))
    
    def _log_usage(self, memory_percent, memory_used_gb, memory_available_gb, swap_percent):
        """ä½¿ç”¨é‡ã‚’ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²"""
        try:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            with open(self.log_file, 'a') as f:
                f.write("{},{:.1f},{:.1f},{:.1f},{:.1f}\n".format(
                    timestamp, memory_percent, memory_used_gb, memory_available_gb, swap_percent))
        except Exception as e:
            logger.warning("ãƒ­ã‚°è¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {}".format(e))
        
    def get_memory_usage(self):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã‚’å–å¾—"""
        try:
            memory = psutil.virtual_memory()
            swap = psutil.swap_memory()
            
            # ãƒ­ã‚°ã«è¨˜éŒ²
            self._log_usage(
                memory.percent,
                memory.used / (1024**3),  # GB
                memory.available / (1024**3),  # GB
                swap.percent
            )
            
            return memory.percent
        except Exception as e:
            logger.warning("ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡å–å¾—ã‚¨ãƒ©ãƒ¼: {}".format(e))
            return 0
    
    def emergency_stop(self):
        """ç·Šæ€¥åœæ­¢å‡¦ç†"""
        logger.error("ğŸš¨ ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡{}%è¶…éï¼ç·Šæ€¥åœæ­¢ã‚’å®Ÿè¡Œã—ã¾ã™".format(self.threshold))
        
        # ç¾åœ¨ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’åœæ­¢
        current_process = psutil.Process()
        logger.error("ğŸ›‘ ãƒ—ãƒ­ã‚»ã‚¹åœæ­¢: PID {}".format(current_process.pid))
        
        # å¼·åˆ¶çµ‚äº†
        os._exit(1)
    
    def monitor_memory(self):
        """ãƒ¡ãƒ¢ãƒªç›£è¦–ãƒ«ãƒ¼ãƒ—"""
        logger.info("ğŸ›¡ï¸ ãƒ¡ãƒ¢ãƒªç›£è¦–é–‹å§‹ï¼ˆé–¾å€¤: {}%ï¼‰".format(self.threshold))
        
        while self.monitoring:
            try:
                usage = self.get_memory_usage()
                
                if usage >= self.threshold:
                    self.emergency_stop()
                elif usage >= self.threshold - 5:
                    logger.warning("âš ï¸ ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡è­¦å‘Š: {:.1f}%".format(usage))
                elif usage >= self.threshold - 10:
                    logger.info("ğŸ“Š ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡æ³¨æ„: {:.1f}%".format(usage))
                
                time.sleep(self.check_interval)
                
            except Exception as e:
                logger.error("ãƒ¡ãƒ¢ãƒªç›£è¦–ã‚¨ãƒ©ãƒ¼: {}".format(e))
                time.sleep(self.check_interval)
    
    def start_monitoring(self):
        """ç›£è¦–é–‹å§‹"""
        if not self.monitoring:
            self.monitoring = True
            self.monitor_thread = threading.Thread(target=self.monitor_memory, daemon=True)
            self.monitor_thread.start()
            logger.info("âœ… ãƒ¡ãƒ¢ãƒªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    
    def stop_monitoring(self):
        """ç›£è¦–åœæ­¢"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5)
        logger.info("ğŸ”š ãƒ¡ãƒ¢ãƒªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ åœæ­¢")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒªã‚¬ãƒ¼ãƒ‰
memory_guard = MemoryGuard(threshold=90, check_interval=30)

# --- ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ç”¨ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
def load_tokenizer_with_fallback(model_name, force_swallow=False):
    """
    ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’è¤‡æ•°ã®æ–¹æ³•ã§è©¦è¡Œã—ã¦èª­ã¿è¾¼ã‚€
    force_swallow=Trueã®å ´åˆã€Swallowãƒ¢ãƒ‡ãƒ«ä»¥å¤–ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ç„¡åŠ¹åŒ–
    """
    if force_swallow and "Swallow" not in model_name:
        raise ValueError("Swallowãƒ¢ãƒ‡ãƒ«å¼·åˆ¶ä½¿ç”¨ãƒ¢ãƒ¼ãƒ‰ã§ã¯ã€Swallowãƒ¢ãƒ‡ãƒ«ä»¥å¤–ã¯ä½¿ç”¨ã§ãã¾ã›ã‚“")
    
    # Swallowãƒ¢ãƒ‡ãƒ«å°‚ç”¨ã®èª­ã¿è¾¼ã¿æ–¹æ³•
    swallow_methods = [
        # æ–¹æ³•1: use_fast=False
        lambda: AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False),
        # æ–¹æ³•2: LlamaTokenizerç›´æ¥æŒ‡å®šï¼ˆSwallowã¯llamaãƒ™ãƒ¼ã‚¹ï¼‰
        lambda: __import__('transformers', fromlist=['LlamaTokenizer']).LlamaTokenizer.from_pretrained(model_name, trust_remote_code=True),
        # æ–¹æ³•3: use_fast=Trueï¼ˆæœ€å¾Œã®æ‰‹æ®µï¼‰
        lambda: AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=True),
    ]
    
    # é€šå¸¸ã®æ–¹æ³•ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰
    normal_methods = [
        # æ–¹æ³•1: use_fast=False
        lambda: AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False),
        # æ–¹æ³•2: LlamaTokenizerç›´æ¥æŒ‡å®š
        lambda: __import__('transformers', fromlist=['LlamaTokenizer']).LlamaTokenizer.from_pretrained(model_name, trust_remote_code=True),
        # æ–¹æ³•3: ä»£æ›¿ãƒ¢ãƒ‡ãƒ«
        lambda: AutoTokenizer.from_pretrained("microsoft/DialoGPT-medium"),
        # æ–¹æ³•4: GPT-2ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        lambda: AutoTokenizer.from_pretrained("gpt2"),
    ]
    
    if force_swallow:
        methods = swallow_methods
        model_names = [model_name, model_name, model_name]
        logger.info("ğŸ¦… Swallowãƒ¢ãƒ‡ãƒ«å¼·åˆ¶ãƒ¢ãƒ¼ãƒ‰: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç„¡åŠ¹")
    else:
        methods = normal_methods
        model_names = [model_name, model_name, "microsoft/DialoGPT-medium", "gpt2"]
    
    for i, (method, fallback_name) in enumerate(zip(methods, model_names)):
        try:
            tokenizer = method()
            logger.info("âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿æˆåŠŸ (æ–¹æ³•{}): {}".format(i+1, fallback_name))
            return tokenizer, fallback_name
        except Exception as e:
            logger.warning("âŒ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å¤±æ•— (æ–¹æ³•{}): {}".format(i+1, e))
            continue
    
    if force_swallow:
        raise RuntimeError("ğŸš¨ Swallowãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ãŒå…¨ã¦å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ¢ãƒ‡ãƒ«ãŒæ­£ã—ããƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    else:
        raise RuntimeError("å…¨ã¦ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿æ–¹æ³•ãŒå¤±æ•—ã—ã¾ã—ãŸ")

def setup_tokenizer_padding(tokenizer):
    """
    ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°è¨­å®š
    """
    if tokenizer.pad_token is None:
        if tokenizer.eos_token is not None:
            tokenizer.pad_token = tokenizer.eos_token
        elif tokenizer.unk_token is not None:
            tokenizer.pad_token = tokenizer.unk_token
        else:
            tokenizer.add_special_tokens({"pad_token": "[PAD]"})
    
    tokenizer.padding_side = "right"
    return tokenizer

# --- å›å¸°ã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ ---
class RegressionDataCollator:
    """
    å›å¸°ã‚¿ã‚¹ã‚¯ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã€‚
    æ•°å€¤ãƒ©ãƒ™ãƒ«ã‚’é©åˆ‡ã«å‡¦ç†ã™ã‚‹ã€‚
    """
    def __init__(self, tokenizer=None, padding=True, max_length=None, 
                 pad_to_multiple_of=None, return_tensors="pt"):
        self.tokenizer = tokenizer
        self.padding = padding
        self.max_length = max_length
        self.pad_to_multiple_of = pad_to_multiple_of
        self.return_tensors = return_tensors

    def __call__(self, features):
        # input_idsã¨attention_maskã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        batch = {}
        
        # input_idsã®å‡¦ç†
        input_ids = [f["input_ids"] for f in features]
        if self.padding:
            max_len = max(len(ids) for ids in input_ids)
            input_ids = [ids + [self.tokenizer.pad_token_id] * (max_len - len(ids)) for ids in input_ids]
        
        # attention_maskã®å‡¦ç†
        attention_mask = [f["attention_mask"] for f in features]
        if self.padding:
            attention_mask = [mask + [0] * (max_len - len(mask)) for mask in attention_mask]
        
        # ãƒ©ãƒ™ãƒ«ï¼ˆæ•°å€¤ï¼‰ã®å‡¦ç†
        labels = [f["labels"] for f in features]
        
        batch["input_ids"] = torch.tensor(input_ids, dtype=torch.long)
        batch["attention_mask"] = torch.tensor(attention_mask, dtype=torch.long)
        batch["labels"] = torch.tensor(labels, dtype=torch.float)
        
        return batch

# --- ã‚«ã‚¹ã‚¿ãƒ Trainerã‚¯ãƒ©ã‚¹ï¼ˆMSEæå¤±ç”¨ï¼‰ ---
class RegressionTrainer(Trainer):
    """
    å›å¸°ã‚¿ã‚¹ã‚¯ç”¨ã®ã‚«ã‚¹ã‚¿ãƒ Trainerã€‚
    0ï½5ç‚¹ã®è©•ä¾¡ã‚¹ã‚³ã‚¢äºˆæ¸¬ã«MSEæå¤±ã‚’ä½¿ç”¨ã€‚
    å®Ÿé¨“ç®¡ç†æ©Ÿèƒ½ä»˜ãã€‚
    """
    
    def __init__(self, experiment_tracker=None, *args, **kwargs):
        super(RegressionTrainer, self).__init__(*args, **kwargs)
        self.experiment_tracker = experiment_tracker
        self.step_count = 0
    
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        """
        MSEæå¤±ã‚’è¨ˆç®—ã™ã‚‹ã€‚
        """
        labels = inputs.pop("labels")
        
        # ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’å–å¾—
        outputs = model(**inputs)
        logits = outputs.logits
        
        # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®éš ã‚ŒçŠ¶æ…‹ã‚’å–å¾—
        # logitsã®å½¢çŠ¶: [batch_size, sequence_length, vocab_size]
        last_hidden_states = logits[:, -1, :]  # [batch_size, vocab_size]
        
        # å›å¸°ç”¨ã®ãƒ˜ãƒƒãƒ‰ã‚’è¿½åŠ ï¼ˆvocab_sizeã‹ã‚‰1æ¬¡å…ƒã¸ï¼‰
        if not hasattr(model, 'regression_head'):
            model.regression_head = nn.Linear(logits.size(-1), 1).to(logits.device)
            # å›å¸°ãƒ˜ãƒƒãƒ‰ã‚’ãƒ¢ãƒ‡ãƒ«ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã—ã¦ç™»éŒ²ï¼ˆä¿å­˜æ™‚ã«å«ã¾ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹ï¼‰
            if hasattr(model, 'add_module'):
                model.add_module('regression_head', model.regression_head)
        
        # å›å¸°äºˆæ¸¬å€¤ã‚’è¨ˆç®—
        predictions = model.regression_head(last_hidden_states).squeeze(-1)  # [batch_size]
        
        # MSEæå¤±ã‚’è¨ˆç®—
        loss = F.mse_loss(predictions, labels.float())
        
        # å®Ÿé¨“ç®¡ç†ãƒ„ãƒ¼ãƒ«ã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²
        if self.experiment_tracker:
            try:
                metrics = {
                    "train_loss": loss.item(),
                    "mse_loss": loss.item(),
                    "step": self.step_count
                }
                self.experiment_tracker.log_metrics(metrics, step=self.step_count)
                self.step_count += 1
            except Exception as e:
                logger.warning("å®Ÿé¨“ç®¡ç†ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {}".format(e))
        
        return (loss, {"logits": predictions}) if return_outputs else loss

# --- ãƒ‡ãƒ¼ã‚¿æº–å‚™ ---
def create_regression_dataset_from_real_labels(original_dataset, max_samples, tokenizer):
    """
    å®Ÿéš›ã®æ­£è§£ãƒ©ãƒ™ãƒ«ã‚’ä½¿ç”¨ã—ã¦å›å¸°ã‚¿ã‚¹ã‚¯ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹ã€‚
    MSEæå¤±ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã€å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã¨æ•°å€¤ãƒ©ãƒ™ãƒ«ã®ãƒšã‚¢ã‚’ä½œæˆã€‚
    """
    regression_data = []

    # å‡¦ç†æ™‚é–“ã‚’çŸ­ç¸®ã™ã‚‹ãŸã‚ã«å¯¾è±¡ä»¶æ•°ã‚’åˆ¶é™
    if len(original_dataset) > max_samples:
        logger.info("{}ä»¶ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦å‡¦ç†ã—ã¾ã™ã€‚".format(max_samples))
        dataset_to_process = original_dataset.shuffle(seed=42).select(range(max_samples))
    else:
        dataset_to_process = original_dataset

    for i, example in enumerate(tqdm(dataset_to_process, desc="å›å¸°ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­ï¼ˆå®Ÿéš›ã®æ­£è§£ãƒ©ãƒ™ãƒ«ä½¿ç”¨ï¼‰")):
        dialogue = example.get('dialogue', [])
        review_jp = example.get('review_by_client_jp', {})
        
        # å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
        if not isinstance(dialogue, list):
            logger.warning("ã‚µãƒ³ãƒ—ãƒ« {}: å¯¾è©±ãƒ‡ãƒ¼ã‚¿ãŒæ­£ã—ã„å½¢å¼ã§ã¯ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—".format(i))
            continue

        # å¯¾è©±ã‚’ã€Œå½¹å‰²: ç™ºè¨€ã€ã®ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã«å¤‰æ›
        try:
            turn_list = create_turn_list(dialogue)
            if not turn_list:
                logger.warning("ã‚µãƒ³ãƒ—ãƒ« {}: turn_listãŒç©ºã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—".format(i))
                continue
            full_conversation_text = "\n".join(turn_list)
            # å¯¾è©±ãƒ†ã‚­ã‚¹ãƒˆã‚’çŸ­ç¸®
            short_conversation = full_conversation_text if len(full_conversation_text) <= 800 else full_conversation_text[:800] + "..."
            logger.info("ã‚µãƒ³ãƒ—ãƒ« {}: å¯¾è©±ãƒ†ã‚­ã‚¹ãƒˆé•· = {}".format(i, len(short_conversation)))
        except Exception as e:
            logger.error("ã‚µãƒ³ãƒ—ãƒ« {}: turn_listä½œæˆã§ã‚¨ãƒ©ãƒ¼: {}".format(i, e))
            continue

        # å®Ÿéš›ã®è©•ä¾¡ã‚¹ã‚³ã‚¢ã‚’å–å¾—
        if not review_jp:
            logger.warning("ã‚µãƒ³ãƒ—ãƒ« {}: è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—".format(i))
            continue

        # å„è©•ä¾¡é …ç›®ã«ã¤ã„ã¦å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        for item in EVALUATION_ITEMS:
            if item not in review_jp:
                logger.warning("ã‚µãƒ³ãƒ—ãƒ« {}: è©•ä¾¡é …ç›® '{}' ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—".format(i, item))
                continue
            
            # å®Ÿéš›ã®ã‚¹ã‚³ã‚¢ï¼ˆ0-5ç‚¹ï¼‰ã‚’å–å¾—
            actual_score = review_jp[item]
            if not isinstance(actual_score, int) or actual_score < 0 or actual_score > 5:
                logger.warning("ã‚µãƒ³ãƒ—ãƒ« {}, é …ç›® '{}': ç„¡åŠ¹ãªã‚¹ã‚³ã‚¢ {} ã‚’ã‚¹ã‚­ãƒƒãƒ—".format(i, item, actual_score))
                continue
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå…¥åŠ›ï¼‰ã®å®šç¾©
            input_text = """### æŒ‡ç¤º
ä»¥ä¸‹ã®å¯¾è©±ã«ã¤ã„ã¦ã€Œ{}ã€ã®æº€è¶³åº¦ã‚’0ï½5ç‚¹ã§è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

### å¯¾è©±
{}

### å›ç­”
""".format(item, short_conversation)
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
            inputs = tokenizer(
                input_text,
                truncation=True,
                padding=False,
                max_length=256,  # M4æœ€é©åŒ–: ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã®ãŸã‚å¤§å¹…çŸ­ç¸®
                return_tensors=None
            )
            
            # å›å¸°ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            regression_data.append({
                "input_ids": inputs["input_ids"],
                "attention_mask": inputs["attention_mask"],
                "labels": float(actual_score)  # æ•°å€¤ãƒ©ãƒ™ãƒ«
            })

    logger.info("æœ€çµ‚çš„ã«ç”Ÿæˆã•ã‚ŒãŸå›å¸°ãƒ‡ãƒ¼ã‚¿æ•°: {}".format(len(regression_data)))
    return Dataset.from_list(regression_data)

def get_regression_dataset(
    tokenizer,
    use_cache=True,
    cache_path="./regression_dataset_real_labels.jsonl",
    max_samples=100
):
    """
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå­˜åœ¨ã™ã‚Œã°ãã‚Œã‚’èª­ã¿è¾¼ã¿ã€ãªã‘ã‚Œã°å®Ÿéš›ã®æ­£è§£ãƒ©ãƒ™ãƒ«ã‹ã‚‰å›å¸°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚
    """
    if use_cache and os.path.exists(cache_path):
        logger.info("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸå›å¸°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ {} ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚".format(cache_path))
        return load_dataset("json", data_files=cache_path, split="train")

    logger.info("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€å®Ÿéš›ã®æ­£è§£ãƒ©ãƒ™ãƒ«ã‹ã‚‰å›å¸°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ–°ãŸã«ç”Ÿæˆã—ã¾ã™ã€‚")
    train_ds, _, _ = load_and_split_dataset()
    
    # ãƒ‡ãƒãƒƒã‚°: æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ã®æ§‹é€ ã‚’è©³ç´°ã«ç¢ºèª
    if len(train_ds) > 0:
        sample = train_ds[0]
        logger.info("=== ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ç¢ºèª ===")
        logger.info("ã‚µãƒ³ãƒ—ãƒ«ã®keys: {}".format(list(sample.keys())))
        if 'review_by_client_jp' in sample:
            review = sample['review_by_client_jp']
            logger.info("review_by_client_jp ã®å‹: {}".format(type(review)))
            logger.info("åˆ©ç”¨å¯èƒ½ãªè©•ä¾¡é …ç›®: {}".format([k for k in review.keys() if k in EVALUATION_ITEMS]))
    
    regression_dataset = create_regression_dataset_from_real_labels(train_ds, max_samples=max_samples, tokenizer=tokenizer)

    # æ¬¡å›ä»¥é™ã®ãŸã‚ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜
    regression_dataset.to_json(cache_path)
    logger.info("ç”Ÿæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ {} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚".format(cache_path))

    return regression_dataset

# --- ãƒ¡ã‚¤ãƒ³ã®å­¦ç¿’å‡¦ç† ---
def main():
    # --- 0. ãƒ¡ãƒ¢ãƒªç›£è¦–é–‹å§‹ ---
    memory_guard.start_monitoring()
    
    # --- 1. è¨­å®š ---
    # â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… Swallow-7b-instruct-hf å°‚ç”¨è¨­å®š â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
    # çµ¶å¯¾ã«Swallowãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç„¡åŠ¹ï¼‰
    sft_model_name = "tokyotech-llm/Swallow-7b-instruct-hf"
    generator_model_name = "tokyotech-llm/Swallow-7b-instruct-hf"
    
    # Swallowãƒ¢ãƒ‡ãƒ«å¼·åˆ¶ä½¿ç”¨ãƒ•ãƒ©ã‚°
    FORCE_SWALLOW_MODEL = True
    logger.info("ğŸ¦… Swallow-7b-instruct-hfãƒ¢ãƒ‡ãƒ«ã®å¼·åˆ¶ä½¿ç”¨ãŒæœ‰åŠ¹ã§ã™")

    # å­¦ç¿’æ¸ˆã¿ã‚¢ãƒ€ãƒ—ã‚¿ã®ä¿å­˜å…ˆ
    output_dir = "./swallow_emotion_reward_adapter"
    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ã†ã‹
    USE_CACHE = True  # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç”¨ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æœ‰åŠ¹åŒ–ï¼ˆå‡¦ç†æ™‚é–“çŸ­ç¸®ï¼‰
    # M4æœ€é©åŒ–: ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’å¤§å¹…å‰Šæ¸›ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡æœ€å„ªå…ˆï¼‰
    MAX_SAMPLES_FOR_DATA_GENERATION = 10  # M4ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ç”¨ã«å¤§å¹…å‰Šæ¸›ï¼ˆ20â†’10ï¼‰
    
    # å®Ÿé¨“ç®¡ç†ã®è¨­å®š
    EXPERIMENT_TRACKING_TOOL = "both"  # "tensorboard", "wandb", "both", "none"

    # --- 1.5. å®Ÿé¨“ç®¡ç†ã®åˆæœŸåŒ– ---
    experiment_tracker = None
    try:
        if EXPERIMENT_TRACKING_AVAILABLE and EXPERIMENT_TRACKING_TOOL != "none":
            experiment_tracker = create_experiment_tracker(
                tracking_tool=EXPERIMENT_TRACKING_TOOL,
                project_name="emotion_reward_sft"
            )
            logger.info("å®Ÿé¨“ç®¡ç†ãƒ„ãƒ¼ãƒ« '{}' ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ".format(EXPERIMENT_TRACKING_TOOL))
    except (NameError, Exception) as e:
        logger.warning("å®Ÿé¨“ç®¡ç†ãƒ„ãƒ¼ãƒ«åˆæœŸåŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—: {}".format(e))
        experiment_tracker = None
    
    # --- 2. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æº–å‚™ ---
    logger.info("ğŸ¦… Swallowãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ '{}' ã‚’èª­ã¿è¾¼ã¿ã¾ã™...".format(sft_model_name))
    
    # Swallowãƒ¢ãƒ‡ãƒ«å¼·åˆ¶ä½¿ç”¨ã§ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿
    tokenizer, actual_model_name = load_tokenizer_with_fallback(sft_model_name, force_swallow=FORCE_SWALLOW_MODEL)
    tokenizer = setup_tokenizer_padding(tokenizer)
    
    # Swallowå¼·åˆ¶ãƒ¢ãƒ¼ãƒ‰ã§ã¯ãƒ¢ãƒ‡ãƒ«åå¤‰æ›´ã‚’è¨±å¯ã—ãªã„
    if FORCE_SWALLOW_MODEL and actual_model_name != sft_model_name:
        raise RuntimeError("ğŸš¨ Swallowå¼·åˆ¶ãƒ¢ãƒ¼ãƒ‰ã§ãƒ¢ãƒ‡ãƒ«åãŒå¤‰æ›´ã•ã‚Œã¾ã—ãŸ: {} -> {}".format(sft_model_name, actual_model_name))
    elif actual_model_name != sft_model_name:
        logger.info("ãƒ¢ãƒ‡ãƒ«åã‚’ '{}' ã‹ã‚‰ '{}' ã«å¤‰æ›´ã—ã¾ã—ãŸ".format(sft_model_name, actual_model_name))
        sft_model_name = actual_model_name
    
    logger.info("ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼è¨­å®šå®Œäº†: pad_token='{}'".format(tokenizer.pad_token))

    # --- 3. å›å¸°ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ï¼ˆå®Ÿéš›ã®æ­£è§£ãƒ©ãƒ™ãƒ«ä½¿ç”¨ï¼‰ ---
    logger.info("å®Ÿéš›ã®æ­£è§£ãƒ©ãƒ™ãƒ«ã‚’ä½¿ç”¨ã—ã¦å›å¸°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™ã—ã¾ã™...")
    regression_dataset = get_regression_dataset(
        tokenizer=tokenizer,
        use_cache=USE_CACHE,
        max_samples=MAX_SAMPLES_FOR_DATA_GENERATION
    )
    logger.info("å›å¸°ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™å®Œäº†ã€‚ã‚µãƒ³ãƒ—ãƒ«æ•°: {}".format(len(regression_dataset)))
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
    if len(regression_dataset) > 0:
        logger.info("=== å›å¸°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µãƒ³ãƒ—ãƒ« ===")
        sample = regression_dataset[0]
        logger.info("å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {}".format(len(sample['input_ids'])))
        logger.info("ãƒ©ãƒ™ãƒ«: {}".format(sample['labels']))

    # --- 4. å›å¸°å¯¾è±¡ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ (CPUç‰ˆ) ---
    logger.info("ğŸ¦… Swallowå›å¸°å¯¾è±¡ãƒ¢ãƒ‡ãƒ« '{}' ã‚’CPUç’°å¢ƒç”¨ã«èª­ã¿è¾¼ã¿ã¾ã™...".format(sft_model_name))

    # M4 MacBook Airæœ€é©åŒ–: ãƒ¡ãƒ¢ãƒªåˆ¶é™ã®ãŸã‚CPUãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
    device = "cpu"
    logger.info("ğŸš€ M4æœ€é©åŒ–: 10ã‚³ã‚¢CPUä¸¦åˆ—å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–ï¼‰")
    logger.info("Device set to use {}".format(device))

    # Swallowãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆCPUç”¨ã€å†è©¦è¡Œæ©Ÿèƒ½ä»˜ãï¼‰
    logger.info("ğŸ¦… Swallowãƒ¢ãƒ‡ãƒ« '{}' ã‚’èª­ã¿è¾¼ã¿ã¾ã™...".format(sft_model_name))
    max_retries = 3
    retry_delay = 60  # 60ç§’å¾…æ©Ÿ
    
    try:
        model = None
        for attempt in range(max_retries):
            try:
                logger.info("ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿è©¦è¡Œ {}/{}".format(attempt + 1, max_retries))
                # M4æœ€é©åŒ–: MPSã§ã¯é‡å­åŒ–ãªã—ï¼ˆMacã§ã¯éå¯¾å¿œï¼‰
                if device == "mps":
                    # MPSã§ã¯é‡å­åŒ–ãªã—ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–
                    model = AutoModelForCausalLM.from_pretrained(
                        sft_model_name,
                        torch_dtype=torch.float32,
                        device_map=None,
                        trust_remote_code=True,
                        resume_download=True,
                        low_cpu_mem_usage=True,  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–
                    )
                else:
                    # CPU/CUDAã®å ´åˆï¼ˆCPUã§ã¯é‡å­åŒ–ãªã—ï¼‰
                    if device == "cpu":
                        # CPUãƒ¢ãƒ¼ãƒ‰ã§ã¯é‡å­åŒ–ãªã—ï¼ˆãƒ¡ãƒ¢ãƒªåˆ¶é™ãŒã‚ã‚‹ãŒå®‰å®šï¼‰
                        model = AutoModelForCausalLM.from_pretrained(
                            sft_model_name,
                            torch_dtype=torch.float32,
                            device_map=None,
                            trust_remote_code=True,
                            resume_download=True,
                            low_cpu_mem_usage=True,  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–
                        )
                    else:
                        # CUDAã®å ´åˆã¯8bité‡å­åŒ–
                        model = AutoModelForCausalLM.from_pretrained(
                            sft_model_name,
                            torch_dtype=torch.float16,
                            device_map=None,
                            trust_remote_code=True,
                            resume_download=True,
                            load_in_8bit=True,
                            llm_int8_enable_fp32_cpu_offload=True,
                        )
                logger.info("âœ… Swallowãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")
                break
            except Exception as retry_error:
                logger.warning("ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿è©¦è¡Œ {}/{} å¤±æ•—: {}".format(attempt + 1, max_retries, retry_error))
                if attempt < max_retries - 1:
                    logger.info("{}ç§’å¾Œã«å†è©¦è¡Œã—ã¾ã™...".format(retry_delay))
                    time.sleep(retry_delay)
                else:
                    # æœ€å¾Œã®è©¦è¡Œã‚‚å¤±æ•—ã—ãŸå ´åˆã€å¤–å´ã®exceptã«é€²ã‚€
                    raise retry_error
        
        # Swallowãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®å‡ºåŠ›
        logger.info("ğŸ¦… Swallowãƒ¢ãƒ‡ãƒ«æƒ…å ±:")
        logger.info("  ãƒ¢ãƒ‡ãƒ«å: {}".format(sft_model_name))
        logger.info("  ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {}".format(type(model).__name__))
        if hasattr(model, 'config'):
            logger.info("  ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£: {}".format(getattr(model.config, 'architectures', 'Unknown')))
            logger.info("  ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {}".format(getattr(model.config, 'model_type', 'Unknown')))
        
    except Exception as e:
        if FORCE_SWALLOW_MODEL:
            logger.error("ğŸš¨ Swallowå¼·åˆ¶ãƒ¢ãƒ¼ãƒ‰: ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—")
            logger.error("ã‚¨ãƒ©ãƒ¼è©³ç´°: {}".format(e))
            logger.error("è§£æ±ºæ–¹æ³•:")
            logger.error("1. ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            logger.error("2. Hugging Face Hubã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ã‹ç¢ºèªã—ã¦ãã ã•ã„")
            logger.error("3. ååˆ†ãªãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãŒã‚ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ï¼ˆç´„14GBå¿…è¦ï¼‰")
            logger.error("4. transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒæœ€æ–°ç‰ˆã‹ç¢ºèªã—ã¦ãã ã•ã„")
            raise RuntimeError("ğŸš¨ Swallowå¼·åˆ¶ãƒ¢ãƒ¼ãƒ‰ã§ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
        else:
            logger.error("ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {}".format(e))
            logger.info("ä»£æ›¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™...")
            # ä»£æ›¿ãƒ¢ãƒ‡ãƒ«ã«åˆ‡ã‚Šæ›¿ãˆ
            if sft_model_name != "microsoft/DialoGPT-medium":
                sft_model_name = "microsoft/DialoGPT-medium"
                model = AutoModelForCausalLM.from_pretrained(
                    sft_model_name,
                    torch_dtype=torch.float32,
                    device_map=None,
                    trust_remote_code=True,
                )
                logger.info("ä»£æ›¿ãƒ¢ãƒ‡ãƒ« '{}' ã§èª­ã¿è¾¼ã¿æˆåŠŸ".format(sft_model_name))
            else:
                raise e
    model = model.to(device)
    model.config.use_cache = False
    
    # M4æœ€é©åŒ–: ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã§ãƒ¡ãƒ¢ãƒªå¤§å¹…å‰Šæ¸›
    model.gradient_checkpointing_enable()
    logger.info("ğŸš€ M4æœ€é©åŒ–: ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæœ‰åŠ¹åŒ–ï¼ˆãƒ¡ãƒ¢ãƒª50%å‰Šæ¸›ï¼‰")
    
    # M4æœ€é©åŒ–: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–
    import gc
    gc.collect()  # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
    
    # PyTorchãƒ¡ãƒ¢ãƒªç®¡ç†è¨­å®š
    if device == "cpu":
        # M4 CPUæœ€é©åŒ–è¨­å®š
        torch.set_num_threads(10)  # M4ã®10ã‚³ã‚¢ã‚’æ´»ç”¨
        torch.set_num_interop_threads(4)  # ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–
        logger.info("ğŸš€ M4 CPUæœ€é©åŒ–: ã‚¹ãƒ¬ãƒƒãƒ‰æ•°=10, ä¸¦åˆ—ã‚¹ãƒ¬ãƒƒãƒ‰æ•°=4")
    elif device == "mps":
        # MPSè¨­å®šï¼ˆä½¿ç”¨ã—ãªã„ãŒå¿µã®ãŸã‚ï¼‰
        torch.mps.set_per_process_memory_fraction(0.6)
        logger.info("ğŸš€ M4 MPS: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’60%ã«è¨­å®š")
    elif device == "cuda":
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = True
        logger.info("ğŸ”¥ CUDAæœ€é©åŒ–è¨­å®š")

    # ãƒ¢ãƒ‡ãƒ«ã®æ§‹é€ ã‚’èª¿ã¹ã¦ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è‡ªå‹•æ¤œå‡º
    def find_target_modules(model):
        """ãƒ¢ãƒ‡ãƒ«ã®ç·šå½¢å±¤ã‚’è‡ªå‹•æ¤œå‡ºã—ã¦ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æ±ºå®š"""
        target_modules = set()
        all_modules = {}
        
        # å…¨ã¦ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’èª¿æŸ»
        for name, module in model.named_modules():
            module_type = type(module).__name__
            all_modules[name] = module_type
            
            if isinstance(module, torch.nn.Linear):
                # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åã®æœ€å¾Œã®éƒ¨åˆ†ã‚’å–å¾—
                module_name = name.split('.')[-1]
                target_modules.add(module_name)
                logger.debug("ç·šå½¢å±¤ç™ºè¦‹: {} ({}æ¬¡å…ƒ -> {}æ¬¡å…ƒ)".format(
                    name, module.in_features, module.out_features))
        
        # ãƒ‡ãƒãƒƒã‚°: æœ€åˆã®æ•°å±¤ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹é€ ã‚’è¡¨ç¤º
        logger.info("=== ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã®è©³ç´° ===")
        layer_count = 0
        for name, module_type in all_modules.items():
            if layer_count < 20:  # æœ€åˆã®20å±¤ã®ã¿è¡¨ç¤º
                logger.info("  {}: {}".format(name, module_type))
                layer_count += 1
            elif layer_count == 20:
                logger.info("  ... (ä»¥ä¸‹çœç•¥)")
                break
        
        # Swallowãƒ¢ãƒ‡ãƒ«ï¼ˆLLaMAãƒ™ãƒ¼ã‚¹ï¼‰å°‚ç”¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
        if FORCE_SWALLOW_MODEL:
            # Swallow/LLaMAã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å°‚ç”¨ï¼ˆå„ªå…ˆé †ï¼‰
            common_targets = [
                # LLaMA/Swallowç³»ï¼ˆæœ€å„ªå…ˆï¼‰
                "q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj",
                # è¿½åŠ ã®Transformerç³»
                "self_attn", "mlp"
            ]
            logger.info("ğŸ¦… Swallowå°‚ç”¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ¤œå‡ºãƒ¢ãƒ¼ãƒ‰")
        else:
            # ä¸€èˆ¬çš„ãªã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åã®ãƒªã‚¹ãƒˆï¼ˆå„ªå…ˆé †ï¼‰
            common_targets = [
                # Transformerç³»
                "q_proj", "k_proj", "v_proj", "o_proj",
                # LLaMAç³»
                "gate_proj", "up_proj", "down_proj",
                # GPTç³»
                "c_attn", "c_proj", "c_fc",
                # BERTç³»
                "query", "key", "value", "dense",
                # DialoGPT/GPT-2ç³»
                "attn", "mlp",
                # ãã®ä»–
                "attention", "feed_forward", "linear", "fc"
            ]
        
        # è¦‹ã¤ã‹ã£ãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰é©åˆ‡ãªã‚‚ã®ã‚’é¸æŠ
        selected_targets = []
        for target in common_targets:
            if target in target_modules:
                selected_targets.append(target)
        
        # æœ€ä½é™ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
        if len(selected_targets) < 1:
            # ã‚ˆã‚ŠæŸ”è»Ÿãªãƒãƒƒãƒãƒ³ã‚°ã‚’è©¦è¡Œ
            flexible_targets = []
            for module_name in target_modules:
                if any(keyword in module_name.lower() for keyword in 
                      ['proj', 'attn', 'mlp', 'dense', 'linear', 'fc']):
                    flexible_targets.append(module_name)
            
            if flexible_targets:
                selected_targets = flexible_targets[:4]  # æœ€å¤§4ã¤ã¾ã§
            else:
                # æœ€å¾Œã®æ‰‹æ®µ: å…¨ã¦ã®ç·šå½¢å±¤ã‹ã‚‰æœ€åˆã®æ•°å€‹ã‚’é¸æŠ
                selected_targets = list(target_modules)[:4]
        
        if FORCE_SWALLOW_MODEL:
            logger.info("ğŸ¦… Swallowæ¤œå‡ºã•ã‚ŒãŸã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«: {}".format(selected_targets))
            logger.info("ğŸ¦… Swallowåˆ©ç”¨å¯èƒ½ãªå…¨ç·šå½¢å±¤: {}".format(sorted(target_modules)))
        else:
            logger.info("æ¤œå‡ºã•ã‚ŒãŸã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«: {}".format(selected_targets))
            logger.info("åˆ©ç”¨å¯èƒ½ãªå…¨ç·šå½¢å±¤: {}".format(sorted(target_modules)))
        
        return selected_targets
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®è‡ªå‹•æ¤œå‡º
    target_modules = find_target_modules(model)
    
    if not target_modules:
        logger.error("é©åˆ‡ãªã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        raise ValueError("LoRAç”¨ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    # M4æœ€é©åŒ–: LoRAã®è¨­å®š
    if device == "cpu":
        # M4 CPUæœ€é©åŒ–è¨­å®š
        peft_config = LoraConfig(
            lora_alpha=16,
            lora_dropout=0.1,
            r=32,  # CPUã§ã‚‚ååˆ†ãªrank
            bias="none",
            task_type="CAUSAL_LM",
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]  # ä¸»è¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
        )
        logger.info("ğŸš€ M4 CPUæœ€é©åŒ–: LoRAè¨­å®šï¼ˆr=32, alpha=16ï¼‰")
    elif device == "mps":
        # MPSè»½é‡è¨­å®šï¼ˆä½¿ç”¨ã—ãªã„ãŒå¿µã®ãŸã‚ï¼‰
        peft_config = LoraConfig(
            lora_alpha=16,
            lora_dropout=0.1,
            r=16,  # ã‚ˆã‚Šè»½é‡
            bias="none",
            task_type="CAUSAL_LM",
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
        )
        logger.info("ğŸš€ M4 MPS: è»½é‡LoRAè¨­å®šï¼ˆr=16, alpha=16ï¼‰")
    else:
        # æ¨™æº–è¨­å®š
        peft_config = LoraConfig(
            lora_alpha=16,
            lora_dropout=0.1,
            r=32,
            bias="none",
            task_type="CAUSAL_LM",
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
        )
        logger.info("ğŸ’» æ¨™æº–LoRAè¨­å®šï¼ˆr=32, alpha=16ï¼‰")
    
    # PEFTãƒ¢ãƒ‡ãƒ«ã®é©ç”¨
    try:
        logger.info("PEFTãƒ¢ãƒ‡ãƒ«ã‚’é©ç”¨ä¸­...")
        logger.info("LoRAè¨­å®š: r={}, alpha={}, dropout={}, targets={}".format(
            peft_config.r, peft_config.lora_alpha, peft_config.lora_dropout, peft_config.target_modules))
        
        model = get_peft_model(model, peft_config)
        logger.info("âœ… PEFTãƒ¢ãƒ‡ãƒ«ã®é©ç”¨æˆåŠŸ")
        
        # å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®è¡¨ç¤º
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in model.parameters())
        logger.info("å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {:,} / {:,} ({:.2f}%)".format(
            trainable_params, total_params, 100 * trainable_params / total_params))
        
    except ValueError as e:
        logger.error("PEFTãƒ¢ãƒ‡ãƒ«é©ç”¨å¤±æ•—: {}".format(e))
        logger.info("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: LoRAãªã—ã§å­¦ç¿’ã‚’ç¶šè¡Œã—ã¾ã™")
        
        # LoRAãªã—ã§ã®å­¦ç¿’ç”¨ã«è¨­å®šã‚’èª¿æ•´
        logger.warning("âš ï¸ LoRAã‚’ä½¿ç”¨ã›ãšã«å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã—ã¾ã™ï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¢—åŠ ã—ã¾ã™ï¼‰")
        
        # å­¦ç¿’ç‡ã‚’ä¸‹ã’ã‚‹ï¼ˆå…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å­¦ç¿’ã®å ´åˆï¼‰
        logger.info("å­¦ç¿’ç‡ã‚’èª¿æ•´: 2e-4 -> 5e-5")
        
        # ãƒ¢ãƒ‡ãƒ«ã¯ãã®ã¾ã¾ä½¿ç”¨ï¼ˆLoRAãªã—ï¼‰
        peft_config = None

    # --- 5. å›å¸°ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šï¼ˆCPUç’°å¢ƒç”¨ã«èª¿æ•´ï¼‰ ---
    # M4æœ€é©åŒ–: å­¦ç¿’ç‡ã®å‹•çš„èª¿æ•´
    if peft_config is not None:
        if device == "cpu":
            # M4 CPUæœ€é©åŒ–: CPUã§ã¯å®‰å®šã—ãŸå­¦ç¿’ç‡
            learning_rate = 2e-4
            logger.info("ğŸš€ M4 CPUæœ€é©åŒ–: å­¦ç¿’ç‡ = {}".format(learning_rate))
        elif device == "mps":
            # MPSç”¨å­¦ç¿’ç‡
            learning_rate = 3e-4
            logger.info("ğŸš€ M4 MPSæœ€é©åŒ–: å­¦ç¿’ç‡ = {}".format(learning_rate))
        else:
            learning_rate = 2e-4  # æ¨™æº–LoRAå­¦ç¿’ç‡
            logger.info("LoRAä½¿ç”¨: å­¦ç¿’ç‡ = {}".format(learning_rate))
    else:
        learning_rate = 5e-5  # å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å­¦ç¿’æ™‚
        logger.info("å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å­¦ç¿’: å­¦ç¿’ç‡ = {}".format(learning_rate))
    
    # M4æœ€é©åŒ–: CPUãƒ¢ãƒ¼ãƒ‰ç”¨ã®ä¸¦åˆ—å‡¦ç†è¨­å®š
    if device == "cpu":
        # M4 10ã‚³ã‚¢CPUæœ€é©åŒ–è¨­å®š
        batch_size = 2  # CPUã§ã¯å°‘ã—å¤§ãã
        accumulation_steps = 8  # å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º16
        num_workers = 8  # M4ã®10ã‚³ã‚¢ã‚’æ´»ç”¨ï¼ˆ8ä¸¦åˆ—ï¼‰
        pin_memory = False  # CPUã§ã¯False
        fp16_enabled = False  # CPUã§ã¯fp16ç„¡åŠ¹
        logger.info("ğŸš€ M4 CPUæœ€é©åŒ–: ãƒãƒƒãƒã‚µã‚¤ã‚º={}, ä¸¦åˆ—æ•°={}".format(batch_size, num_workers))
    elif device == "mps":
        # MPSè¨­å®šï¼ˆä½¿ç”¨ã—ãªã„ãŒå¿µã®ãŸã‚ï¼‰
        batch_size = 1
        accumulation_steps = 16
        num_workers = 0
        pin_memory = False
        fp16_enabled = False
    else:
        # CUDAè¨­å®š
        batch_size = 1
        accumulation_steps = 16
        num_workers = 0
        pin_memory = False
        fp16_enabled = True
    
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=batch_size,  # M4æœ€é©åŒ–
        gradient_accumulation_steps=accumulation_steps,  # M4æœ€é©åŒ–
        optim="adamw_torch",
        save_steps=100,
        logging_steps=10,
        learning_rate=learning_rate,
        fp16=fp16_enabled,  # M4æœ€é©åŒ–
        max_grad_norm=0.3,
        num_train_epochs=1,
        warmup_ratio=0.03,
        lr_scheduler_type="constant",
        dataloader_pin_memory=pin_memory,  # M4æœ€é©åŒ–
        dataloader_num_workers=num_workers,  # M4æœ€é©åŒ–: ä¸¦åˆ—å‡¦ç†
        remove_unused_columns=False,
        # M4æœ€é©åŒ–: ãƒ¡ãƒ¢ãƒªå‰Šæ¸›è¨­å®š
        max_steps=50,  # ã‚¹ãƒ†ãƒƒãƒ—æ•°åˆ¶é™ã§ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
        gradient_checkpointing=True,  # ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
        dataloader_drop_last=True,  # ä¸å®Œå…¨ãƒãƒƒãƒã‚’å‰Šé™¤
    )

    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨˜éŒ²
    if experiment_tracker:
        hyperparams = {
            "model_name": sft_model_name,
            "max_samples": MAX_SAMPLES_FOR_DATA_GENERATION,
            "use_cache": USE_CACHE,
            "batch_size": training_args.per_device_train_batch_size,
            "gradient_accumulation_steps": training_args.gradient_accumulation_steps,
            "learning_rate": training_args.learning_rate,
            "num_epochs": training_args.num_train_epochs,
            "device": device,
            "dataset_size": len(regression_dataset),
            "use_lora": peft_config is not None
        }
        
        # LoRAé–¢é€£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆLoRAä½¿ç”¨æ™‚ã®ã¿ï¼‰
        if peft_config is not None:
            hyperparams.update({
                "lora_r": peft_config.r,
                "lora_alpha": peft_config.lora_alpha,
                "lora_dropout": peft_config.lora_dropout,
                "lora_target_modules": peft_config.target_modules
            })
        else:
            hyperparams.update({
                "training_mode": "full_parameter_training",
                "lora_r": "N/A",
                "lora_alpha": "N/A",
                "lora_dropout": "N/A"
            })
        
        experiment_tracker.log_hyperparameters(hyperparams)

    # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã®æº–å‚™ï¼ˆå›å¸°ã‚¿ã‚¹ã‚¯ç”¨ï¼‰
    data_collator = RegressionDataCollator(tokenizer=tokenizer)

    # --- 6. ã‚«ã‚¹ã‚¿ãƒ Trainerã®åˆæœŸåŒ–ã¨å­¦ç¿’é–‹å§‹ï¼ˆMSEæå¤±ï¼‰ ---
    trainer = RegressionTrainer(
        experiment_tracker=experiment_tracker,
        model=model,
        args=training_args,
        train_dataset=regression_dataset,
        data_collator=data_collator,
        processing_class=tokenizer,  # tokenizerã®ä»£ã‚ã‚Šã«processing_classã‚’ä½¿ç”¨
    )

    logger.info("ğŸš€ MSEæå¤±ã‚’ä½¿ç”¨ã—ãŸå›å¸°ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
    trainer.train()

    # --- 7. å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ ---
    final_model_path = os.path.join(output_dir, "final_model")
    trainer.save_model(final_model_path)
    logger.info("âœ… å›å¸°ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ã€‚ãƒ¢ãƒ‡ãƒ«ã¯ {} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚".format(final_model_path))
    
    # ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã®ä¿å­˜ï¼ˆW&Bï¼‰
    if experiment_tracker:
        try:
            experiment_tracker.log_model_artifact(final_model_path)
            experiment_tracker.log_text(
                "å›å¸°ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚MSEæå¤±ã‚’ä½¿ç”¨ã—ãŸæ„Ÿæƒ…å ±é…¬ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã€‚",
                "training_summary"
            )
        except Exception as e:
            logger.error("å®Ÿé¨“ç®¡ç†ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {}".format(e))
        finally:
            # å®Ÿé¨“ç®¡ç†ãƒ„ãƒ¼ãƒ«ã®çµ‚äº†å‡¦ç†
            experiment_tracker.finish()

if __name__ == "__main__":
    main()