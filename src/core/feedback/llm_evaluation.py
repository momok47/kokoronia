import re
import logging
from typing import Dict, List
from transformers import pipeline

# ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦è¡Œã€å¤±æ•—ã—ãŸå ´åˆã¯çµ¶å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from .data_processing import EVALUATION_ITEMS
except ImportError:
    from data_processing import EVALUATION_ITEMS

logger = logging.getLogger(__name__)

def create_unified_evaluation_prompt(conversation_text: str, turn_index: int) -> str:
    """
    LLMã«æ¸¡ã™ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹ï¼ˆFew-shotãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ç‰ˆï¼‰
    """
    # Few-shotãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã®ãŸã‚ã®å®Œç’§ãªã€ŒãŠæ‰‹æœ¬ã€ã‚’ç”¨æ„ã™ã‚‹
    example_input = (
        "counselor: ã“ã‚“ã«ã¡ã¯ï¼ä»Šæ—¥ã¯ã©ã®ã‚ˆã†ãªãŠè©±ã‚’èã‹ã›ã¦ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ\n"
        "client: æœ€è¿‘ã€ä»•äº‹ã§ã‚¹ãƒˆãƒ¬ã‚¹ãŒæºœã¾ã£ã¦ã„ã¦...\n"
        "counselor: ãŠç–²ã‚Œæ§˜ã§ã™ã€‚ãã®ã‚¹ãƒˆãƒ¬ã‚¹ã«ã¤ã„ã¦ã€ã‚‚ã†å°‘ã—è©³ã—ãæ•™ãˆã¦ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ\n"
        "client: ã¯ã„ã€ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚ä¸Šå¸ã¨ã®é–¢ä¿‚ã§æ‚©ã‚“ã§ã„ã¦...\n"
        "counselor: ãã‚Œã¯å¤§å¤‰ã§ã—ãŸã­ã€‚ä¸Šå¸ã¨ã®é–¢ä¿‚ã§å…·ä½“çš„ã«ã©ã®ã‚ˆã†ãªã“ã¨ãŒèµ·ãã¦ã„ã‚‹ã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ\n"
        "client: ã¨ã¦ã‚‚åŠ©ã‹ã‚Šã¾ã—ãŸï¼è©±ã‚’èã„ã¦ã‚‚ã‚‰ãˆã¦ã€æ°—æŒã¡ãŒæ¥½ã«ãªã‚Šã¾ã—ãŸã€‚"
    )

    example_output = "0ç‚¹: 0%, 1ç‚¹: 0%, 2ç‚¹: 10%, 3ç‚¹: 30%, 4ç‚¹: 40%, 5ç‚¹: 20%"

    # å®Ÿéš›ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµ„ã¿ç«‹ã¦ã‚‹
    prompt = (
        "ã‚ãªãŸã¯å¯¾è©±è©•ä¾¡ã®å°‚é–€å®¶ã§ã™ã€‚æç¤ºã•ã‚ŒãŸå¯¾è©±ã‚’åˆ†æã—ã€ä¼šè©±å…¨ä½“ã®ãƒã‚¸ãƒ†ã‚£ãƒ–ã•ã‚’0ç‚¹ã‹ã‚‰5ç‚¹ã®6æ®µéšã§è©•ä¾¡ã—ã€ãã®ç¢ºç‡åˆ†å¸ƒã‚’ç®—å‡ºã—ã¦ãã ã•ã„ã€‚\n\n"
        "--- ãŠæ‰‹æœ¬ ---\n"
        "ã€åˆ†æå¯¾è±¡ã®å¯¾è©±ã€‘:\n"
        f"{example_input}\n"
        "ã€å‡ºåŠ›ã€‘:\n"
        f"{example_output}\n\n"
        "--- æœ¬ç•ª ---\n"
        "ã€åˆ†æå¯¾è±¡ã®å¯¾è©±ã€‘:\n"
        f"{conversation_text}\n"
        "ã€å‡ºåŠ›ã€‘:\n"
    )
    
    return prompt



def call_llm_for_probability_distribution(tokenizer, model, conversation_text: str) -> List[float]:
    """
    LLMã‚’å‘¼ã³å‡ºã—ã¦ã€ä¼šè©±ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç¢ºç‡åˆ†å¸ƒã‚’å–å¾—ã™ã‚‹
    """
    prompt = create_unified_evaluation_prompt(conversation_text, 0)
    
    # ãƒ¢ãƒ‡ãƒ«ã¸ã®å…¥åŠ›ã‚’æº–å‚™
    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(model.device)

    # ğŸš¨ã€é‡è¦ã€‘ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã—ã¦ã€LLMã®å¿œç­”ã‚’åˆ¶å¾¡
    response_ids = model.generate(
        input_ids,
        max_new_tokens=100,         # ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        do_sample=True,             # ğŸ‘ˆ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’æœ‰åŠ¹ã«ã—ã€å¤šæ§˜ãªå‡ºåŠ›ã‚’ä¿ƒã™
        temperature=0.7,            # ğŸ‘ˆ å‡ºåŠ›ã®ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’åˆ¶å¾¡ (å‰µé€ æ€§ã‚’å°‘ã—åŠ ãˆã‚‹)
        top_p=0.95,                 # ğŸ‘ˆ ä¸Šä½95%ã®ç¢ºç‡ã‚’æŒã¤å˜èªã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        repetition_penalty=1.15,    # ğŸ‘ˆ ç¹°ã‚Šè¿”ã—ã‚’æŠ‘åˆ¶ã™ã‚‹ãŸã‚ã®ãƒšãƒŠãƒ«ãƒ†ã‚£
        pad_token_id=tokenizer.eos_token_id  # pad_token_id ã‚’ eos_token_id ã«è¨­å®š
    )
    
    # å¿œç­”ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
    response_only = tokenizer.decode(response_ids[0][input_ids.shape[-1]:], skip_special_tokens=True)
    
    logger.info(f"LLM Raw Response: '{response_only}'")
    
    # ç¢ºç‡ã‚’æŠ½å‡º
    probabilities = parse_probabilities_from_llm_response(response_only)
    logger.info(f"Parsed Probabilities: {probabilities}")

    return probabilities

def parse_probabilities_from_llm_response(response: str) -> List[float]:
    """
    LLMã®å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå¤šå°‘ã®æºã‚Œã‚„ä½™è¨ˆãªæ–‡ç« ãŒã‚ã£ã¦ã‚‚å¯¾å¿œå¯èƒ½ï¼‰ã‹ã‚‰
    ç¢ºç‡åˆ†å¸ƒã‚’æŠ½å‡ºã™ã‚‹ã€ã‚ˆã‚Šé ‘å¥ãªé–¢æ•°ã€‚
    """
    # æœŸå¾…ã™ã‚‹ç¢ºç‡åˆ†å¸ƒã®è¡Œã‚’ã™ã¹ã¦è¦‹ã¤ã‘ã‚‹ãŸã‚ã®æ­£è¦è¡¨ç¾
    # "0ç‚¹: 10.5%" ã‚„ " 1 ç‚¹ : 5 % " ã®ã‚ˆã†ãªè¡¨è¨˜ã®æºã‚Œã«ã‚‚å¯¾å¿œ
    pattern = r"(\d)\s*ç‚¹\s*:\s*([\d\.]+)\s*%"
    
    try:
        matches = re.findall(pattern, response)
        
        if not matches:
            logger.warning(f"å¿œç­”ã‹ã‚‰ç¢ºç‡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å¿œç­”: '{response}'")
            return [1/6] * 6

        # æŠ½å‡ºã—ãŸç¢ºç‡ã‚’æ ¼ç´ã™ã‚‹è¾æ›¸ã‚’åˆæœŸåŒ–
        probabilities_dict = {i: 0.0 for i in range(6)}
        
        for score_str, prob_str in matches:
            score = int(score_str)
            prob = float(prob_str)
            if 0 <= score <= 5:
                probabilities_dict[score] = prob / 100.0  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆã‚’å°æ•°ã«å¤‰æ›

        # 0ç‚¹ã‹ã‚‰5ç‚¹ã®ãƒªã‚¹ãƒˆå½¢å¼ã«å¤‰æ›
        probabilities = [probabilities_dict[i] for i in range(6)]

        # åˆè¨ˆãŒ0ã€ã¾ãŸã¯åˆè¨ˆãŒæ¥µç«¯ã«ãšã‚Œã¦ã„ã‚‹å ´åˆã¯æ­£è¦åŒ–ã™ã‚‹
        total_prob = sum(probabilities)
        if total_prob <= 0:
            logger.warning(f"æŠ½å‡ºã—ãŸç¢ºç‡ã®åˆè¨ˆãŒ0ã§ã™ã€‚å‡ç­‰åˆ†å¸ƒã‚’è¿”ã—ã¾ã™ã€‚æŠ½å‡ºçµæœ: {probabilities}")
            return [1/6] * 6
        
        # åˆè¨ˆãŒ1ã«ãªã‚‹ã‚ˆã†ã«æ­£è¦åŒ–ï¼ˆLLMã®è¨ˆç®—ãƒŸã‚¹ã‚’è£œæ­£ï¼‰
        probabilities = [p / total_prob for p in probabilities]
        
        return probabilities

    except Exception as e:
        logger.error(f"ç¢ºç‡ã®ãƒ‘ãƒ¼ã‚¹ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\nå¿œç­”: '{response}'")
        return [1/6] * 6

def evaluate_turn_on_items(turn_list: list, review: str, llm_pipeline) -> Dict[str, List[float]]:
    """
    ã‚¿ãƒ¼ãƒ³ã®å„è©•ä¾¡é …ç›®ã«ã¤ã„ã¦ç¢ºç‡åˆ†å¸ƒã‚’è¨ˆç®—
    Args:
        turn_list: ã‚¿ãƒ¼ãƒ³ã®è¦ç´ ãƒªã‚¹ãƒˆ [{'role': 'counselor', 'utterance': '...'}, ...]
        review: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è©•ä¾¡
        llm_pipeline: LLMãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    Returns:
        å„è©•ä¾¡é …ç›®ã®ç¢ºç‡åˆ†å¸ƒè¾æ›¸
    """
    evaluation_probabilities = {}
    
    for item in EVALUATION_ITEMS:
        probabilities = calculate_item_probabilities(turn_list, item, review, llm_pipeline)
        evaluation_probabilities[item] = probabilities
    
    return evaluation_probabilities

def calculate_item_probabilities(turn_list: list, item: str, review: str, llm_pipeline) -> List[float]:
    """
    ç‰¹å®šã®è©•ä¾¡é …ç›®ã«ã¤ã„ã¦ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ç¢ºç‡åˆ†å¸ƒã‚’è¨ˆç®—ï¼ˆLLMãƒ™ãƒ¼ã‚¹ï¼‰
    
    Args:
        turn_list: ã‚¿ãƒ¼ãƒ³ã®è¦ç´ ãƒªã‚¹ãƒˆ
        item: è©•ä¾¡é …ç›®å
        review: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è©•ä¾¡
        llm_pipeline: LLMãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    
    Returns:
        ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ç¢ºç‡åˆ†å¸ƒ [p0, p1, p2, p3, p4, p5]
    """
    # ã‚¿ãƒ¼ãƒ³ãƒªã‚¹ãƒˆã‹ã‚‰ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
    conversation_text = ""
    for turn in turn_list:
        role = turn.get('role', 'unknown')
        utterance = turn.get('utterance', '')
        conversation_text += f"{role}: {utterance}\n"
    
    # LLMã‚’ä½¿ç”¨ã—ãŸç¢ºç‡åˆ†å¸ƒå–å¾—
    # llm_pipelineã‹ã‚‰tokenizerã¨modelã‚’å–å¾—
    tokenizer = llm_pipeline.tokenizer
    model = llm_pipeline.model
    
    probabilities = call_llm_for_probability_distribution(tokenizer, model, conversation_text)
    return probabilities

def create_emotion_prompt(dialogue: str, review: str, llm_pipeline) -> str:
    """æ„Ÿæƒ…è©•ä¾¡ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ"""
    try:
        from .turn_segmentation import segment_turns, create_turn_text, create_turn_list
    except ImportError:
        from turn_segmentation import segment_turns, create_turn_text, create_turn_list
    
    try:
        from .data_processing import calculate_weighted_average_probabilities, probability_to_expected_score
    except ImportError:
        from data_processing import calculate_weighted_average_probabilities, probability_to_expected_score
    
    if isinstance(dialogue, dict) and 'dialogue' in dialogue:
        # ã‚¿ãƒ¼ãƒ³åˆ†å‰²ã‚’å®Ÿè¡Œ
        turns = dialogue['dialogue']
        counselor_turns, client_turns, max_turns = segment_turns(turns)
        
        # ã‚¿ãƒ¼ãƒ³ã”ã¨ã®è¦ç´ ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        turn_list = create_turn_list(counselor_turns, client_turns, max_turns)
        turn_evaluations = []
        
        # å„ã‚¿ãƒ¼ãƒ³ã®17é …ç›®è©•ä¾¡ã‚’è¨ˆç®—
        for i, current_turn in enumerate(turn_list):
            # 17é …ç›®ã®ç¢ºç‡åˆ†å¸ƒã‚’è¨ˆç®—
            evaluation_probabilities = evaluate_turn_on_items(current_turn, review, llm_pipeline)
            turn_evaluations.append(evaluation_probabilities)
            
            print(f"ã‚¿ãƒ¼ãƒ³ {i+1} ã®è©•ä¾¡:")
            print(f"  17é …ç›®ç¢ºç‡åˆ†å¸ƒ:")
            for item, probabilities in evaluation_probabilities.items():
                expected_score = probability_to_expected_score(probabilities)
                print(f"    {item}: æœŸå¾…å€¤ {expected_score:.2f} (ç¢ºç‡åˆ†å¸ƒ: {probabilities})")
            print()
        
        # turn_segmentationãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®create_turn_textã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ
        conversation_text = create_turn_text(counselor_turns, client_turns, max_turns)
        
    else:
        conversation_text = str(dialogue)
        turn_evaluations = []
    
    # 17é …ç›®ãã‚Œãã‚Œã«ã¤ã„ã¦ç¢ºç‡åˆ†å¸ƒã®åŠ é‡å¹³å‡ã‚’è¨ˆç®—
    item_weighted_probabilities = {}
    for item in EVALUATION_ITEMS:
        item_probabilities = []
        for turn_eval in turn_evaluations:
            item_probabilities.append(turn_eval.get(item, [0.0, 0.0, 0.1, 0.8, 0.1, 0.0]))
        
        if item_probabilities:
            weighted_probs = calculate_weighted_average_probabilities(item_probabilities)
            item_weighted_probabilities[item] = weighted_probs
        else:
            item_weighted_probabilities[item] = [0.0, 0.0, 0.1, 0.8, 0.1, 0.0]
    
    # 17é …ç›®ã®è©•ä¾¡ç¢ºç‡åˆ†å¸ƒã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å«ã‚ã‚‹
    evaluation_prompt = ""
    for i, turn_eval in enumerate(turn_evaluations):
        evaluation_prompt += f"\n--- ã‚¿ãƒ¼ãƒ³ {i+1} ã®17é …ç›®ç¢ºç‡åˆ†å¸ƒ ---\n"
        for item, probabilities in turn_eval.items():
            expected_score = probability_to_expected_score(probabilities)
            evaluation_prompt += f"{item}: æœŸå¾…å€¤ {expected_score:.2f} (ç¢ºç‡: {probabilities})\n"
    
    # 17é …ç›®ã®åŠ é‡å¹³å‡ç¢ºç‡åˆ†å¸ƒã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å«ã‚ã‚‹
    weighted_averages_prompt = ""
    for item, probabilities in item_weighted_probabilities.items():
        expected_score = probability_to_expected_score(probabilities)
        weighted_averages_prompt += f"{item}: æœŸå¾…å€¤ {expected_score:.2f} (ç¢ºç‡: {probabilities})\n"
    
    # çµ±ä¸€ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ï¼ˆå…¨é …ç›®è©•ä¾¡ç”¨ï¼‰
    conversation_text = ""
    for turn in turn_list:
        role = turn.get('role', 'unknown')
        utterance = turn.get('utterance', '')
        conversation_text += f"{role}: {utterance}\n"
    
    prompt = create_unified_evaluation_prompt(conversation_text, 0)
    
    # è©•ä¾¡çµæœã‚’è¿½åŠ 
    prompt += f"\n\nè©•ä¾¡çµæœ:\n{evaluation_prompt}\n\nåŠ é‡å¹³å‡çµæœ:\n{weighted_averages_prompt}"
    
    return prompt 