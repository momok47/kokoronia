#!/usr/bin/env python3
"""
ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ä¼šè©±ã®è©•ä¾¡ç‚¹æ•°ã‚’äºˆæ¸¬ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€ã‚«ã‚¦ãƒ³ã‚»ãƒªãƒ³ã‚°å¯¾è©±ã«å¯¾ã™ã‚‹æº€è¶³åº¦è©•ä¾¡ç‚¹æ•°ï¼ˆ0-5ç‚¹ï¼‰ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚

ä½¿ç”¨æ–¹æ³•:
1. ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ä¸€è¦§è¡¨ç¤º:
   python use_saved_model.py --list

2. ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆã®è©•ä¾¡ç‚¹æ•°äºˆæ¸¬:
   python use_saved_model.py --model-id ft:gpt-4o-mini-2024-07-18:personal::CAJ6PxFB --evaluate-conversation "å¯¾è©±ãƒ†ã‚­ã‚¹ãƒˆ" --evaluation-item "è´ã„ã¦ã‚‚ã‚‰ãˆãŸã€ã‚ã‹ã£ã¦ã‚‚ã‚‰ãˆãŸã¨æ„Ÿã˜ãŸ"

3. ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰:
   python use_saved_model.py --interactive --model-id ft:gpt-4o-mini-2024-07-18:personal::CAJ6PxFB

4. æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ã§è©•ä¾¡:
   python use_saved_model.py --use-latest --evaluate-conversation "å¯¾è©±ãƒ†ã‚­ã‚¹ãƒˆ"

5. è¤‡æ•°è©•ä¾¡é …ç›®ã§ã®ä¸€æ‹¬è©•ä¾¡:
   python use_saved_model.py --comprehensive-evaluation --model-id ft:gpt-4o-mini-2024-07-18:personal::CAJ6PxFB --conversation-file "conversation.txt"
"""

import os
import argparse
import logging
from pathlib import Path
from dotenv import load_dotenv

from openai_sft import OpenAISFT
import re
import json

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# è©•ä¾¡é …ç›®ãƒªã‚¹ãƒˆ
EVALUATION_ITEMS = [
    "è´ã„ã¦ã‚‚ã‚‰ãˆãŸã€ã‚ã‹ã£ã¦ã‚‚ã‚‰ãˆãŸã¨æ„Ÿã˜ãŸ",
    "å°Šé‡ã•ã‚ŒãŸã¨æ„Ÿã˜ãŸ", 
    "æ–°ã—ã„æ°—ã¥ãã‚„ä½“é¨“ãŒã‚ã£ãŸ",
    "å¸Œæœ›ã‚„æœŸå¾…ã‚’æ„Ÿã˜ã‚‰ã‚ŒãŸ",
    "å–ã‚Šçµ„ã¿ãŸã‹ã£ãŸã“ã¨ã‚’æ‰±ãˆãŸ",
    "ä¸€ç·’ã«è€ƒãˆãªãŒã‚‰å–ã‚Šçµ„ã‚ãŸ",
    "ã‚„ã‚Šã¨ã‚Šã®ãƒªã‚ºãƒ ãŒã‚ã£ã¦ã„ãŸ",
    "å±…å¿ƒåœ°ã®ã‚ˆã„ã‚„ã‚Šã¨ã‚Šã ã£ãŸ",
    "å…¨ä½“ã¨ã—ã¦é©åˆ‡ã§ã‚ˆã‹ã£ãŸ",
    "ä»Šå›ã®ç›¸è«‡ã¯ä¾¡å€¤ãŒã‚ã£ãŸ",
    "ç›¸è«‡é–‹å§‹ã®å††æ»‘ã•",
    "ç›¸è«‡çµ‚äº†ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ï¼ˆä¸å¿…è¦ã«è´ãã™ãã¦ã„ãªã„ã‹ï¼‰ã€å††æ»‘ã•",
    "å—å®¹ãƒ»å…±æ„Ÿ",
    "è‚¯å®šãƒ»æ‰¿èª", 
    "çš„ç¢ºãªè³ªå•ã«ã‚ˆã‚‹ä¼šè©±ã®ä¿ƒé€²",
    "è¦ç´„",
    "å•é¡Œã®æ˜ç¢ºåŒ–",
    "ã“ã®ç›¸è«‡ã§ã®ç›®æ¨™ã®æ˜ç¢ºåŒ–",
    "æ¬¡ã®è¡Œå‹•ã«ã¤ãªãŒã‚‹ææ¡ˆ",
    "å‹‡æ°—ã¥ã‘ãƒ»å¸Œæœ›ã®å–šèµ·"
]

def create_evaluation_prompt(conversation_text: str, evaluation_item: str) -> str:
    """
    è©•ä¾¡ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
    
    Args:
        conversation_text: å¯¾è©±ãƒ†ã‚­ã‚¹ãƒˆ
        evaluation_item: è©•ä¾¡é …ç›®
        
    Returns:
        è©•ä¾¡ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    """
    prompt = f"""### æŒ‡ç¤º
ä»¥ä¸‹ã®å¯¾è©±ã«ã¤ã„ã¦ã€Œ{evaluation_item}ã€ã®æº€è¶³åº¦ã‚’ç›¸è«‡è€…ã®è¦–ç‚¹ã§0ï½5ç‚¹ã§è©•ä¾¡ã—ã€å„ç‚¹æ•°ã®ç¢ºç‡ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

### å¯¾è©±
{conversation_text}

### å‡ºåŠ›å½¢å¼ï¼ˆæ•°å€¤ã®ã¿ï¼‰
0ç‚¹: XX%
1ç‚¹: XX%
2ç‚¹: XX%
3ç‚¹: XX%
4ç‚¹: XX%
5ç‚¹: XX%"""
    
    return prompt

def parse_evaluation_response(response: str) -> dict:
    """
    è©•ä¾¡å¿œç­”ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ç‚¹æ•°ã¨ç¢ºç‡ã‚’æŠ½å‡º
    
    Args:
        response: ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”
        
    Returns:
        ç‚¹æ•°ã¨ç¢ºç‡ã®è¾æ›¸
    """
    scores = {}
    lines = response.strip().split('\n')
    
    for line in lines:
        # ã€ŒXç‚¹: XX%ã€ã®å½¢å¼ã‚’æŠ½å‡º
        match = re.search(r'(\d+)ç‚¹:\s*(\d+)%', line)
        if match:
            score = int(match.group(1))
            probability = int(match.group(2))
            scores[score] = probability
    
    return scores

def calculate_expected_score(scores: dict) -> float:
    """
    æœŸå¾…å€¤ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    
    Args:
        scores: ç‚¹æ•°ã¨ç¢ºç‡ã®è¾æ›¸
        
    Returns:
        æœŸå¾…å€¤ã‚¹ã‚³ã‚¢
    """
    if not scores:
        return 0.0
    
    total_probability = sum(scores.values())
    if total_probability == 0:
        return 0.0
    
    expected_score = sum(score * (prob / total_probability) for score, prob in scores.items())
    return expected_score

def evaluate_conversation(sft: OpenAISFT, model_id: str, conversation_text: str, evaluation_item: str = None):
    """
    ä¼šè©±ã®è©•ä¾¡ç‚¹æ•°ã‚’äºˆæ¸¬
    
    Args:
        sft: OpenAISFTã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        model_id: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ID
        conversation_text: è©•ä¾¡ã™ã‚‹å¯¾è©±ãƒ†ã‚­ã‚¹ãƒˆ
        evaluation_item: è©•ä¾¡é …ç›®ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé …ç›®ã‚’ä½¿ç”¨ï¼‰
    """
    if evaluation_item is None:
        evaluation_item = "è´ã„ã¦ã‚‚ã‚‰ãˆãŸã€ã‚ã‹ã£ã¦ã‚‚ã‚‰ãˆãŸã¨æ„Ÿã˜ãŸ"
    
    logger.info(f"å¯¾è©±è©•ä¾¡å®Ÿè¡Œ: {model_id}")
    logger.info(f"è©•ä¾¡é …ç›®: {evaluation_item}")
    
    # è©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
    prompt = create_evaluation_prompt(conversation_text, evaluation_item)
    
    try:
        # ãƒ¢ãƒ‡ãƒ«ã«é€ä¿¡
        from openai import OpenAI
        client = OpenAI(api_key=sft.api_key)
        
        response = client.chat.completions.create(
            model=model_id,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=300,
            temperature=0.3  # è©•ä¾¡ã§ã¯ä¸€è²«æ€§ã‚’é‡è¦–ã—ã¦ä½ã‚ã«è¨­å®š
        )
        
        response_text = response.choices[0].message.content
        
        # å¿œç­”ã‚’ãƒ‘ãƒ¼ã‚¹
        scores = parse_evaluation_response(response_text)
        expected_score = calculate_expected_score(scores)
        
        # çµæœè¡¨ç¤º
        print(f"\n=== å¯¾è©±è©•ä¾¡çµæœ ===")
        print(f"è©•ä¾¡é …ç›®: {evaluation_item}")
        print(f"æœŸå¾…å€¤ã‚¹ã‚³ã‚¢: {expected_score:.2f}/5.0")
        print(f"\nç¢ºç‡åˆ†å¸ƒ:")
        for score in range(6):
            prob = scores.get(score, 0)
            print(f"  {score}ç‚¹: {prob:2d}%")
        
        print(f"\nç”Ÿã®å¿œç­”:")
        print(response_text)
        
        return {
            'evaluation_item': evaluation_item,
            'expected_score': expected_score,
            'probability_distribution': scores,
            'raw_response': response_text
        }
        
    except Exception as e:
        logger.error(f"è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def comprehensive_evaluation(sft: OpenAISFT, model_id: str, conversation_text: str):
    """
    è¤‡æ•°ã®è©•ä¾¡é …ç›®ã§åŒ…æ‹¬çš„è©•ä¾¡ã‚’å®Ÿè¡Œ
    
    Args:
        sft: OpenAISFTã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        model_id: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ID
        conversation_text: è©•ä¾¡ã™ã‚‹å¯¾è©±ãƒ†ã‚­ã‚¹ãƒˆ
    """
    logger.info(f"åŒ…æ‹¬çš„è©•ä¾¡å®Ÿè¡Œ: {model_id}")
    logger.info(f"è©•ä¾¡é …ç›®æ•°: {len(EVALUATION_ITEMS)}")
    
    results = {}
    
    print(f"\n=== åŒ…æ‹¬çš„å¯¾è©±è©•ä¾¡ ===")
    print(f"ãƒ¢ãƒ‡ãƒ«: {model_id}")
    print(f"è©•ä¾¡é …ç›®æ•°: {len(EVALUATION_ITEMS)}")
    print("=" * 60)
    
    for i, item in enumerate(EVALUATION_ITEMS, 1):
        print(f"\n[{i}/{len(EVALUATION_ITEMS)}] {item}")
        
        result = evaluate_conversation(sft, model_id, conversation_text, item)
        if result:
            results[item] = result
            print(f"æœŸå¾…å€¤: {result['expected_score']:.2f}/5.0")
        else:
            print("è©•ä¾¡å¤±æ•—")
    
    # ç·åˆçµæœè¡¨ç¤º
    if results:
        print(f"\n=== ç·åˆçµæœ ===")
        total_score = sum(r['expected_score'] for r in results.values())
        average_score = total_score / len(results)
        print(f"å¹³å‡ã‚¹ã‚³ã‚¢: {average_score:.2f}/5.0")
        
        # ä¸Šä½ãƒ»ä¸‹ä½é …ç›®
        sorted_results = sorted(results.items(), key=lambda x: x[1]['expected_score'], reverse=True)
        
        print(f"\nğŸ† é«˜è©•ä¾¡é …ç›® TOP3:")
        for item, result in sorted_results[:3]:
            print(f"  {result['expected_score']:.2f} - {item}")
        
        print(f"\nâš ï¸ æ”¹å–„é …ç›® TOP3:")
        for item, result in sorted_results[-3:]:
            print(f"  {result['expected_score']:.2f} - {item}")
    
    return results

def interactive_evaluation_mode(sft: OpenAISFT, model_id: str):
    """
    ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰
    
    Args:
        sft: OpenAISFTã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹  
        model_id: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ID
    """
    logger.info(f"ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰é–‹å§‹: {model_id}")
    logger.info("å¯¾è©±ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆ'quit'ã¾ãŸã¯'exit'ã§çµ‚äº†ï¼‰")
    
    while True:
        try:
            print(f"\n{'='*50}")
            print("å¯¾è©±ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:")
            print("ï¼ˆè¤‡æ•°è¡Œã®å ´åˆã¯ã€æœ€å¾Œã«ç©ºè¡Œã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼‰")
            
            lines = []
            while True:
                line = input()
                if line.strip() == "":
                    break
                lines.append(line)
            
            conversation_text = '\n'.join(lines).strip()
            
            if conversation_text.lower() in ['quit', 'exit', 'q']:
                logger.info("è©•ä¾¡ã‚’çµ‚äº†ã—ã¾ã™")
                break
                
            if not conversation_text:
                continue
            
            # è©•ä¾¡é …ç›®é¸æŠ
            print(f"\nè©•ä¾¡é …ç›®ã‚’é¸æŠã—ã¦ãã ã•ã„:")
            print("0. å…¨é …ç›®ã§åŒ…æ‹¬è©•ä¾¡")
            for i, item in enumerate(EVALUATION_ITEMS[:10], 1):  # æœ€åˆã®10é …ç›®ã‚’è¡¨ç¤º
                print(f"{i}. {item}")
            print("11. ãã®ä»–ï¼ˆã‚«ã‚¹ã‚¿ãƒ é …ç›®ï¼‰")
            
            choice = input("\nç•ªå·ã‚’å…¥åŠ› (0-11): ").strip()
            
            if choice == "0":
                # åŒ…æ‹¬è©•ä¾¡
                comprehensive_evaluation(sft, model_id, conversation_text)
            elif choice.isdigit() and 1 <= int(choice) <= 10:
                # ç‰¹å®šé …ç›®è©•ä¾¡
                item_index = int(choice) - 1
                evaluation_item = EVALUATION_ITEMS[item_index]
                evaluate_conversation(sft, model_id, conversation_text, evaluation_item)
            elif choice == "11":
                # ã‚«ã‚¹ã‚¿ãƒ é …ç›®
                custom_item = input("ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡é …ç›®ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
                if custom_item:
                    evaluate_conversation(sft, model_id, conversation_text, custom_item)
            else:
                print("ç„¡åŠ¹ãªé¸æŠã§ã™")
                
        except KeyboardInterrupt:
            logger.info("\nè©•ä¾¡ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
            break
        except Exception as e:
            logger.error(f"è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")

def load_conversation_from_file(file_path: str) -> str:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿
    
    Args:
        file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        
    Returns:
        ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆ
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read().strip()
    except Exception as e:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return ""

def main():
    parser = argparse.ArgumentParser(description='ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§å¯¾è©±è©•ä¾¡')
    
    # å‹•ä½œãƒ¢ãƒ¼ãƒ‰
    parser.add_argument('--list', action='store_true', help='ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ä¸€è¦§è¡¨ç¤º')
    parser.add_argument('--interactive', action='store_true', help='ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰')
    parser.add_argument('--use-latest', action='store_true', help='æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•é¸æŠ')
    parser.add_argument('--comprehensive-evaluation', action='store_true', help='å…¨è©•ä¾¡é …ç›®ã§ã®åŒ…æ‹¬è©•ä¾¡')
    
    # ãƒ¢ãƒ‡ãƒ«æŒ‡å®š
    parser.add_argument('--model-id', type=str, help='ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ID')
    parser.add_argument('--model-file', type=str, help='ãƒ¢ãƒ‡ãƒ«æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
    
    # è©•ä¾¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument('--evaluate-conversation', type=str, help='è©•ä¾¡ã™ã‚‹å¯¾è©±ãƒ†ã‚­ã‚¹ãƒˆ')
    parser.add_argument('--conversation-file', type=str, help='å¯¾è©±ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
    parser.add_argument('--evaluation-item', type=str, help='è©•ä¾¡é …ç›®ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰')
    
    # APIè¨­å®š
    parser.add_argument('--api-key', type=str, help='OpenAI APIã‚­ãƒ¼')
    
    args = parser.parse_args()
    
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    project_root = Path(__file__).parent.parent.parent.parent
    env_path = project_root / ".env"
    if env_path.exists():
        load_dotenv(env_path)
        logger.info(f".envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {env_path}")
    
    # APIã‚­ãƒ¼ã®ç¢ºèª
    api_key = args.api_key or os.getenv("OPENAI_API_KEY")
    if not api_key:
        logger.error("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        logger.error(".envãƒ•ã‚¡ã‚¤ãƒ«ã«OPENAI_API_KEY=your-api-key-here ã‚’è¨­å®šã™ã‚‹ã‹ã€")
        logger.error("--api-key ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")
        return 1
    
    try:
        # SFTã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
        sft = OpenAISFT(api_key=api_key)
        
        if args.list:
            # ãƒ¢ãƒ‡ãƒ«ä¸€è¦§è¡¨ç¤º
            logger.info("=== ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ ===")
            models = sft.list_saved_models()
            
            if not models:
                logger.info("ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
                return 0
            
            for i, model in enumerate(models, 1):
                print(f"{i}. {model['model_id']}")
                print(f"   ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {model['timestamp']}")
                print(f"   å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {model['training_params']}")
                print(f"   ãƒ•ã‚¡ã‚¤ãƒ«: {model['file_path']}")
                print()
            
            return 0
        
        # ãƒ¢ãƒ‡ãƒ«IDã®æ±ºå®š
        model_id = None
        
        if args.use_latest:
            # æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•é¸æŠ
            models = sft.list_saved_models()
            if models:
                model_id = models[0]['model_id']  # æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«
                logger.info(f"æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ: {model_id}")
            else:
                logger.error("ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
                return 1
                
        elif args.model_file:
            # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
            model_info = sft.load_model_from_file(args.model_file)
            model_id = model_info['model_id']
            
        elif args.model_id:
            # ç›´æ¥æŒ‡å®š
            model_id = args.model_id
            
        else:
            logger.error("ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ (--model-id, --model-file, ã¾ãŸã¯ --use-latest)")
            return 1
        
        # å¯¾è©±ãƒ†ã‚­ã‚¹ãƒˆã®å–å¾—
        conversation_text = None
        if args.evaluate_conversation:
            conversation_text = args.evaluate_conversation
        elif args.conversation_file:
            conversation_text = load_conversation_from_file(args.conversation_file)
            if not conversation_text:
                logger.error("ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å¯¾è©±ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ")
                return 1
        
        if args.interactive:
            # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰
            interactive_evaluation_mode(sft, model_id)
            
        elif args.comprehensive_evaluation:
            # åŒ…æ‹¬çš„è©•ä¾¡
            if not conversation_text:
                logger.error("--evaluate-conversation ã¾ãŸã¯ --conversation-file ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
                return 1
            comprehensive_evaluation(sft, model_id, conversation_text)
            
        elif conversation_text:
            # å˜ä¸€é …ç›®è©•ä¾¡
            evaluate_conversation(sft, model_id, conversation_text, args.evaluation_item)
            
        else:
            logger.error("--interactive, --comprehensive-evaluation, ã¾ãŸã¯ --evaluate-conversation ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
            return 1
        
        return 0
        
    except Exception as e:
        logger.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return 1

if __name__ == "__main__":
    exit(main())
