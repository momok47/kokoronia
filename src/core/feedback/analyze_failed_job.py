#!/usr/bin/env python3
"""
å¤±æ•—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã®è©³ç´°ã‚’åˆ†æã—ã€å•é¡Œã‚’å›é¿ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import json
import logging
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI
from datetime import datetime
from typing import List, Dict, Any, Tuple

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def validate_json_format(data: List[Dict[str, Any]]) -> Tuple[bool, List[int], List[str]]:
    """
    JSONãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ã‚’æ¤œè¨¼
    
    Args:
        data: æ¤œè¨¼ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆ
        
    Returns:
        (is_valid, invalid_indices, error_messages)
    """
    is_valid = True
    invalid_indices = []
    error_messages = []
    
    for i, item in enumerate(data):
        try:
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å­˜åœ¨ç¢ºèª
            if not isinstance(item, dict):
                raise ValueError(f"è¡Œ {i+1}: è¾æ›¸å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
            
            if 'messages' not in item:
                raise ValueError(f"è¡Œ {i+1}: 'messages'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
            
            if not isinstance(item['messages'], list):
                raise ValueError(f"è¡Œ {i+1}: 'messages'ãŒãƒªã‚¹ãƒˆå½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
            
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å½¢å¼ç¢ºèª
            for j, message in enumerate(item['messages']):
                if not isinstance(message, dict):
                    raise ValueError(f"è¡Œ {i+1}, ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ {j+1}: è¾æ›¸å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
                
                if 'role' not in message or 'content' not in message:
                    raise ValueError(f"è¡Œ {i+1}, ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ {j+1}: 'role'ã¾ãŸã¯'content'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
                
                if not isinstance(message['role'], str) or not isinstance(message['content'], str):
                    raise ValueError(f"è¡Œ {i+1}, ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ {j+1}: 'role'ã¾ãŸã¯'content'ãŒæ–‡å­—åˆ—ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
            
            # JSONã¨ã—ã¦ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ã‹ãƒ†ã‚¹ãƒˆ
            json.dumps(item, ensure_ascii=False)
            
        except Exception as e:
            is_valid = False
            invalid_indices.append(i)
            error_messages.append(str(e))
    
    return is_valid, invalid_indices, error_messages

def fix_json_data(data: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], int]:
    """
    ç ´æã—ãŸJSONãƒ‡ãƒ¼ã‚¿ã‚’ä¿®å¾©
    
    Args:
        data: ä¿®å¾©ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆ
        
    Returns:
        (ä¿®å¾©ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿, ä¿®å¾©ã•ã‚ŒãŸè¡Œæ•°)
    """
    fixed_data = []
    fixed_count = 0
    
    for i, item in enumerate(data):
        try:
            # åŸºæœ¬çš„ãªå½¢å¼ãƒã‚§ãƒƒã‚¯
            if not isinstance(item, dict):
                logger.warning(f"è¡Œ {i+1}: è¾æ›¸å½¢å¼ã§ã¯ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                continue
            
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®è£œå®Œ
            if 'messages' not in item:
                logger.warning(f"è¡Œ {i+1}: 'messages'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ç©ºã®ãƒªã‚¹ãƒˆã§è£œå®Œ")
                item['messages'] = []
            
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ä¿®å¾©
            if isinstance(item['messages'], list):
                fixed_messages = []
                for j, message in enumerate(item['messages']):
                    if isinstance(message, dict) and 'role' in message and 'content' in message:
                        if isinstance(message['role'], str) and isinstance(message['content'], str):
                            fixed_messages.append(message)
                        else:
                            # å‹ã‚’ä¿®æ­£
                            fixed_message = {
                                'role': str(message.get('role', 'user')),
                                'content': str(message.get('content', ''))
                            }
                            fixed_messages.append(fixed_message)
                            fixed_count += 1
                    else:
                        # ç„¡åŠ¹ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚¹ã‚­ãƒƒãƒ—
                        logger.warning(f"è¡Œ {i+1}, ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ {j+1}: ç„¡åŠ¹ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                
                item['messages'] = fixed_messages
            else:
                item['messages'] = []
                fixed_count += 1
            
            # JSONã¨ã—ã¦ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ã‹ãƒ†ã‚¹ãƒˆ
            json.dumps(item, ensure_ascii=False)
            fixed_data.append(item)
            
        except Exception as e:
            logger.error(f"è¡Œ {i+1}: ä¿®å¾©ä¸å¯èƒ½ - {e}")
            continue
    
    return fixed_data, fixed_count

def create_safe_training_file(input_file: Path, output_file: Path) -> Tuple[bool, int, int]:
    """
    å®‰å…¨ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    
    Args:
        input_file: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        
    Returns:
        (æˆåŠŸãƒ•ãƒ©ã‚°, ç·è¡Œæ•°, æœ‰åŠ¹è¡Œæ•°)
    """
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        # JSONãƒ‡ãƒ¼ã‚¿ã‚’è§£æ
        data = []
        for line in lines:
            try:
                item = json.loads(line.strip())
                data.append(item)
            except json.JSONDecodeError as e:
                logger.warning(f"JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        if not data:
            logger.error("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False, 0, 0
        
        # ãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚’æ¤œè¨¼
        is_valid, invalid_indices, error_messages = validate_json_format(data)
        
        if not is_valid:
            logger.warning(f"ãƒ‡ãƒ¼ã‚¿å½¢å¼ã«å•é¡ŒãŒã‚ã‚Šã¾ã™: {len(invalid_indices)}è¡Œ")
            for idx, error in zip(invalid_indices, error_messages):
                logger.warning(f"  è¡Œ {idx+1}: {error}")
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’ä¿®å¾©
            logger.info("ãƒ‡ãƒ¼ã‚¿ã®ä¿®å¾©ã‚’è©¦è¡Œä¸­...")
            fixed_data, fixed_count = fix_json_data(data)
            
            if not fixed_data:
                logger.error("ãƒ‡ãƒ¼ã‚¿ã®ä¿®å¾©ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return False, len(data), 0
            
            logger.info(f"ä¿®å¾©å®Œäº†: {fixed_count}è¡Œã‚’ä¿®å¾©")
            data = fixed_data
        
        # å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        with open(output_file, 'w', encoding='utf-8') as f:
            for item in data:
                json.dump(item, f, ensure_ascii=False)
                f.write('\n')
        
        logger.info(f"å®‰å…¨ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ: {output_file}")
        return True, len(lines), len(data)
        
    except Exception as e:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        return False, 0, 0

def create_openai_sft_safe_file(input_file: Path, output_file: Path) -> Tuple[bool, str]:
    """
    OpenAI SFTç”¨ã®å®‰å…¨ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    
    Args:
        input_file: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        
    Returns:
        (æˆåŠŸãƒ•ãƒ©ã‚°, ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸)
    """
    try:
        # å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        success, total_lines, valid_lines = create_safe_training_file(input_file, output_file)
        
        if not success:
            return False, "å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ"
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®ç¢ºèªï¼ˆOpenAIåˆ¶é™: 100MBä»¥ä¸‹ï¼‰
        file_size_mb = output_file.stat().st_size / (1024 * 1024)
        if file_size_mb > 100:
            return False, f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™: {file_size_mb:.2f}MB (åˆ¶é™: 100MB)"
        
        # è¡Œæ•°ã®ç¢ºèªï¼ˆæœ€å°1è¡Œï¼‰
        if valid_lines < 1:
            return False, "æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒ1è¡Œã‚‚ã‚ã‚Šã¾ã›ã‚“"
        
        logger.info(f"OpenAI SFTç”¨ã®å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ: {output_file}")
        logger.info(f"  ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size_mb:.2f}MB")
        logger.info(f"  æœ‰åŠ¹è¡Œæ•°: {valid_lines}")
        
        return True, ""
        
    except Exception as e:
        error_msg = f"ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}"
        logger.error(error_msg)
        return False, error_msg

def validate_openai_sft_format(file_path: Path) -> Tuple[bool, List[str]]:
    """
    OpenAI SFTå½¢å¼ã®æ¤œè¨¼
    
    Args:
        file_path: æ¤œè¨¼ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        
    Returns:
        (æœ‰åŠ¹ãƒ•ãƒ©ã‚°, ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ)
    """
    errors = []
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        if not lines:
            errors.append("ãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™")
            return False, errors
        
        # å„è¡Œã‚’æ¤œè¨¼
        for i, line in enumerate(lines):
            try:
                data = json.loads(line.strip())
                
                # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç¢ºèª
                if not isinstance(data, dict):
                    errors.append(f"è¡Œ {i+1}: è¾æ›¸å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
                    continue
                
                if 'messages' not in data:
                    errors.append(f"è¡Œ {i+1}: 'messages'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
                    continue
                
                if not isinstance(data['messages'], list):
                    errors.append(f"è¡Œ {i+1}: 'messages'ãŒãƒªã‚¹ãƒˆå½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
                    continue
                
                if len(data['messages']) == 0:
                    errors.append(f"è¡Œ {i+1}: 'messages'ãŒç©ºã§ã™")
                    continue
                
                # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å½¢å¼ç¢ºèª
                for j, message in enumerate(data['messages']):
                    if not isinstance(message, dict):
                        errors.append(f"è¡Œ {i+1}, ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ {j+1}: è¾æ›¸å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
                        continue
                    
                    if 'role' not in message:
                        errors.append(f"è¡Œ {i+1}, ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ {j+1}: 'role'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
                        continue
                    
                    if 'content' not in message:
                        errors.append(f"è¡Œ {i+1}, ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ {j+1}: 'content'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
                        continue
                    
                    if not isinstance(message['role'], str):
                        errors.append(f"è¡Œ {i+1}, ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ {j+1}: 'role'ãŒæ–‡å­—åˆ—ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
                        continue
                    
                    if not isinstance(message['content'], str):
                        errors.append(f"è¡Œ {i+1}, ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ {j+1}: 'content'ãŒæ–‡å­—åˆ—ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
                        continue
                    
                    if message['role'] not in ['system', 'user', 'assistant']:
                        errors.append(f"è¡Œ {i+1}, ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ {j+1}: ç„¡åŠ¹ãª'role'å€¤: {message['role']}")
                        continue
                    
                    if len(message['content'].strip()) == 0:
                        errors.append(f"è¡Œ {i+1}, ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ {j+1}: 'content'ãŒç©ºã§ã™")
                        continue
                
            except json.JSONDecodeError as e:
                errors.append(f"è¡Œ {i+1}: JSONè§£æã‚¨ãƒ©ãƒ¼ - {e}")
                continue
        
        return len(errors) == 0, errors
        
    except Exception as e:
        errors.append(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return False, errors

def get_failed_job_info(job_id: str, api_key: str):
    """
    å¤±æ•—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã®è©³ç´°æƒ…å ±ã‚’å–å¾—
    
    Args:
        job_id: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ID
        api_key: OpenAI APIã‚­ãƒ¼
        
    Returns:
        ã‚¸ãƒ§ãƒ–æƒ…å ±
    """
    client = OpenAI(api_key=api_key)
    
    try:
        job = client.fine_tuning.jobs.retrieve(job_id)
        logger.info(f"ã‚¸ãƒ§ãƒ–æƒ…å ±ã‚’å–å¾—: {job_id}")
        return job
    except Exception as e:
        logger.error(f"ã‚¸ãƒ§ãƒ–æƒ…å ±ã®å–å¾—ã«å¤±æ•—: {e}")
        return None

def analyze_failure_cause(job_info):
    """
    å¤±æ•—ã®åŸå› ã‚’åˆ†æ
    
    Args:
        job_info: ã‚¸ãƒ§ãƒ–æƒ…å ±
    """
    print(f"\n=== å¤±æ•—åŸå› ã®åˆ†æ ===")
    
    if hasattr(job_info, 'error') and job_info.error:
        print(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {job_info.error}")
    
    if hasattr(job_info, 'status') and job_info.status == 'failed':
        print(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {job_info.status}")
        
        # å¤±æ•—ã®ä¸€èˆ¬çš„ãªåŸå› ã‚’åˆ†æ
        if hasattr(job_info, 'training_file'):
            print(f"ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {job_info.training_file}")
        
        if hasattr(job_info, 'validation_file'):
            print(f"æ¤œè¨¼ãƒ•ã‚¡ã‚¤ãƒ«: {job_info.validation_file}")
        
        if hasattr(job_info, 'hyperparameters'):
            print(f"ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {job_info.hyperparameters}")
        
        if hasattr(job_info, 'created_at'):
            created_time = datetime.fromtimestamp(job_info.created_at)
            print(f"ä½œæˆæ—¥æ™‚: {created_time}")
        
        if hasattr(job_info, 'finished_at') and job_info.finished_at:
            finished_time = datetime.fromtimestamp(job_info.finished_at)
            print(f"å®Œäº†æ—¥æ™‚: {finished_time}")
            duration = finished_time - created_time
            print(f"å®Ÿè¡Œæ™‚é–“: {duration}")
        else:
            print("å®Œäº†æ—¥æ™‚: æœªå®Œäº†")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®åˆ†æ
    analyze_file_sizes()
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ç¢ºèª
    analyze_training_file_content()

def analyze_file_sizes():
    """
    ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚µã‚¤ã‚ºã‚’åˆ†æ
    """
    print(f"\n=== ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®åˆ†æ ===")
    
    output_dir = Path("openai_sft_outputs")
    if not output_dir.exists():
        print("å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’æ™‚ç³»åˆ—ã§è¡¨ç¤º
    files = []
    for file_path in output_dir.glob("*.jsonl"):
        if file_path.name.startswith(("train_", "test_", "valid_")):
            stat = file_path.stat()
            created_time = datetime.fromtimestamp(stat.st_mtime)
            size_mb = stat.st_size / (1024 * 1024)
            files.append((created_time, file_path.name, size_mb))
    
    # æ™‚ç³»åˆ—é †ã«ã‚½ãƒ¼ãƒˆ
    files.sort(key=lambda x: x[0])
    
    print("ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®æ¨ç§»:")
    for created_time, filename, size_mb in files:
        print(f"  {created_time.strftime('%H:%M:%S')} - {filename}: {size_mb:.2f} MB")

def analyze_training_file_content():
    """
    ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’åˆ†æ
    """
    print(f"\n=== ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹åˆ†æ ===")
    
    output_dir = Path("openai_sft_outputs")
    if not output_dir.exists():
        print("å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # æœ€æ–°ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
    train_files = list(output_dir.glob("train_*.jsonl"))
    if not train_files:
        print("ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    latest_train_file = max(train_files, key=lambda x: x.stat().st_mtime)
    print(f"æœ€æ–°ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {latest_train_file.name}")
    
    try:
        with open(latest_train_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        print(f"ç·è¡Œæ•°: {len(lines)}")
        
        # æœ€åˆã®æ•°è¡Œã‚’ç¢ºèª
        print("\næœ€åˆã®5è¡Œã®å†…å®¹:")
        for i, line in enumerate(lines[:5]):
            try:
                data = json.loads(line.strip())
                print(f"è¡Œ {i+1}: æœ‰åŠ¹ãªJSON âœ“")
                if 'messages' in data:
                    print(f"  ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°: {len(data['messages'])}")
            except json.JSONDecodeError as e:
                print(f"è¡Œ {i+1}: ç„¡åŠ¹ãªJSON âœ— - {e}")
                print(f"  å†…å®¹: {line.strip()[:100]}...")
        
        # ç„¡åŠ¹ãªè¡Œã‚’æ¤œå‡º
        invalid_lines = []
        for i, line in enumerate(lines):
            try:
                json.loads(line.strip())
            except json.JSONDecodeError:
                invalid_lines.append(i + 1)
        
        if invalid_lines:
            print(f"\nâš ï¸  ç„¡åŠ¹ãªJSONè¡Œ: {len(invalid_lines)}è¡Œ")
            print(f"  è¡Œç•ªå·: {invalid_lines[:10]}{'...' if len(invalid_lines) > 10 else ''}")
        else:
            print("\nâœ… ã™ã¹ã¦ã®è¡ŒãŒæœ‰åŠ¹ãªJSONå½¢å¼ã§ã™")
            
    except Exception as e:
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

def get_openai_sft_integration_code():
    """
    OpenAI SFTã‚¯ãƒ©ã‚¹ã«çµ±åˆã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’è¡¨ç¤º
    """
    print("\n" + "="*60)
    print("ğŸ”§ OpenAI SFTã‚¯ãƒ©ã‚¹çµ±åˆç”¨ã‚³ãƒ¼ãƒ‰")
    print("="*60)
    
    integration_code = '''
# OpenAI SFTã‚¯ãƒ©ã‚¹ã«ä»¥ä¸‹ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ ã—ã¦ãã ã•ã„

def create_safe_training_file(self, input_file: str, output_file: str = None) -> Tuple[bool, str]:
    """
    å®‰å…¨ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    
    Args:
        input_file: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
        
    Returns:
        (æˆåŠŸãƒ•ãƒ©ã‚°, ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸)
    """
    if output_file is None:
        input_path = Path(input_file)
        output_file = str(input_path.parent / f"safe_{input_path.name}")
    
    success, error_msg = create_openai_sft_safe_file(Path(input_file), Path(output_file))
    return success, error_msg

def validate_training_file(self, file_path: str) -> Tuple[bool, List[str]]:
    """
    ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ã‚’æ¤œè¨¼
    
    Args:
        file_path: æ¤œè¨¼ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        
    Returns:
        (æœ‰åŠ¹ãƒ•ãƒ©ã‚°, ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ)
    """
    return validate_openai_sft_format(Path(file_path))

def run_safe_fine_tuning(self, training_file: str, **kwargs) -> str:
    """
    å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ
    
    Args:
        training_file: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
    Returns:
        ã‚¸ãƒ§ãƒ–ID
    """
    # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’æ¤œè¨¼
    is_valid, errors = self.validate_training_file(training_file)
    if not is_valid:
        raise ValueError(f"ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ãŒç„¡åŠ¹ã§ã™: {errors[:3]}")
    
    # å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    safe_file = str(Path(training_file).parent / f"safe_{Path(training_file).name}")
    success, error_msg = self.create_safe_training_file(training_file, safe_file)
    
    if not success:
        raise ValueError(f"å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆã«å¤±æ•—: {error_msg}")
    
    # å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ
    logger.info(f"å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ: {safe_file}")
    return self.create_fine_tune_job(safe_file, **kwargs)
'''
    
    print(integration_code)
    print("="*60)

def main():
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    project_root = Path(__file__).parent.parent.parent.parent
    env_path = project_root / ".env"
    if env_path.exists():
        load_dotenv(env_path)
        logger.info(f".envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {env_path}")
    
    # APIã‚­ãƒ¼ã®ç¢ºèª
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        logger.error("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return
    
    # å¤±æ•—ã—ãŸã‚¸ãƒ§ãƒ–ID
    failed_job_id = "ftjob-N6KpXmRlivMYE0sMCKNJqxVz"
    
    print("=== å¤±æ•—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã®åˆ†æ ===")
    print(f"ã‚¸ãƒ§ãƒ–ID: {failed_job_id}")
    
    try:
        # ã‚¸ãƒ§ãƒ–æƒ…å ±ã‚’å–å¾—
        job_info = get_failed_job_info(failed_job_id, api_key)
        if not job_info:
            return
        
        # å¤±æ•—åŸå› ã‚’åˆ†æ
        analyze_failure_cause(job_info)
        
        # æˆåŠŸã—ãŸã‚¸ãƒ§ãƒ–ã¨ã®æ¯”è¼ƒ
        print(f"\n=== æˆåŠŸã—ãŸã‚¸ãƒ§ãƒ–ã¨ã®æ¯”è¼ƒ ===")
        print("æˆåŠŸã—ãŸã‚¸ãƒ§ãƒ–: ftjob-3FLRyCkixK8eFBaxzPNELbTl")
        print("å¤±æ•—ã—ãŸã‚¸ãƒ§ãƒ–: ftjob-N6KpXmRlivMYE0sMCKNJqxVz")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®é•ã„ã‚’å¼·èª¿
        print(f"\nğŸ’¡ æ¨æ¸¬ã•ã‚Œã‚‹å¤±æ•—åŸå› :")
        print("1. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒå¤§ãã™ãã‚‹ï¼ˆ7.3MB vs 0.8MBï¼‰")
        print("2. ãƒ‡ãƒ¼ã‚¿å½¢å¼ã®å•é¡Œï¼ˆinvalid_file_formatï¼‰")
        print("3. OpenAIã®åˆ¶é™ã«å¼•ã£ã‹ã‹ã£ãŸ")
        
        # å®‰å…¨ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
        print(f"\n=== å®‰å…¨ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ ===")
        output_dir = Path("openai_sft_outputs")
        if output_dir.exists():
            train_files = list(output_dir.glob("train_*.jsonl"))
            if train_files:
                latest_train_file = max(train_files, key=lambda x: x.stat().st_mtime)
                safe_file = output_dir / f"safe_{latest_train_file.name}"
                
                print(f"å…ƒãƒ•ã‚¡ã‚¤ãƒ«: {latest_train_file.name}")
                print(f"å®‰å…¨ãƒ•ã‚¡ã‚¤ãƒ«: {safe_file.name}")
                
                success, total_lines, valid_lines = create_safe_training_file(latest_train_file, safe_file)
                
                if success:
                    print(f"âœ… å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆæˆåŠŸ")
                    print(f"  ç·è¡Œæ•°: {total_lines}")
                    print(f"  æœ‰åŠ¹è¡Œæ•°: {valid_lines}")
                    print(f"  ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {safe_file.stat().st_size / (1024 * 1024):.2f} MB")
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®æ¤œè¨¼
                    print(f"\n=== å®‰å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼ ===")
                    with open(safe_file, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                    
                    data = []
                    for line in lines:
                        try:
                            item = json.loads(line.strip())
                            data.append(item)
                        except json.JSONDecodeError:
                            continue
                    
                    is_valid, invalid_indices, error_messages = validate_json_format(data)
                    
                    if is_valid:
                        print("âœ… ã™ã¹ã¦ã®è¡ŒãŒæœ‰åŠ¹ãªJSONå½¢å¼ã§ã™")
                    else:
                        print(f"âš ï¸  {len(invalid_indices)}è¡Œã«å•é¡ŒãŒã‚ã‚Šã¾ã™")
                        for idx, error in zip(invalid_indices[:5], error_messages[:5]):
                            print(f"  è¡Œ {idx+1}: {error}")
                    
                    # OpenAI SFTå½¢å¼ã®æ¤œè¨¼
                    print(f"\n=== OpenAI SFTå½¢å¼ã®æ¤œè¨¼ ===")
                    sft_valid, sft_errors = validate_openai_sft_format(safe_file)
                    
                    if sft_valid:
                        print("âœ… OpenAI SFTå½¢å¼ã¨ã—ã¦æœ‰åŠ¹ã§ã™")
                    else:
                        print(f"âš ï¸  OpenAI SFTå½¢å¼ã«å•é¡ŒãŒã‚ã‚Šã¾ã™: {len(sft_errors)}ä»¶")
                        for error in sft_errors[:5]:
                            print(f"  {error}")
                    
                    # æ¨å¥¨äº‹é …ã®è¡¨ç¤º
                    print(f"\nğŸ’¡ æ¨å¥¨äº‹é …:")
                    if sft_valid:
                        print("1. ã“ã®å®‰å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦SFTã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
                        print("2. å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã¯å•é¡ŒãŒã‚ã‚‹ãŸã‚ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„")
                        print("3. ä»Šå¾Œã®ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆæ™‚ã¯ã€ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§äº‹å‰æ¤œè¨¼ã‚’è¡Œã£ã¦ãã ã•ã„")
                    else:
                        print("1. ãƒ‡ãƒ¼ã‚¿å½¢å¼ã®å•é¡Œã‚’ä¿®æ­£ã—ã¦ã‹ã‚‰å†å®Ÿè¡Œã—ã¦ãã ã•ã„")
                        print("2. ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®roleã¨contentãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
                        print("3. ç©ºã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚„ç„¡åŠ¹ãªå½¢å¼ã‚’é™¤å»ã—ã¦ãã ã•ã„")
                else:
                    print("âŒ å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        # OpenAI SFTã‚¯ãƒ©ã‚¹çµ±åˆç”¨ã‚³ãƒ¼ãƒ‰ã®è¡¨ç¤º
        get_openai_sft_integration_code()
        
    except Exception as e:
        logger.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

if __name__ == "__main__":
    main()
