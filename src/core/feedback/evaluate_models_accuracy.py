#!/usr/bin/env python3
"""
ä»Šå›ä½œæˆã—ãŸ4ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®testãƒ‡ãƒ¼ã‚¿ã§ã®æ­£è§£ç‡ã‚’è¨ˆç®—ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ä¼šè©±ã®å¾—ç‚¹ï¼ˆ0~5ç‚¹ï¼‰ã‚’äºˆæ¸¬ã—ã€æ­£è§£ã®ç‚¹æ•°ã¨æ¯”è¼ƒã™ã‚‹æ©Ÿèƒ½ä»˜ã
"""

import os
import json
import logging
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI
import time
from typing import List, Dict, Any, Tuple
import numpy as np
from datetime import datetime
import pandas as pd
import re

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger(__name__)

class ModelAccuracyEvaluator:
    """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ­£è§£ç‡è©•ä¾¡ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, api_key: str):
        """
        åˆæœŸåŒ–
        
        Args:
            api_key: OpenAI APIã‚­ãƒ¼
        """
        self.client = OpenAI(api_key=api_key)
        # ã©ã®éšå±¤ã‹ã‚‰å®Ÿè¡Œã•ã‚Œã¦ã‚‚æ­£ã—ããƒ‘ã‚¹ã‚’æŒ‡å®šã§ãã‚‹ã‚ˆã†ã«ã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆè‡ªèº«ã®å ´æ‰€ã‚’åŸºæº–ã«ã™ã‚‹
        self.script_dir = Path(__file__).resolve().parent
        self.output_dir = self.script_dir / "openai_sft_outputs"
        logger.info(f"çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š: {self.output_dir}")
        self.output_dir.mkdir(exist_ok=True) # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒãªã‘ã‚Œã°ä½œæˆ

    def _find_latest_results_file(self) -> Path:
        """æœ€æ–°ã®ãƒãƒƒãƒçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¦‹ã¤ã‘ã‚‹"""
        result_files = list(self.output_dir.glob("batch_fine_tuning_results_*.json"))
        if not result_files:
            raise FileNotFoundError(f"ãƒãƒƒãƒçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.output_dir}")
        
        latest_file = max(result_files, key=lambda x: x.stat().st_mtime)
        logger.info(f"ä½¿ç”¨ã™ã‚‹çµæœãƒ•ã‚¡ã‚¤ãƒ«: {latest_file}")
        return latest_file
    
    def load_test_data_and_models(self) -> Tuple[List[str], List[Dict[str, Any]]]:
        """
        æœ€æ–°ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«IDã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
        
        Returns:
            (ãƒ¢ãƒ‡ãƒ«IDã®ãƒªã‚¹ãƒˆ, testãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ)
        """
        latest_results_file = self._find_latest_results_file()
        
        with open(latest_results_file, 'r', encoding='utf-8') as f:
            results_data = json.load(f)

        # ãƒ¢ãƒ‡ãƒ«IDã®èª­ã¿è¾¼ã¿ - ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«å¿œã˜ã¦ä¿®æ­£
        model_ids = []
        if 'batches' in results_data:
            # æ–°ã—ã„å½¢å¼: batchesã‚­ãƒ¼ãŒã‚ã‚‹å ´åˆ
            for batch in results_data['batches']:
                if 'fine_tuned_model' in batch:
                    model_ids.append(batch['fine_tuned_model'])
        elif 'batch_results' in results_data:
            # å¤ã„å½¢å¼: batch_resultsã‚­ãƒ¼ãŒã‚ã‚‹å ´åˆ
            for batch in results_data['batch_results']:
                if batch.get('final_status') == 'succeeded' and 'final_model_id' in batch:
                    model_ids.append(batch['final_model_id'])
        else:
            # ãã®ä»–ã®å½¢å¼ã‚’è©¦ã™
            logger.warning(f"äºˆæœŸã—ãªã„ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã§ã™: {list(results_data.keys())}")
            # ç›´æ¥ãƒ¢ãƒ‡ãƒ«IDãŒå«ã¾ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯
            for key, value in results_data.items():
                if isinstance(value, list) and value:
                    for item in value:
                        if isinstance(item, dict):
                            if 'fine_tuned_model' in item:
                                model_ids.append(item['fine_tuned_model'])
                            elif 'final_model_id' in item:
                                model_ids.append(item['final_model_id'])
        
        if not model_ids:
            raise ValueError(f"çµæœãƒ•ã‚¡ã‚¤ãƒ« {latest_results_file.name} ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«IDã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        logger.info(f"è©•ä¾¡å¯¾è±¡ãƒ¢ãƒ‡ãƒ«æ•°: {len(model_ids)}")
        for i, model_id in enumerate(model_ids):
            logger.info(f"  ãƒ¢ãƒ‡ãƒ« {i+1}: {model_id}")

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾— - è¤‡æ•°ã®æ–¹æ³•ã‚’è©¦ã™
        test_data_path = None
        
        # æ–¹æ³•1: test_data_fileã‚­ãƒ¼ã‹ã‚‰å–å¾—
        if 'test_data_file' in results_data:
            test_data_filename = results_data['test_data_file']
            test_data_path = self.output_dir / test_data_filename
            logger.info(f"test_data_fileã‚­ãƒ¼ã‹ã‚‰å–å¾—: {test_data_filename}")
        
        # æ–¹æ³•2: ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ¨æ¸¬
        if not test_data_path or not test_data_path.exists():
            test_files = list(self.output_dir.glob("test_data_*.jsonl"))
            if test_files:
                # æœ€æ–°ã®testãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
                latest_test_file = max(test_files, key=lambda x: x.stat().st_mtime)
                test_data_path = latest_test_file
                logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ¨æ¸¬: {test_data_path.name}")
        
        # æ–¹æ³•3: ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ¨æ¸¬ï¼ˆã‚ˆã‚ŠæŸ”è»Ÿã«ï¼‰
        if not test_data_path or not test_data_path.exists():
            test_files = list(self.output_dir.glob("*test*data*.jsonl"))
            if test_files:
                # æœ€æ–°ã®testãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
                latest_test_file = max(test_files, key=lambda x: x.stat().st_mtime)
                test_data_path = latest_test_file
                logger.info(f"æŸ”è»Ÿãªæ¤œç´¢ã§ç™ºè¦‹: {test_data_path.name}")
        
        if not test_data_path or not test_data_path.exists():
            raise FileNotFoundError(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
        
        logger.info(f"ä½¿ç”¨ã™ã‚‹testãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: {test_data_path}")

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        test_data = []
        try:
            with open(test_data_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if line:  # ç©ºè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                        try:
                            test_data.append(json.loads(line))
                        except json.JSONDecodeError as e:
                            logger.warning(f"è¡Œ {line_num} ã®JSONè§£æã«å¤±æ•—: {e}")
                            continue
            
            logger.info(f"testãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(test_data)}ã‚µãƒ³ãƒ—ãƒ«")
            
            # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ç¢ºèª
            if test_data:
                sample_keys = list(test_data[0].keys())
                logger.info(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ : {sample_keys}")
                
                # messagesã‚­ãƒ¼ã®å­˜åœ¨ç¢ºèª
                if 'messages' in test_data[0]:
                    first_messages = test_data[0]['messages']
                    if first_messages:
                        logger.info(f"æœ€åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹é€ : {[msg.get('role', 'unknown') for msg in first_messages]}")
        
        except Exception as e:
            logger.error(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            raise
        
        return model_ids, test_data

    def extract_score_from_response(self, response_text: str) -> Tuple[float, str, bool, Dict[int, float]]:
        """
        å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç¢ºç‡åˆ†å¸ƒã¨æœŸå¾…å€¤ã‚’æŠ½å‡º
        """
        logger.debug(f"å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ: {response_text}")
        
        probability_patterns = [
            r'(\d+)ç‚¹\s*[:ï¼š]\s*(\d+(?:\.\d+)?)%',
            r'(\d+)ç‚¹\s+(\d+(?:\.\d+)?)%',
        ]
        
        probabilities = {}
        for pattern in probability_patterns:
            matches = re.findall(pattern, response_text)
            for match in matches:
                point, prob = int(match[0]), float(match[1])
                if 0 <= point <= 5 and point not in probabilities:
                    probabilities[point] = prob
        
        total_probability = sum(probabilities.values())
        
        if len(probabilities) == 6 and abs(total_probability - 100.0) < 10.0:
            expected_value = sum(p * (pr / 100.0) for p, pr in probabilities.items())
            return expected_value, "æŠ½å‡ºæˆåŠŸ", False, probabilities
        else:
            logger.warning(f"ç¢ºç‡åˆ†å¸ƒã®å½¢å¼ãŒä¸æ­£: åˆè¨ˆ={total_probability}%, æ•°={len(probabilities)}")
            return 0.0, "response error", True, {}

    def evaluate_model(self, model_id: str, test_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡"""
        logger.info(f"ãƒ¢ãƒ‡ãƒ« {model_id} ã®è©•ä¾¡ã‚’é–‹å§‹...")
        score_comparison = []

        for i, sample in enumerate(test_data):
            try:
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
                user_message = None
                for msg in sample['messages']:
                    if msg['role'] == 'user':
                        user_message = msg['content']
                        break
                
                if not user_message:
                    logger.warning(f"ã‚µãƒ³ãƒ—ãƒ« {i+1}: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    continue
                
                # ãƒ¢ãƒ‡ãƒ«ã«è³ªå•ã‚’é€ä¿¡ï¼ˆç¢ºç‡åˆ†å¸ƒå½¢å¼ã®å¾—ç‚¹äºˆæ¸¬ç”¨ï¼‰
                score_response = self.client.chat.completions.create(
                    model=model_id,
                    messages=[
                        {"role": "system", "content": "ã‚ãªãŸã¯å¿ƒç†ã‚«ã‚¦ãƒ³ã‚»ãƒªãƒ³ã‚°ã®è©•ä¾¡è€…ã§ã™ã€‚ç›¸è«‡è€…ã®æº€è¶³åº¦ã‚’0ï½5ç‚¹ã§è©•ä¾¡ã—ã€å„ç‚¹æ•°ã®ç¢ºç‡ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"},
                        {"role": "user", "content": f"""### æŒ‡ç¤º
ä»¥ä¸‹ã®å¯¾è©±ã«ã¤ã„ã¦ã€Œç›¸è«‡è€…ã®æº€è¶³åº¦ã€ã‚’ç›¸è«‡è€…ã®è¦–ç‚¹ã§0ï½5ç‚¹ã§è©•ä¾¡ã—ã€å„ç‚¹æ•°ã®ç¢ºç‡ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

### å¯¾è©±
{user_message}

### å‡ºåŠ›å½¢å¼ï¼ˆæ•°å€¤ã®ã¿ï¼‰
0ç‚¹: XX%
1ç‚¹: XX%
2ç‚¹: XX%
3ç‚¹: XX%
4ç‚¹: XX%
5ç‚¹: XX%"""}
                    ],
                    max_tokens=200,
                    temperature=0.7
                )
                
                response_text = score_response.choices[0].message.content.strip()
                time.sleep(1)
            except Exception as e:
                logger.error(f"ã‚µãƒ³ãƒ—ãƒ« {i+1}/{len(test_data)}: APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
                response_text = "API error"

            expected_value, _, is_error, probabilities = self.extract_score_from_response(response_text)
            
            icon = "âŒ" if is_error else "âœ…"
            log_msg = f"äºˆæ¸¬å¾—ç‚¹: {expected_value:.1f}ç‚¹" if not is_error else "response error"
            logger.info(f"ã‚µãƒ³ãƒ—ãƒ« {i+1}/{len(test_data)}: {icon} ({log_msg})")
            
            if is_error:
                 logger.info(f"   å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ: {response_text}")

            try:
                correct_score_str = sample["messages"][-1]["content"]
                correct_score = float(re.search(r'(\d+(?:\.\d+)?)', correct_score_str).group(1))
            except (AttributeError, IndexError, ValueError):
                correct_score = -1

            score_comparison.append({
                "model_id": model_id, "sample_index": i,
                "predicted_score": expected_value if not is_error else None,
                "correct_score": correct_score, "is_error": is_error,
            })

        errors = sum(1 for s in score_comparison if s['is_error'])
        valid_preds = [s for s in score_comparison if not s['is_error']]
        mae = np.mean([abs(s['predicted_score'] - s['correct_score']) for s in valid_preds]) if valid_preds else 0
        
        return {"model_id": model_id, "total_samples": len(test_data), "response_errors": errors,
                "mean_absolute_error": mae, "score_comparison": score_comparison}

    def evaluate_all_models(self, max_test_samples: int = None):
        """å…¨ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡"""
        try:
            model_ids, test_data = self.load_test_data_and_models()
        except (FileNotFoundError, KeyError, IndexError) as e:
            logger.error(e)
            return
            
        if max_test_samples:
            test_data = test_data[:max_test_samples]
            logger.info(f"è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’ {max_test_samples} ã«åˆ¶é™")
        
        all_results = [self.evaluate_model(model_id, test_data) for i, model_id in enumerate(model_ids)]
        
        df = pd.DataFrame([item for res in all_results for item in res['score_comparison']])
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = self.output_dir / f"accuracy_evaluation_results_{ts}.csv"
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        logger.info(f"è©•ä¾¡çµæœã‚’ {output_path} ã«ä¿å­˜ã—ã¾ã—ãŸ")

        self.print_summary(all_results)

    def print_summary(self, all_results: List[Dict[str, Any]]):
        """è©•ä¾¡çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        print("\n--- ğŸ“Š è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼ ğŸ“Š ---\n")
        summary = [{"Model ID": r['model_id'],
                      "MAE (å¹³å‡çµ¶å¯¾èª¤å·®)": f"{r['mean_absolute_error']:.3f}",
                      "Errorç‡": f"{(r['response_errors']/r['total_samples']*100):.1f}%"}
                     for r in all_results]
        print(pd.DataFrame(summary).to_string(index=False))
        print("\n--------------------------\n")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    try:
        project_root = Path(__file__).resolve().parent.parent.parent.parent
        env_path = project_root / ".env"
        if env_path.exists():
            load_dotenv(env_path)
            logger.info(f".envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {env_path}")
        
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OpenAI APIã‚­ãƒ¼ãŒ.envãƒ•ã‚¡ã‚¤ãƒ«ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        
        print("ï¿½ï¿½ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ­£è§£ç‡è©•ä¾¡ã‚’é–‹å§‹ã—ã¾ã™")
        evaluator = ModelAccuracyEvaluator(api_key)
        evaluator.evaluate_all_models()
        print(f"\nâœ… è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

    except Exception as e:
        logger.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)

if __name__ == "__main__":
    main()