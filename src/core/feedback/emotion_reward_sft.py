import torch
import logging
import os
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, Trainer
from transformers import TrainingArguments
from datasets import Dataset
from sklearn.model_selection import train_test_split

# ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦è¡Œã€å¤±æ•—ã—ãŸå ´åˆã¯çµ¶å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from .data_processing import load_and_split_dataset
    from .llm_evaluation import create_emotion_prompt
except ImportError:
    from data_processing import load_and_split_dataset
    from llm_evaluation import create_emotion_prompt

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def create_output_directories():
    """å‡ºåŠ›ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
    directories = [
        "./supervised_finetuned_model",
        "./logs_supervised",
        "./model_checkpoints"
    ]
    
    for directory in directories:
        if not os.path.exists(directory):
            os.makedirs(directory)
            print(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: {directory}")
        else:
            print(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ—¢å­˜: {directory}")
    
    return directories

def prepare_supervised_finetuning_data(data, llm_pipeline):
    """æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™"""
    finetuning_data = []
    
    print("=== æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿æº–å‚™ ===")
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(data)}")
    
    processed_count = 0
    total_turns = 0
    
    for i in range(len(data)):
        if i % 100 == 0:
            print(f"ğŸ”„ å‡¦ç†ä¸­: {i}/{len(data)} ({i/len(data)*100:.1f}%)")
        
        dialogue = data[i]['dialogue']
        review = data[i]['review_by_client_jp']
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¿½åŠ 
        if i == 0:
            print(f"ğŸ” ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ - dialogue type: {type(dialogue)}")
            print(f"ğŸ” ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ - dialogue keys: {dialogue.keys() if isinstance(dialogue, dict) else 'Not a dict'}")
        
        # ã‚¿ãƒ¼ãƒ³åˆ†å‰²ã‚’å®Ÿè¡Œ - dialogueãŒlistå‹ã®å ´åˆã‚‚å‡¦ç†
        turns = None
        if isinstance(dialogue, dict) and 'dialogue' in dialogue:
            turns = dialogue['dialogue']
        elif isinstance(dialogue, list):
            turns = dialogue
        else:
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¿½åŠ 
            if i == 0:
                print(f"âš ï¸  dialogueãŒæœŸå¾…ã•ã‚Œã‚‹å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {type(dialogue)}")
            continue
        
        try:
            from .turn_segmentation import segment_turns, create_turn_list
        except ImportError:
            from turn_segmentation import segment_turns, create_turn_list
        counselor_turns, client_turns, max_turns = segment_turns(turns)
        turn_list = create_turn_list(counselor_turns, client_turns, max_turns)
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¿½åŠ 
        if i == 0:
            print(f"ğŸ“ˆ ã‚¿ãƒ¼ãƒ³æ•°: {len(turn_list)}")
        
        total_turns += len(turn_list)
        
        # å„ã‚¿ãƒ¼ãƒ³ã«å¯¾ã—ã¦17é …ç›®ã®è©•ä¾¡ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        for turn_idx, turn in enumerate(turn_list):
            # 17é …ç›®ã®ç¢ºç‡åˆ†å¸ƒã‚’è¨ˆç®—ï¼ˆLLMä½¿ç”¨ï¼‰
            try:
                from .llm_evaluation import evaluate_turn_on_items
            except ImportError:
                from llm_evaluation import evaluate_turn_on_items
            evaluation_probabilities = evaluate_turn_on_items(turn, review, llm_pipeline)
            
            # å„è©•ä¾¡é …ç›®ã«ã¤ã„ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å¿œç­”ã®ãƒšã‚¢ã‚’ä½œæˆ
            try:
                from .data_processing import EVALUATION_ITEMS
            except ImportError:
                from data_processing import EVALUATION_ITEMS
            for item in EVALUATION_ITEMS:
                probabilities = evaluation_probabilities.get(item, [0.0, 0.0, 0.1, 0.8, 0.1, 0.0])
                # ç¢ºç‡åˆ†å¸ƒã‹ã‚‰æœŸå¾…å€¤ã‚’è¨ˆç®—
                try:
                    from .data_processing import probability_to_expected_score
                except ImportError:
                    from data_processing import probability_to_expected_score
                score = probability_to_expected_score(probabilities)
                
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
                counselor_text = ""
                client_text = ""
                for utterance in turn:
                    if utterance['role'] == 'counselor':
                        counselor_text += f"ã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼: {utterance['utterance']}\n"
                    elif utterance['role'] == 'client':
                        client_text += f"ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ: {utterance['utterance']}\n"
                
                prompt = f"""ä»¥ä¸‹ã®ã‚«ã‚¦ãƒ³ã‚»ãƒªãƒ³ã‚°ä¼šè©±ã«ã¤ã„ã¦ã€è©•ä¾¡ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚

ä¼šè©±å†…å®¹:
ã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼ã®ç™ºè¨€:
{counselor_text}

ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ç™ºè¨€:
{client_text}

ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è©•ä¾¡:
{review}

è©•ä¾¡åŸºæº–:
0=éå¸¸ã«æ‚ªã„, 1=æ‚ªã„, 2=æ™®é€š, 3=è‰¯ã„, 4=éå¸¸ã«è‰¯ã„, 5=æœ€é«˜

ã€é‡è¦ã€‘ä»¥ä¸‹ã®å½¢å¼ã§å¿…ãšå›ç­”ã—ã¦ãã ã•ã„ã€‚ä»–ã®èª¬æ˜ã¯ä¸è¦ã§ã™ã€‚

{item}ã®è¦³ç‚¹ã§ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è©•ä¾¡ç¢ºç‡åˆ†å¸ƒã‚’0.0-1.0ã®ç¯„å›²ã§å›ç­”ã—ã¦ãã ã•ã„ï¼ˆåˆè¨ˆ1.0ã«ãªã‚‹ã‚ˆã†ã«ï¼‰ã€‚

å¿…ãšä»¥ä¸‹ã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
0ç‚¹ã®ç¢ºç‡: [æ•°å€¤]
1ç‚¹ã®ç¢ºç‡: [æ•°å€¤]
2ç‚¹ã®ç¢ºç‡: [æ•°å€¤]
3ç‚¹ã®ç¢ºç‡: [æ•°å€¤]
4ç‚¹ã®ç¢ºç‡: [æ•°å€¤]
5ç‚¹ã®ç¢ºç‡: [æ•°å€¤]

ä¾‹ï¼š
0ç‚¹ã®ç¢ºç‡: 0.05
1ç‚¹ã®ç¢ºç‡: 0.15
2ç‚¹ã®ç¢ºç‡: 0.25
3ç‚¹ã®ç¢ºç‡: 0.35
4ç‚¹ã®ç¢ºç‡: 0.15
5ç‚¹ã®ç¢ºç‡: 0.05"""
                
                # å¿œç­”ã‚’ä½œæˆï¼ˆç¢ºç‡åˆ†å¸ƒå½¢å¼ï¼‰
                response = f"""0ç‚¹ã®ç¢ºç‡: {probabilities[0]:.3f}
1ç‚¹ã®ç¢ºç‡: {probabilities[1]:.3f}
2ç‚¹ã®ç¢ºç‡: {probabilities[2]:.3f}
3ç‚¹ã®ç¢ºç‡: {probabilities[3]:.3f}
4ç‚¹ã®ç¢ºç‡: {probabilities[4]:.3f}
5ç‚¹ã®ç¢ºç‡: {probabilities[5]:.3f}"""
                
                finetuning_data.append({
                    "prompt": prompt,
                    "response": response,
                    "probabilities": probabilities,
                    "expected_score": score,
                    "item": item,
                    "turn_idx": turn_idx
                })
        
        processed_count += 1
    
    print(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†:")
    print(f"   - å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿: {processed_count}ä»¶")
    print(f"   - ç·ã‚¿ãƒ¼ãƒ³æ•°: {total_turns}")
    print(f"   - ç”Ÿæˆã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«æ•°: {len(finetuning_data)}ä»¶")
    return finetuning_data

class SupervisedFinetuningDataCollator:
    """æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼"""
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
    
    def __call__(self, batch):
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å¿œç­”ã‚’çµåˆ
        texts = []
        for item in batch:
            full_text = item["prompt"] + item["response"]
            texts.append(full_text)
        
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        tokenized = self.tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors="pt"
        )
        
        # ãƒ©ãƒ™ãƒ«ã‚’ä½œæˆï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã¯-100ã€å¿œç­”éƒ¨åˆ†ã¯ãƒˆãƒ¼ã‚¯ãƒ³IDï¼‰
        labels = []
        for i, item in enumerate(batch):
            prompt_tokens = self.tokenizer(
                item["prompt"], 
                return_tensors="pt",
                add_special_tokens=False
            )["input_ids"][0]
            
            response_tokens = self.tokenizer(
                item["response"], 
                return_tensors="pt",
                add_special_tokens=False
            )["input_ids"][0]
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã¯-100ã€å¿œç­”éƒ¨åˆ†ã¯ãƒˆãƒ¼ã‚¯ãƒ³ID
            label = torch.cat([
                torch.full((len(prompt_tokens),), -100),
                response_tokens
            ])
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            if len(label) < 512:
                label = torch.cat([label, torch.full((512 - len(label),), -100)])
            else:
                label = label[:512]
            
            labels.append(label)
        
        labels = torch.stack(labels)
        
        return {
            "input_ids": tokenized["input_ids"],
            "attention_mask": tokenized["attention_mask"],
            "labels": labels
        }

class SupervisedFinetuningTrainer(Trainer):
    """æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ï¼ˆMSEæå¤±ï¼‰"""
    def compute_loss(self, model, inputs, return_outputs=False):
        # ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’å–å¾—
        outputs = model(input_ids=inputs["input_ids"], attention_mask=inputs["attention_mask"])
        
        # å¹³å‡äºŒä¹—èª¤å·®ï¼ˆMSEï¼‰æå¤±ã‚’è¨ˆç®—
        logits = outputs.logits
        
        # ãƒ©ãƒ™ãƒ«ã‹ã‚‰æœ‰åŠ¹ãªãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ã‚’æŠ½å‡ºï¼ˆ-100ä»¥å¤–ï¼‰
        labels = inputs["labels"]
        active_loss = labels.view(-1) != -100
        active_logits = logits.view(-1, logits.size(-1))
        active_labels = labels.view(-1)[active_loss]
        
        # MSEæå¤±ã‚’è¨ˆç®—
        loss_fct = torch.nn.MSELoss()
        loss = loss_fct(active_logits, active_labels.float())
        
        return (loss, outputs) if return_outputs else loss

def initialize_model_and_pipeline():
    """ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’åˆæœŸåŒ–"""
    print("\n=== ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ ===")
    model_name = "tokyotech-llm/Swallow-7b-instruct-hf"
    print(f"èª­ã¿è¾¼ã¿ä¸­: {model_name}")

    # SentencePieceã®ä¾å­˜ã‚’å›é¿ã™ã‚‹ãŸã‚ã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
    import os
    import sys
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ¬ãƒ™ãƒ«ã®SentencePieceã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚ã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
    os.environ["PKG_CONFIG_PATH"] = "/opt/homebrew/lib/pkgconfig:" + os.environ.get("PKG_CONFIG_PATH", "")
    os.environ["LD_LIBRARY_PATH"] = "/opt/homebrew/lib:" + os.environ.get("LD_LIBRARY_PATH", "")
    os.environ["DYLD_LIBRARY_PATH"] = "/opt/homebrew/lib:" + os.environ.get("DYLD_LIBRARY_PATH", "")
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ¬ãƒ™ãƒ«ã®Pythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’è¿½åŠ 
    sys.path.append('/Users/shirakawamomoko/Library/Python/3.11/lib/python/site-packages')
    
    # SentencePieceãŒåˆ©ç”¨å¯èƒ½ã‹ã©ã†ã‹ã‚’ç¢ºèª
    try:
        import sentencepiece
        print("SentencePieceåˆ©ç”¨å¯èƒ½")
    except ImportError:
        print("SentencePieceåˆ©ç”¨ä¸å¯ - ã‚·ã‚¹ãƒ†ãƒ ãƒ¬ãƒ™ãƒ«ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„")

    try:
        # Swallowãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
            local_files_only=False,
            revision="main",
            use_fast=True,  # é«˜é€Ÿãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨
            legacy=False,  # æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å®Ÿè£…ã‚’ä½¿ç”¨
            padding_side="left"  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’å·¦å´ã«é…ç½®
        )
        
        # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,  # å…¬å¼æ¨å¥¨ã®ãƒ‡ãƒ¼ã‚¿å‹
            low_cpu_mem_usage=True,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True,
            local_files_only=False,
            revision="main"
        )
        
        print("ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")
        
    except Exception as e:
        print(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é«˜é€Ÿãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ç„¡åŠ¹ã«ã—ã¦å†è©¦è¡Œ
        try:
            print("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é«˜é€Ÿãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ç„¡åŠ¹ã«ã—ã¦å†è©¦è¡Œ")
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                use_fast=False,
                trust_remote_code=True,
                local_files_only=False,
                revision="main",
                legacy=True,  # ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ¢ãƒ¼ãƒ‰ã§è©¦è¡Œ
                padding_side="left"
            )
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.bfloat16,
                low_cpu_mem_usage=True,
                device_map="auto" if torch.cuda.is_available() else None,
                trust_remote_code=True,
                local_files_only=False,
                revision="main"
            )
            print("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆåŠŸ")
        except Exception as e2:
            print(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚å¤±æ•—: {e2}")
            # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªè¨­å®šã§å†è©¦è¡Œ
            try:
                print("æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªè¨­å®šã§å†è©¦è¡Œ")
                tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    trust_remote_code=True
                )
                model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    trust_remote_code=True
                )
                print("æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆåŠŸ")
            except Exception as e3:
                print(f"æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚å¤±æ•—: {e3}")
                raise e3

    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®š
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print(f"eos_token: {tokenizer.eos_token}")
    print(f"pad_token: {tokenizer.pad_token}")

    # LLMãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åˆæœŸåŒ–
    print("\n=== LLMãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ– ===")
    try:
        llm_pipeline = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            device=0 if torch.cuda.is_available() else -1,
            max_length=512,
            do_sample=True,
            temperature=0.7,
            top_p=0.9
        )
        print("LLMãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†")
    except Exception as e:
        print(f"LLMãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        llm_pipeline = None

    return tokenizer, model, llm_pipeline

def run_supervised_finetuning(tokenizer, model, llm_pipeline, train_data, valid_data):
    print("\n=== æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹ ===")
    
    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ï¼ˆtrain_dataã¯æ—¢ã«8å‰²ã®ãƒ‡ãƒ¼ã‚¿ï¼‰
    print("ğŸ“Š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã‚’é–‹å§‹...")
    train_finetuning_data = prepare_supervised_finetuning_data(train_data, llm_pipeline)
    
    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ï¼ˆvalid_dataã¯æ—¢ã«1å‰²ã®ãƒ‡ãƒ¼ã‚¿ï¼‰
    print("ğŸ“Š æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã‚’é–‹å§‹...")
    val_finetuning_data = prepare_supervised_finetuning_data(valid_data, llm_pipeline)
    
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†:")
    print(f"   - å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(train_finetuning_data)}ä»¶")
    print(f"   - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(val_finetuning_data)}ä»¶")
    
    if len(train_finetuning_data) == 0:
        print("âŒ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚")
        raise ValueError("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ã™")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¤‰æ›
    from datasets import Dataset
    train_dataset = Dataset.from_list(train_finetuning_data)
    val_dataset = Dataset.from_list(val_finetuning_data)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’åˆæœŸåŒ–
    data_collator = SupervisedFinetuningDataCollator(tokenizer)
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã‚’è¨­å®š
    from transformers import TrainingArguments
    training_args = TrainingArguments(
        output_dir="./supervised_finetuned_model",
        overwrite_output_dir=True,
        num_train_epochs=10,                    # ã‚¨ãƒãƒƒã‚¯æ•°10
        per_device_train_batch_size=32,         # ãƒãƒƒãƒã‚µã‚¤ã‚º32ï¼ˆ4GPUã§128ï¼‰
        per_device_eval_batch_size=32,          # è©•ä¾¡ãƒãƒƒãƒã‚µã‚¤ã‚º32
        eval_steps=200,                         # 200ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«è©•ä¾¡
        save_steps=500,                         # 500ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ä¿å­˜
        warmup_steps=200,                       # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—200
        evaluation_strategy="steps",             # ã‚¹ãƒ†ãƒƒãƒ—å˜ä½ã§è©•ä¾¡
        logging_dir="./logs_supervised",
        logging_steps=50,                       # 50ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒ­ã‚°
        save_total_limit=3,                     # ä¿å­˜ãƒ¢ãƒ‡ãƒ«æ•°3å€‹
        load_best_model_at_end=True,            # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’æœ€çµ‚çš„ã«èª­ã¿è¾¼ã¿
        metric_for_best_model="eval_loss",      # æ¤œè¨¼æå¤±ã§æœ€è‰¯ãƒ¢ãƒ‡ãƒ«é¸æŠ
        greater_is_better=False,                # æå¤±ã¯å°ã•ã„ã»ã©è‰¯ã„
        gradient_accumulation_steps=1,          # å‹¾é…è“„ç©ãªã—ï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå¤§ãã„ãŸã‚ï¼‰
        learning_rate=3e-5,                     # å­¦ç¿’ç‡èª¿æ•´
        weight_decay=0.01,
        fp16=torch.cuda.is_available(),
        # è©•ä¾¡æˆ¦ç•¥ã¨ä¿å­˜æˆ¦ç•¥ã‚’ä¸€è‡´ã•ã›ã‚‹
        save_strategy="steps",                  # ä¿å­˜æˆ¦ç•¥ã‚‚ã‚¹ãƒ†ãƒƒãƒ—å˜ä½ã«è¨­å®š
        # é€²è¡ŒçŠ¶æ³è¡¨ç¤ºã®è¨­å®š
        report_to=None,                         # wandbãªã©ã®å¤–éƒ¨ãƒ­ã‚°ã‚’ç„¡åŠ¹åŒ–
        dataloader_pin_memory=False,            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›
    )
    
    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’åˆæœŸåŒ–
    trainer = SupervisedFinetuningTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    
    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ
    print("ğŸš€ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...")
    print(f"   - ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°: {len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs}")
    print(f"   - ã‚¨ãƒãƒƒã‚¯æ•°: {training_args.num_train_epochs}")
    print(f"   - ãƒãƒƒãƒã‚µã‚¤ã‚º: {training_args.per_device_train_batch_size}")
    print(f"   - å­¦ç¿’ç‡: {training_args.learning_rate}")
    
    trainer.train()
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    print("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ä¸­...")
    trainer.save_model()
    tokenizer.save_pretrained("./supervised_finetuned_model")
    print("âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
    
    return trainer, tokenizer

def evaluate_finetuned_model(trainer, tokenizer, test_data, llm_pipeline):
    """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡"""
    print("\n=== ãƒ¢ãƒ‡ãƒ«è©•ä¾¡é–‹å§‹ ===")
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    test_finetuning_data = prepare_supervised_finetuning_data(test_data, llm_pipeline)
    
    # è©•ä¾¡çµæœ
    results = {
        "model_predictions": [],
        "llm_predictions": [],
        "ground_truth": []
    }
    
    # å„ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦äºˆæ¸¬
    for i, data in enumerate(test_finetuning_data):
        if i % 10 == 0:
            print(f"è©•ä¾¡ä¸­: {i}/{len(test_finetuning_data)}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã®äºˆæ¸¬
        inputs = tokenizer(data["prompt"], return_tensors="pt", truncation=True, max_length=512)
        
        with torch.no_grad():
            outputs = trainer.model.generate(
                **inputs,
                max_new_tokens=100,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        model_response = tokenizer.decode(outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)
        
        # çµæœã‚’è¨˜éŒ²
        results["model_predictions"].append(model_response)
        results["llm_predictions"].append(data["response"])
        results["ground_truth"].append(data["expected_score"])
    
    print("è©•ä¾¡å®Œäº†ï¼")
    return results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=== ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹ ===")
    
    # å‡ºåŠ›ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    print("\n=== ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™ ===")
    create_output_directories()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿
    print("\n=== ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ ===")
    train_data, test_data, valid_data = load_and_split_dataset()
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’åˆæœŸåŒ–
    tokenizer, model, llm_pipeline = initialize_model_and_pipeline()
    
    # æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ
    try:
        trainer, tokenizer = run_supervised_finetuning(tokenizer, model, llm_pipeline, train_data, valid_data)
        print("æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚")
        
        # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã‚’å®Ÿè¡Œ
        try:
            results = evaluate_finetuned_model(trainer, tokenizer, test_data, llm_pipeline)
            print("ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚")
            print(f"è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(results['model_predictions'])}")
        except Exception as e:
            print(f"ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            
    except Exception as e:
        print(f"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        print("LLMãƒ™ãƒ¼ã‚¹ã®è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

if __name__ == "__main__":
    main()