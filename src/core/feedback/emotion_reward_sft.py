import torch
import logging
import os
import sys
from datetime import datetime
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, Trainer
from transformers import TrainingArguments
from datasets import Dataset
from sklearn.model_selection import train_test_split

# ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦è¡Œã€å¤±æ•—ã—ãŸå ´åˆã¯çµ¶å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from .data_processing import load_and_split_dataset
    from .llm_evaluation import create_emotion_prompt
except ImportError:
    from data_processing import load_and_split_dataset
    from llm_evaluation import create_emotion_prompt

# ãƒ­ã‚°è¨­å®š
def setup_logging():
    """ãƒ­ã‚°è¨­å®šã‚’åˆæœŸåŒ–"""
    # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    log_dir = "./logs_supervised"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    
    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«åã«ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¿½åŠ 
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = f"{log_dir}/training_{timestamp}.log"
    
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    print(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")
    return logging.getLogger(__name__)

logger = setup_logging()

def create_output_directories():
    """å‡ºåŠ›ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
    directories = [
        "./supervised_finetuned_model",
        "./logs_supervised",
        "./model_checkpoints"
    ]
    
    for directory in directories:
        if not os.path.exists(directory):
            os.makedirs(directory)
            logger.info(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: {directory}")
        else:
            logger.info(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ—¢å­˜: {directory}")
    
    return directories

def prepare_supervised_finetuning_data(data, llm_pipeline, max_samples=None):
    """æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ï¼ˆå…¨ã‚¿ãƒ¼ãƒ³ã‚’1ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ï¼‰"""
    finetuning_data = []
    
    logger.info("=== æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿æº–å‚™ ===")
    logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(data)}")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆå‡¦ç†æ™‚é–“çŸ­ç¸®ã®ãŸã‚ï¼‰
    if max_samples and len(data) > max_samples:
        import random
        random.seed(42)  # å†ç¾æ€§ã®ãŸã‚
        data = random.sample(data, max_samples)
        logger.info(f"ğŸ“Š ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(data)}")
    
    processed_count = 0
    
    for i in range(len(data)):
        if i % 10 == 0:  # 10ä»¶ã”ã¨ã«é€²æ—ã‚’è¡¨ç¤º
            logger.info(f"ğŸ”„ å‡¦ç†ä¸­: {i}/{len(data)} ({i/len(data)*100:.1f}%)")
        
        try:
            data_item = data[i]
        except Exception as e:
            logger.error(f"data[{i}] ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: {e}")
            continue

        dialogue = data_item['dialogue']
        review = data_item['review_by_client_jp']
        
        # ã‚¿ãƒ¼ãƒ³åˆ†å‰²ã‚’å®Ÿè¡Œ
        turns = None
        if isinstance(dialogue, dict) and 'dialogue' in dialogue:
            turns = dialogue['dialogue']
        elif isinstance(dialogue, list):
            turns = dialogue
        else:
            continue
        
        try:
            from .turn_segmentation import segment_turns, create_turn_list
        except ImportError:
            from turn_segmentation import segment_turns, create_turn_list
        
        counselor_turns, client_turns, max_turns = segment_turns(turns)
        turn_list = create_turn_list(counselor_turns, client_turns, max_turns)
        
        # å…¨ã‚¿ãƒ¼ãƒ³ã®ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ
        full_conversation_text = ""
        for turn in turn_list:
            role = turn.get('role', 'unknown')
            utterance = turn.get('utterance', '')
            full_conversation_text += f"{role}: {utterance}\n"
        
        # 17é …ç›®ã®ç¢ºç‡åˆ†å¸ƒã‚’è¨ˆç®—ï¼ˆå…¨ã‚¿ãƒ¼ãƒ³ã‚’1ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ï¼‰
        try:
            from .llm_evaluation import evaluate_conversation_on_items
        except ImportError:
            from llm_evaluation import evaluate_conversation_on_items
        
        evaluation_probabilities = evaluate_conversation_on_items(full_conversation_text, review, llm_pipeline)
        
        # å„è©•ä¾¡é …ç›®ã«ã¤ã„ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å¿œç­”ã®ãƒšã‚¢ã‚’ä½œæˆ
        try:
            from .data_processing import EVALUATION_ITEMS
        except ImportError:
            from data_processing import EVALUATION_ITEMS
        
        for item in EVALUATION_ITEMS:
            probabilities = evaluation_probabilities.get(item, [0.0, 0.0, 0.1, 0.8, 0.1, 0.0])
            
            # ç¢ºç‡åˆ†å¸ƒã‹ã‚‰æœŸå¾…å€¤ã‚’è¨ˆç®—
            try:
                from .data_processing import probability_to_expected_score
            except ImportError:
                from data_processing import probability_to_expected_score
            score = probability_to_expected_score(probabilities)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆï¼ˆå…¨ä¼šè©±ã‚’å«ã‚€ï¼‰
            prompt = f"""Rate {item}:

Conversation:
{full_conversation_text[:500]}...

ã€é‡è¦ã€‘å¿…ãšä»¥ä¸‹ã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚ä»–ã®èª¬æ˜ã¯ä¸€åˆ‡ä¸è¦ã§ã™ï¼š

0ç‚¹ã®ç¢ºç‡: [æ•°å€¤]
1ç‚¹ã®ç¢ºç‡: [æ•°å€¤]
2ç‚¹ã®ç¢ºç‡: [æ•°å€¤]
3ç‚¹ã®ç¢ºç‡: [æ•°å€¤]
4ç‚¹ã®ç¢ºç‡: [æ•°å€¤]
5ç‚¹ã®ç¢ºç‡: [æ•°å€¤]

Answer:"""
            
            # å¿œç­”ã‚’ä½œæˆï¼ˆç¢ºç‡åˆ†å¸ƒå½¢å¼ï¼‰
            response = f"""0ç‚¹ã®ç¢ºç‡: {probabilities[0]:.3f}
1ç‚¹ã®ç¢ºç‡: {probabilities[1]:.3f}
2ç‚¹ã®ç¢ºç‡: {probabilities[2]:.3f}
3ç‚¹ã®ç¢ºç‡: {probabilities[3]:.3f}
4ç‚¹ã®ç¢ºç‡: {probabilities[4]:.3f}
5ç‚¹ã®ç¢ºç‡: {probabilities[5]:.3f}"""
            
            # LLMã‚’å®Ÿéš›ã«å‘¼ã³å‡ºã—ã¦å¿œç­”ã‚’å–å¾—
            try:
                from .llm_evaluation import call_llm_for_probability_distribution
                llm_response = call_llm_for_probability_distribution(prompt, llm_pipeline)
                if llm_response and len(llm_response) == 6:
                    # LLMã®å¿œç­”ã‚’ä½¿ç”¨
                    response = f"""0ç‚¹ã®ç¢ºç‡: {llm_response[0]:.3f}
1ç‚¹ã®ç¢ºç‡: {llm_response[1]:.3f}
2ç‚¹ã®ç¢ºç‡: {llm_response[2]:.3f}
3ç‚¹ã®ç¢ºç‡: {llm_response[3]:.3f}
4ç‚¹ã®ç¢ºç‡: {llm_response[4]:.3f}
5ç‚¹ã®ç¢ºç‡: {llm_response[5]:.3f}"""
                    logger.info(f"âœ… LLMå¿œç­”æˆåŠŸ: {item} - ãƒ‡ãƒ¼ã‚¿{i}")
                else:
                    logger.warning(f"âŒ LLMå¿œç­”å¤±æ•—: {item} - ãƒ‡ãƒ¼ã‚¿{i} - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç¢ºç‡åˆ†å¸ƒã‚’ä½¿ç”¨")
            except Exception as e:
                logger.error(f"âŒ LLMå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {item} - ãƒ‡ãƒ¼ã‚¿{i} - {e}")
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å¿œç­”ã‚’ä½¿ç”¨
            
            finetuning_data.append({
                "prompt": prompt,
                "response": response,
                "probabilities": probabilities,
                "expected_score": score,
                "item": item,
                "data_index": i
            })
        
        processed_count += 1
    
    logger.info(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†:")
    logger.info(f"   - å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿: {processed_count}ä»¶")
    logger.info(f"   - ç”Ÿæˆã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«æ•°: {len(finetuning_data)}ä»¶")
    return finetuning_data

class SupervisedFinetuningDataCollator:
    """æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼"""
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
    
    def __call__(self, batch):
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å¿œç­”ã‚’çµåˆ
        texts = []
        for item in batch:
            full_text = item["prompt"] + item["response"]
            texts.append(full_text)
        
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        tokenized = self.tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors="pt"
        )
        
        # ãƒ©ãƒ™ãƒ«ã‚’ä½œæˆï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã¯-100ã€å¿œç­”éƒ¨åˆ†ã¯ãƒˆãƒ¼ã‚¯ãƒ³IDï¼‰
        labels = []
        for i, item in enumerate(batch):
            prompt_tokens = self.tokenizer(
                item["prompt"], 
                return_tensors="pt",
                add_special_tokens=False
            )["input_ids"][0]
            
            response_tokens = self.tokenizer(
                item["response"], 
                return_tensors="pt",
                add_special_tokens=False
            )["input_ids"][0]
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã¯-100ã€å¿œç­”éƒ¨åˆ†ã¯ãƒˆãƒ¼ã‚¯ãƒ³ID
            label = torch.cat([
                torch.full((len(prompt_tokens),), -100),
                response_tokens
            ])
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            if len(label) < 512:
                label = torch.cat([label, torch.full((512 - len(label),), -100)])
            else:
                label = label[:512]
            
            labels.append(label)
        
        labels = torch.stack(labels)
        
        return {
            "input_ids": tokenized["input_ids"],
            "attention_mask": tokenized["attention_mask"],
            "labels": labels
        }

class SupervisedFinetuningTrainer(Trainer):
    """æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ï¼ˆMSEæå¤±ï¼‰"""
    def compute_loss(self, model, inputs, return_outputs=False):
        # ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’å–å¾—
        outputs = model(input_ids=inputs["input_ids"], attention_mask=inputs["attention_mask"])
        
        # å¹³å‡äºŒä¹—èª¤å·®ï¼ˆMSEï¼‰æå¤±ã‚’è¨ˆç®—
        logits = outputs.logits
        
        # ãƒ©ãƒ™ãƒ«ã‹ã‚‰æœ‰åŠ¹ãªãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ã‚’æŠ½å‡ºï¼ˆ-100ä»¥å¤–ï¼‰
        labels = inputs["labels"]
        active_loss = labels.view(-1) != -100
        active_logits = logits.view(-1, logits.size(-1))
        active_labels = labels.view(-1)[active_loss]
        
        # MSEæå¤±ã‚’è¨ˆç®—
        loss_fct = torch.nn.MSELoss()
        loss = loss_fct(active_logits, active_labels.float())
        
        return (loss, outputs) if return_outputs else loss

def initialize_model_and_pipeline():
    """ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’åˆæœŸåŒ–"""
    print("\n=== ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ ===")
    model_name = "tokyotech-llm/Swallow-7b-instruct-hf"
    print(f"èª­ã¿è¾¼ã¿ä¸­: {model_name}")

    # SentencePieceã®ä¾å­˜ã‚’å›é¿ã™ã‚‹ãŸã‚ã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
    import os
    import sys
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ¬ãƒ™ãƒ«ã®SentencePieceã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚ã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
    os.environ["PKG_CONFIG_PATH"] = "/opt/homebrew/lib/pkgconfig:" + os.environ.get("PKG_CONFIG_PATH", "")
    os.environ["LD_LIBRARY_PATH"] = "/opt/homebrew/lib:" + os.environ.get("LD_LIBRARY_PATH", "")
    os.environ["DYLD_LIBRARY_PATH"] = "/opt/homebrew/lib:" + os.environ.get("DYLD_LIBRARY_PATH", "")
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ¬ãƒ™ãƒ«ã®Pythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’è¿½åŠ 
    sys.path.append('/Users/shirakawamomoko/Library/Python/3.11/lib/python/site-packages')
    
    # SentencePieceãŒåˆ©ç”¨å¯èƒ½ã‹ã©ã†ã‹ã‚’ç¢ºèª
    try:
        import sentencepiece
        print("SentencePieceåˆ©ç”¨å¯èƒ½")
    except ImportError:
        print("SentencePieceåˆ©ç”¨ä¸å¯ - ã‚·ã‚¹ãƒ†ãƒ ãƒ¬ãƒ™ãƒ«ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„")

    try:
        # Swallowãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
            local_files_only=False,
            revision="main",
            use_fast=True,  # é«˜é€Ÿãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨
            legacy=False,  # æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å®Ÿè£…ã‚’ä½¿ç”¨
            padding_side="left"  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’å·¦å´ã«é…ç½®
        )
        
        # Swallowãƒ¢ãƒ‡ãƒ«ã®chat_templateã‚’è¨­å®š
        if not hasattr(tokenizer, 'chat_template') or tokenizer.chat_template is None:
            # Swallowãƒ¢ãƒ‡ãƒ«ã®ç‹¬è‡ªchat_templateã‚’è¨­å®š
            tokenizer.chat_template = """{% for message in messages %}{% if message['role'] == 'system' %}{{ message['content'] + '\n\n'}}{% elif message['role'] == 'user' %}{{ '### æŒ‡ç¤º:\n' + message['content'] + '\n\n### å¿œç­”:\n' }}{% endif %}{% endfor %}"""
            print("Swallowãƒ¢ãƒ‡ãƒ«ã®chat_templateã‚’è¨­å®šã—ã¾ã—ãŸ")
        
        # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,  # å…¬å¼æ¨å¥¨ã®ãƒ‡ãƒ¼ã‚¿å‹
            low_cpu_mem_usage=True,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True,
            local_files_only=False,
            revision="main"
        )
        
        print("ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")
        
    except Exception as e:
        print(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é«˜é€Ÿãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ç„¡åŠ¹ã«ã—ã¦å†è©¦è¡Œ
        try:
            print("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é«˜é€Ÿãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ç„¡åŠ¹ã«ã—ã¦å†è©¦è¡Œ")
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                use_fast=False,
                trust_remote_code=True,
                local_files_only=False,
                revision="main",
                legacy=True,  # ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ¢ãƒ¼ãƒ‰ã§è©¦è¡Œ
                padding_side="left"
            )
            
            # Swallowãƒ¢ãƒ‡ãƒ«ã®chat_templateã‚’è¨­å®š
            if not hasattr(tokenizer, 'chat_template') or tokenizer.chat_template is None:
                tokenizer.chat_template = """{% for message in messages %}{% if message['role'] == 'system' %}{{ message['content'] + '\n\n'}}{% elif message['role'] == 'user' %}{{ '### æŒ‡ç¤º:\n' + message['content'] + '\n\n### å¿œç­”:\n' }}{% endif %}{% endfor %}"""
                print("Swallowãƒ¢ãƒ‡ãƒ«ã®chat_templateã‚’è¨­å®šã—ã¾ã—ãŸ")
            
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.bfloat16,
                low_cpu_mem_usage=True,
                device_map="auto" if torch.cuda.is_available() else None,
                trust_remote_code=True,
                local_files_only=False,
                revision="main"
            )
            print("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆåŠŸ")
        except Exception as e2:
            print(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚å¤±æ•—: {e2}")
            # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªè¨­å®šã§å†è©¦è¡Œ
            try:
                print("æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªè¨­å®šã§å†è©¦è¡Œ")
                tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    trust_remote_code=True
                )
                
                # Swallowãƒ¢ãƒ‡ãƒ«ã®chat_templateã‚’è¨­å®š
                if not hasattr(tokenizer, 'chat_template') or tokenizer.chat_template is None:
                    tokenizer.chat_template = """{% for message in messages %}{% if message['role'] == 'system' %}{{ message['content'] + '\n\n'}}{% elif message['role'] == 'user' %}{{ '### æŒ‡ç¤º:\n' + message['content'] + '\n\n### å¿œç­”:\n' }}{% endif %}{% endfor %}"""
                    print("Swallowãƒ¢ãƒ‡ãƒ«ã®chat_templateã‚’è¨­å®šã—ã¾ã—ãŸ")
                
                model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    trust_remote_code=True
                )
                print("æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆåŠŸ")
            except Exception as e3:
                print(f"æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚å¤±æ•—: {e3}")
                raise e3

    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®š
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print(f"eos_token: {tokenizer.eos_token}")
    print(f"pad_token: {tokenizer.pad_token}")

    # LLMãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åˆæœŸåŒ–
    print("\n=== LLMãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ– ===")
    try:
        llm_pipeline = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            device=0 if torch.cuda.is_available() else -1,
            max_length=512,
            do_sample=True,
            temperature=1.0,  # æ¸©åº¦ã‚’æœ€å¤§ã«
            top_p=1.0,  # top_pã‚‚æœ€å¤§ã«
            repetition_penalty=1.0,  # ç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’ç„¡åŠ¹åŒ–
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            # ã‚ˆã‚Šç¢ºå®Ÿã«å¿œç­”ã™ã‚‹ãŸã‚ã®è¨­å®š
            max_new_tokens=100,  # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¸›ã‚‰ã™
            num_return_sequences=1,
            # early_stoppingã‚’å‰Šé™¤ï¼ˆç„¡åŠ¹ãªãƒ•ãƒ©ã‚°ï¼‰
        )
        print("LLMãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†")
    except Exception as e:
        print(f"LLMãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        llm_pipeline = None

    return tokenizer, model, llm_pipeline

def run_supervised_finetuning(tokenizer, model, llm_pipeline, train_data, valid_data, max_samples=100):
    logger.info("\n=== æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹ ===")
    
    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ï¼ˆtrain_dataã¯æ—¢ã«8å‰²ã®ãƒ‡ãƒ¼ã‚¿ï¼‰
    logger.info("ğŸ“Š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã‚’é–‹å§‹...")
    train_finetuning_data = prepare_supervised_finetuning_data(train_data, llm_pipeline, max_samples)
    
    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ï¼ˆvalid_dataã¯æ—¢ã«1å‰²ã®ãƒ‡ãƒ¼ã‚¿ï¼‰
    logger.info("ğŸ“Š æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã‚’é–‹å§‹...")
    val_finetuning_data = prepare_supervised_finetuning_data(valid_data, llm_pipeline, max_samples//4)
    
    logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†:")
    logger.info(f"   - å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(train_finetuning_data)}ä»¶")
    logger.info(f"   - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(val_finetuning_data)}ä»¶")
    
    if len(train_finetuning_data) == 0:
        logger.error("âŒ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚")
        raise ValueError("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ã™")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¤‰æ›
    from datasets import Dataset
    train_dataset = Dataset.from_list(train_finetuning_data)
    val_dataset = Dataset.from_list(val_finetuning_data)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’åˆæœŸåŒ–
    data_collator = SupervisedFinetuningDataCollator(tokenizer)
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã‚’è¨­å®š
    from transformers import TrainingArguments
    training_args = TrainingArguments(
        output_dir="./supervised_finetuned_model",
        per_device_train_batch_size=2,
        per_device_eval_batch_size=2,
        num_train_epochs=3,
        learning_rate=2e-5,
        warmup_steps=100,
        logging_steps=10,
        evaluation_strategy="steps",
        eval_steps=100,
        save_steps=500,
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        dataloader_async_init_persistent_workers=False,
        dataloader_async_init_timeout_override=None,
        dataloader_async_init_batch_size_override=None,
        dataloader_async_init_num_workers_override=None,
        dataloader_async_init_pin_memory_override=None,
        dataloader_async_init_prefetch_factor_override=None,
        dataloader_async_init_persistent_workers_override=None,
        dataloader_async_init_timeout_override_override=None,
        dataloader_async_init_batch_size_override_override=None,
        dataloader_async_init_num_workers_override_override=None,
        dataloader_async_init_pin_memory_override_override=None,
        dataloader_async_init_prefetch_factor_override_override=None,
        dataloader_async_init_persistent_workers_override_override=None,
    )
    
    # ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’åˆæœŸåŒ–
    trainer = SupervisedFinetuningTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    
    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ
    logger.info("ğŸš€ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...")
    logger.info(f"   - ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°: {len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs}")
    logger.info(f"   - ã‚¨ãƒãƒƒã‚¯æ•°: {training_args.num_train_epochs}")
    logger.info(f"   - ãƒãƒƒãƒã‚µã‚¤ã‚º: {training_args.per_device_train_batch_size}")
    logger.info(f"   - å­¦ç¿’ç‡: {training_args.learning_rate}")
    
    trainer.train()
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    logger.info("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ä¸­...")
    trainer.save_model()
    tokenizer.save_pretrained("./supervised_finetuned_model")
    logger.info("âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
    
    return trainer, tokenizer

def evaluate_finetuned_model(trainer, tokenizer, test_data, llm_pipeline, max_samples=50):
    """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡"""
    logger.info("\n=== ãƒ¢ãƒ‡ãƒ«è©•ä¾¡é–‹å§‹ ===")
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    test_finetuning_data = prepare_supervised_finetuning_data(test_data, llm_pipeline, max_samples)
    
    # è©•ä¾¡çµæœ
    results = {
        "model_predictions": [],
        "llm_predictions": [],
        "ground_truth": []
    }
    
    # å„ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦äºˆæ¸¬
    for i, data in enumerate(test_finetuning_data):
        if i % 10 == 0:
            logger.info(f"è©•ä¾¡ä¸­: {i}/{len(test_finetuning_data)}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã®äºˆæ¸¬
        inputs = tokenizer(data["prompt"], return_tensors="pt", truncation=True, max_length=512)
        
        with torch.no_grad():
            outputs = trainer.model.generate(
                **inputs,
                max_new_tokens=100,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        model_response = tokenizer.decode(outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)
        
        # çµæœã‚’è¨˜éŒ²
        results["model_predictions"].append(model_response)
        results["llm_predictions"].append(data["response"])
        results["ground_truth"].append(data["expected_score"])
    
    logger.info("è©•ä¾¡å®Œäº†ï¼")
    return results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    # å‡ºåŠ›ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    logger.info("\n=== ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™ ===")
    create_output_directories()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿
    logger.info("\n=== ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ ===")
    train_data, test_data, valid_data = load_and_split_dataset()
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’åˆæœŸåŒ–
    tokenizer, model, llm_pipeline = initialize_model_and_pipeline()
    
    # æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ
    try:
        # Phase 1: 100ä»¶ã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        logger.info("=== Phase 1: 100ä»¶ã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ ===")
        trainer, tokenizer = run_supervised_finetuning(tokenizer, model, llm_pipeline, train_data, valid_data, max_samples=100)
        logger.info("âœ… 100ä»¶ã§ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡ŒãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚")
        
        # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã‚’å®Ÿè¡Œ
        try:
            results = evaluate_finetuned_model(trainer, tokenizer, test_data, llm_pipeline, max_samples=50)
            logger.info("âœ… ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚")
            logger.info(f"è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(results['model_predictions'])}")
            
            # çµæœã®è¦ç´„
            logger.info("=== ãƒ†ã‚¹ãƒˆå®Ÿè¡Œçµæœ ===")
            logger.info(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: 100ä»¶")
            logger.info(f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: 25ä»¶")
            logger.info(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: 50ä»¶")
            logger.info("âœ… Phase 1å®Œäº†ï¼100ä»¶ã§ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡ŒãŒæ­£å¸¸ã«çµ‚äº†ã—ã¾ã—ãŸã€‚")
            logger.info("ğŸ“ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: 5000ä»¶ã§ã®æœ¬æ ¼å®Ÿè¡Œã«é€²ã‚€å ´åˆã¯ã€max_samplesã‚’5000ã«å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚")
            
        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            
    except Exception as e:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        logger.error("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:")
        logger.error(traceback.format_exc())
        logger.info("LLMãƒ™ãƒ¼ã‚¹ã®è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

# Phase 2: 5000ä»¶ã§ã®æœ¬æ ¼å®Ÿè¡Œç”¨è¨­å®š
# ä»¥ä¸‹ã®è¡Œã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤–ã—ã¦ã€max_samplesã‚’5000ã«å¤‰æ›´ã—ã¦ãã ã•ã„
# trainer, tokenizer = run_supervised_finetuning(tokenizer, model, llm_pipeline, train_data, valid_data, max_samples=5000)
# results = evaluate_finetuned_model(trainer, tokenizer, test_data, llm_pipeline, max_samples=500)

if __name__ == "__main__":
    main()