import torch
import logging
import os
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, Trainer
from transformers import TrainingArguments
from datasets import Dataset
from sklearn.model_selection import train_test_split

# ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦è¡Œã€å¤±æ•—ã—ãŸå ´åˆã¯çµ¶å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from .data_processing import load_and_split_dataset
    from .llm_evaluation import create_emotion_prompt
except ImportError:
    from data_processing import load_and_split_dataset
    from llm_evaluation import create_emotion_prompt

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger(__name__)

def create_output_directories():
    """å‡ºåŠ›ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
    directories = [
        "./supervised_finetuned_model",
        "./logs_supervised",
        "./model_checkpoints"
    ]
    
    for directory in directories:
        if not os.path.exists(directory):
            os.makedirs(directory)
            print(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: {directory}")
        else:
            print(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ—¢å­˜: {directory}")
    
    return directories

def prepare_supervised_finetuning_data(data, llm_pipeline):
    """æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™"""
    finetuning_data = []
    
    print("=== æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿æº–å‚™ ===")
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(data)}")
    
    # ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ã‚’ãƒ‡ãƒãƒƒã‚°
    print(f"ğŸ” ãƒ‡ãƒ¼ã‚¿ã®å‹: {type(data)}")
    if len(data) > 0:
        print(f"ğŸ” æœ€åˆã®ãƒ‡ãƒ¼ã‚¿ã®å‹: {type(data[0])}")
        print(f"ğŸ” æœ€åˆã®ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ¼: {data[0].keys() if hasattr(data[0], 'keys') else 'Not a dict'}")
    

    
    processed_count = 0
    total_turns = 0
    
    for i in range(len(data)):
        if i % 50 == 0:
            print(f"ğŸ”„ å‡¦ç†ä¸­: {i}/{len(data)} ({i/len(data)*100:.1f}%)")
        try:
            data_item = data[i]
        except Exception as e:
            print(f"data[{i}] ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: {e}")
            print(f"dataã®å‹: {type(data)}")
            print(f"dataã®repr: {repr(data)}")
            raise

        dialogue = data_item['dialogue']
        review = data_item['review_by_client_jp']
        

        
        # ã‚¿ãƒ¼ãƒ³åˆ†å‰²ã‚’å®Ÿè¡Œ - dialogueãŒlistå‹ã®å ´åˆã‚‚å‡¦ç†
        turns = None
        if isinstance(dialogue, dict) and 'dialogue' in dialogue:
            turns = dialogue['dialogue']
        elif isinstance(dialogue, list):
            turns = dialogue
        else:
            
            continue
        
        try:
            from .turn_segmentation import segment_turns, create_turn_list
        except ImportError:
            from turn_segmentation import segment_turns, create_turn_list
        counselor_turns, client_turns, max_turns = segment_turns(turns)
        turn_list = create_turn_list(counselor_turns, client_turns, max_turns)
        

        
        total_turns += len(turn_list)
        
        # å„ã‚¿ãƒ¼ãƒ³ã«å¯¾ã—ã¦17é …ç›®ã®è©•ä¾¡ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        for turn_idx, turn in enumerate(turn_list):
            if turn_idx % 10 == 0:
                print(f"=== ã‚¿ãƒ¼ãƒ³ {turn_idx + 1}/{len(turn_list)} ã®å‡¦ç† ===")
            
            # 17é …ç›®ã®ç¢ºç‡åˆ†å¸ƒã‚’è¨ˆç®—ï¼ˆLLMä½¿ç”¨ï¼‰
            try:
                from .llm_evaluation import evaluate_turn_on_items
            except ImportError:
                from llm_evaluation import evaluate_turn_on_items
            evaluation_probabilities = evaluate_turn_on_items(turn, review, llm_pipeline)
            
            # å„è©•ä¾¡é …ç›®ã«ã¤ã„ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å¿œç­”ã®ãƒšã‚¢ã‚’ä½œæˆ
            try:
                from .data_processing import EVALUATION_ITEMS
            except ImportError:
                from data_processing import EVALUATION_ITEMS
            for item in EVALUATION_ITEMS:
                probabilities = evaluation_probabilities.get(item, [0.0, 0.0, 0.1, 0.8, 0.1, 0.0])
                # ç¢ºç‡åˆ†å¸ƒã‹ã‚‰æœŸå¾…å€¤ã‚’è¨ˆç®—
                try:
                    from .data_processing import probability_to_expected_score
                except ImportError:
                    from data_processing import probability_to_expected_score
                score = probability_to_expected_score(probabilities)
                
        
                
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
                counselor_text = ""
                client_text = ""
                for utterance in turn:
                    if utterance['role'] == 'counselor':
                        counselor_text += f"ã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼: {utterance['utterance']}\n"
                    elif utterance['role'] == 'client':
                        client_text += f"ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ: {utterance['utterance']}\n"
                
                prompt = f"""Rate {item}:

C: {counselor_text[:20]}...
U: {client_text[:20]}...

ã€é‡è¦ã€‘å¿…ãšä»¥ä¸‹ã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚ä»–ã®èª¬æ˜ã¯ä¸€åˆ‡ä¸è¦ã§ã™ï¼š

0ç‚¹ã®ç¢ºç‡: [æ•°å€¤]
1ç‚¹ã®ç¢ºç‡: [æ•°å€¤]
2ç‚¹ã®ç¢ºç‡: [æ•°å€¤]
3ç‚¹ã®ç¢ºç‡: [æ•°å€¤]
4ç‚¹ã®ç¢ºç‡: [æ•°å€¤]
5ç‚¹ã®ç¢ºç‡: [æ•°å€¤]

Answer:"""
                
                # å¿œç­”ã‚’ä½œæˆï¼ˆç¢ºç‡åˆ†å¸ƒå½¢å¼ï¼‰
                response = f"""0ç‚¹ã®ç¢ºç‡: {probabilities[0]:.3f}
1ç‚¹ã®ç¢ºç‡: {probabilities[1]:.3f}
2ç‚¹ã®ç¢ºç‡: {probabilities[2]:.3f}
3ç‚¹ã®ç¢ºç‡: {probabilities[3]:.3f}
4ç‚¹ã®ç¢ºç‡: {probabilities[4]:.3f}
5ç‚¹ã®ç¢ºç‡: {probabilities[5]:.3f}"""
                
                # LLMã‚’å®Ÿéš›ã«å‘¼ã³å‡ºã—ã¦å¿œç­”ã‚’å–å¾—
                try:
                    from .llm_evaluation import call_llm_for_probability_distribution
                    llm_response = call_llm_for_probability_distribution(prompt, llm_pipeline)
                    if llm_response and len(llm_response) == 6:
                        # LLMã®å¿œç­”ã‚’ä½¿ç”¨
                        response = f"""0ç‚¹ã®ç¢ºç‡: {llm_response[0]:.3f}
1ç‚¹ã®ç¢ºç‡: {llm_response[1]:.3f}
2ç‚¹ã®ç¢ºç‡: {llm_response[2]:.3f}
3ç‚¹ã®ç¢ºç‡: {llm_response[3]:.3f}
4ç‚¹ã®ç¢ºç‡: {llm_response[4]:.3f}
5ç‚¹ã®ç¢ºç‡: {llm_response[5]:.3f}"""
                        print(f"âœ… LLMå¿œç­”æˆåŠŸ: {item} - ç¢ºç‡åˆ†å¸ƒ: {llm_response}")
                    else:
                        print(f"âŒ LLMå¿œç­”å¤±æ•—: {item} - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç¢ºç‡åˆ†å¸ƒã‚’ä½¿ç”¨")
                except Exception as e:
                    print(f"âŒ LLMå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {item} - {e}")
                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å¿œç­”ã‚’ä½¿ç”¨
                
                finetuning_data.append({
                    "prompt": prompt,
                    "response": response,
                    "probabilities": probabilities,
                    "expected_score": score,
                    "item": item,
                    "turn_idx": turn_idx
                })
        
        processed_count += 1
    
    print(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†:")
    print(f"   - å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿: {processed_count}ä»¶")
    print(f"   - ç·ã‚¿ãƒ¼ãƒ³æ•°: {total_turns}")
    print(f"   - ç”Ÿæˆã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«æ•°: {len(finetuning_data)}ä»¶")
    return finetuning_data

class SupervisedFinetuningDataCollator:
    """æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼"""
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
    
    def __call__(self, batch):
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å¿œç­”ã‚’çµåˆ
        texts = []
        for item in batch:
            full_text = item["prompt"] + item["response"]
            texts.append(full_text)
        
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        tokenized = self.tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors="pt"
        )
        
        # ãƒ©ãƒ™ãƒ«ã‚’ä½œæˆï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã¯-100ã€å¿œç­”éƒ¨åˆ†ã¯ãƒˆãƒ¼ã‚¯ãƒ³IDï¼‰
        labels = []
        for i, item in enumerate(batch):
            prompt_tokens = self.tokenizer(
                item["prompt"], 
                return_tensors="pt",
                add_special_tokens=False
            )["input_ids"][0]
            
            response_tokens = self.tokenizer(
                item["response"], 
                return_tensors="pt",
                add_special_tokens=False
            )["input_ids"][0]
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã¯-100ã€å¿œç­”éƒ¨åˆ†ã¯ãƒˆãƒ¼ã‚¯ãƒ³ID
            label = torch.cat([
                torch.full((len(prompt_tokens),), -100),
                response_tokens
            ])
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            if len(label) < 512:
                label = torch.cat([label, torch.full((512 - len(label),), -100)])
            else:
                label = label[:512]
            
            labels.append(label)
        
        labels = torch.stack(labels)
        
        return {
            "input_ids": tokenized["input_ids"],
            "attention_mask": tokenized["attention_mask"],
            "labels": labels
        }

class SupervisedFinetuningTrainer(Trainer):
    """æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ï¼ˆMSEæå¤±ï¼‰"""
    def compute_loss(self, model, inputs, return_outputs=False):
        # ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’å–å¾—
        outputs = model(input_ids=inputs["input_ids"], attention_mask=inputs["attention_mask"])
        
        # å¹³å‡äºŒä¹—èª¤å·®ï¼ˆMSEï¼‰æå¤±ã‚’è¨ˆç®—
        logits = outputs.logits
        
        # ãƒ©ãƒ™ãƒ«ã‹ã‚‰æœ‰åŠ¹ãªãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ã‚’æŠ½å‡ºï¼ˆ-100ä»¥å¤–ï¼‰
        labels = inputs["labels"]
        active_loss = labels.view(-1) != -100
        active_logits = logits.view(-1, logits.size(-1))
        active_labels = labels.view(-1)[active_loss]
        
        # MSEæå¤±ã‚’è¨ˆç®—
        loss_fct = torch.nn.MSELoss()
        loss = loss_fct(active_logits, active_labels.float())
        
        return (loss, outputs) if return_outputs else loss

def initialize_model_and_pipeline():
    """ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’åˆæœŸåŒ–"""
    print("\n=== ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ ===")
    model_name = "tokyotech-llm/Swallow-7b-instruct-hf"
    print(f"èª­ã¿è¾¼ã¿ä¸­: {model_name}")

    # SentencePieceã®ä¾å­˜ã‚’å›é¿ã™ã‚‹ãŸã‚ã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
    import os
    import sys
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ¬ãƒ™ãƒ«ã®SentencePieceã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚ã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
    os.environ["PKG_CONFIG_PATH"] = "/opt/homebrew/lib/pkgconfig:" + os.environ.get("PKG_CONFIG_PATH", "")
    os.environ["LD_LIBRARY_PATH"] = "/opt/homebrew/lib:" + os.environ.get("LD_LIBRARY_PATH", "")
    os.environ["DYLD_LIBRARY_PATH"] = "/opt/homebrew/lib:" + os.environ.get("DYLD_LIBRARY_PATH", "")
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ¬ãƒ™ãƒ«ã®Pythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’è¿½åŠ 
    sys.path.append('/Users/shirakawamomoko/Library/Python/3.11/lib/python/site-packages')
    
    # SentencePieceãŒåˆ©ç”¨å¯èƒ½ã‹ã©ã†ã‹ã‚’ç¢ºèª
    try:
        import sentencepiece
        print("SentencePieceåˆ©ç”¨å¯èƒ½")
    except ImportError:
        print("SentencePieceåˆ©ç”¨ä¸å¯ - ã‚·ã‚¹ãƒ†ãƒ ãƒ¬ãƒ™ãƒ«ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„")

    try:
        # Swallowãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
            local_files_only=False,
            revision="main",
            use_fast=True,  # é«˜é€Ÿãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨
            legacy=False,  # æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å®Ÿè£…ã‚’ä½¿ç”¨
            padding_side="left"  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’å·¦å´ã«é…ç½®
        )
        
        # Swallowãƒ¢ãƒ‡ãƒ«ã®chat_templateã‚’è¨­å®š
        if not hasattr(tokenizer, 'chat_template') or tokenizer.chat_template is None:
            # Swallowãƒ¢ãƒ‡ãƒ«ã®ç‹¬è‡ªchat_templateã‚’è¨­å®š
            tokenizer.chat_template = """{% for message in messages %}{% if message['role'] == 'system' %}{{ message['content'] + '\n\n'}}{% elif message['role'] == 'user' %}{{ '### æŒ‡ç¤º:\n' + message['content'] + '\n\n### å¿œç­”:\n' }}{% endif %}{% endfor %}"""
            print("Swallowãƒ¢ãƒ‡ãƒ«ã®chat_templateã‚’è¨­å®šã—ã¾ã—ãŸ")
        
        # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,  # å…¬å¼æ¨å¥¨ã®ãƒ‡ãƒ¼ã‚¿å‹
            low_cpu_mem_usage=True,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True,
            local_files_only=False,
            revision="main"
        )
        
        print("ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")
        
    except Exception as e:
        print(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é«˜é€Ÿãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ç„¡åŠ¹ã«ã—ã¦å†è©¦è¡Œ
        try:
            print("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é«˜é€Ÿãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ç„¡åŠ¹ã«ã—ã¦å†è©¦è¡Œ")
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                use_fast=False,
                trust_remote_code=True,
                local_files_only=False,
                revision="main",
                legacy=True,  # ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ¢ãƒ¼ãƒ‰ã§è©¦è¡Œ
                padding_side="left"
            )
            
            # Swallowãƒ¢ãƒ‡ãƒ«ã®chat_templateã‚’è¨­å®š
            if not hasattr(tokenizer, 'chat_template') or tokenizer.chat_template is None:
                tokenizer.chat_template = """{% for message in messages %}{% if message['role'] == 'system' %}{{ message['content'] + '\n\n'}}{% elif message['role'] == 'user' %}{{ '### æŒ‡ç¤º:\n' + message['content'] + '\n\n### å¿œç­”:\n' }}{% endif %}{% endfor %}"""
                print("Swallowãƒ¢ãƒ‡ãƒ«ã®chat_templateã‚’è¨­å®šã—ã¾ã—ãŸ")
            
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.bfloat16,
                low_cpu_mem_usage=True,
                device_map="auto" if torch.cuda.is_available() else None,
                trust_remote_code=True,
                local_files_only=False,
                revision="main"
            )
            print("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆåŠŸ")
        except Exception as e2:
            print(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚å¤±æ•—: {e2}")
            # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªè¨­å®šã§å†è©¦è¡Œ
            try:
                print("æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªè¨­å®šã§å†è©¦è¡Œ")
                tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    trust_remote_code=True
                )
                
                # Swallowãƒ¢ãƒ‡ãƒ«ã®chat_templateã‚’è¨­å®š
                if not hasattr(tokenizer, 'chat_template') or tokenizer.chat_template is None:
                    tokenizer.chat_template = """{% for message in messages %}{% if message['role'] == 'system' %}{{ message['content'] + '\n\n'}}{% elif message['role'] == 'user' %}{{ '### æŒ‡ç¤º:\n' + message['content'] + '\n\n### å¿œç­”:\n' }}{% endif %}{% endfor %}"""
                    print("Swallowãƒ¢ãƒ‡ãƒ«ã®chat_templateã‚’è¨­å®šã—ã¾ã—ãŸ")
                
                model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    trust_remote_code=True
                )
                print("æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆåŠŸ")
            except Exception as e3:
                print(f"æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚å¤±æ•—: {e3}")
                raise e3

    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®š
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print(f"eos_token: {tokenizer.eos_token}")
    print(f"pad_token: {tokenizer.pad_token}")

    # LLMãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åˆæœŸåŒ–
    print("\n=== LLMãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ– ===")
    try:
        llm_pipeline = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            device=0 if torch.cuda.is_available() else -1,
            max_length=512,
            do_sample=True,
            temperature=1.0,  # æ¸©åº¦ã‚’æœ€å¤§ã«
            top_p=1.0,  # top_pã‚‚æœ€å¤§ã«
            repetition_penalty=1.0,  # ç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’ç„¡åŠ¹åŒ–
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            # ã‚ˆã‚Šç¢ºå®Ÿã«å¿œç­”ã™ã‚‹ãŸã‚ã®è¨­å®š
            max_new_tokens=100,  # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¸›ã‚‰ã™
            num_return_sequences=1,
            # early_stoppingã‚’å‰Šé™¤ï¼ˆç„¡åŠ¹ãªãƒ•ãƒ©ã‚°ï¼‰
        )
        print("LLMãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†")
    except Exception as e:
        print(f"LLMãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        llm_pipeline = None

    return tokenizer, model, llm_pipeline

def run_supervised_finetuning(tokenizer, model, llm_pipeline, train_data, valid_data):
    print("\n=== æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹ ===")
    
    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ï¼ˆtrain_dataã¯æ—¢ã«8å‰²ã®ãƒ‡ãƒ¼ã‚¿ï¼‰
    print("ğŸ“Š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã‚’é–‹å§‹...")
    train_finetuning_data = prepare_supervised_finetuning_data(train_data, llm_pipeline)
    
    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ï¼ˆvalid_dataã¯æ—¢ã«1å‰²ã®ãƒ‡ãƒ¼ã‚¿ï¼‰
    print("ğŸ“Š æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã‚’é–‹å§‹...")
    val_finetuning_data = prepare_supervised_finetuning_data(valid_data, llm_pipeline)
    
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†:")
    print(f"   - å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(train_finetuning_data)}ä»¶")
    print(f"   - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(val_finetuning_data)}ä»¶")
    
    if len(train_finetuning_data) == 0:
        print("âŒ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚")
        raise ValueError("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ã™")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¤‰æ›
    from datasets import Dataset
    train_dataset = Dataset.from_list(train_finetuning_data)
    val_dataset = Dataset.from_list(val_finetuning_data)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’åˆæœŸåŒ–
    data_collator = SupervisedFinetuningDataCollator(tokenizer)
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã‚’è¨­å®š
    from transformers import TrainingArguments
    training_args = TrainingArguments(
        output_dir="./supervised_finetuned_model",
        overwrite_output_dir=True,
        num_train_epochs=10,                    # ã‚¨ãƒãƒƒã‚¯æ•°10
        per_device_train_batch_size=32,         # ãƒãƒƒãƒã‚µã‚¤ã‚º32ï¼ˆ4GPUã§128ï¼‰
        per_device_eval_batch_size=32,          # è©•ä¾¡ãƒãƒƒãƒã‚µã‚¤ã‚º32
        eval_steps=200,                         # 200ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«è©•ä¾¡
        save_steps=500,                         # 500ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ä¿å­˜
        warmup_steps=200,                       # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—200
        learning_rate=2e-5,                     # å­¦ç¿’ç‡2e-5
        weight_decay=0.01,                      # é‡ã¿æ¸›è¡°0.01
        logging_dir="./logs_supervised",        # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        logging_steps=100,                      # 100ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒ­ã‚°
        evaluation_strategy="steps",            # ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«è©•ä¾¡
        save_strategy="steps",                  # ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ä¿å­˜
        load_best_model_at_end=True,           # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        metric_for_best_model="eval_loss",     # è©•ä¾¡æŒ‡æ¨™
        greater_is_better=False,               # å°ã•ã„æ–¹ãŒè‰¯ã„
        report_to=None,                        # ãƒ¬ãƒãƒ¼ãƒˆç„¡åŠ¹
        remove_unused_columns=False,           # æœªä½¿ç”¨ã‚«ãƒ©ãƒ ã‚’å‰Šé™¤ã—ãªã„
        dataloader_pin_memory=False,           # ãƒ”ãƒ³ãƒ¡ãƒ¢ãƒªç„¡åŠ¹
        dataloader_num_workers=0,              # ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°0
        gradient_accumulation_steps=1,         # å‹¾é…è“„ç©ã‚¹ãƒ†ãƒƒãƒ—1
        fp16=False,                            # FP16ç„¡åŠ¹
        bf16=True,                             # BF16æœ‰åŠ¹
        optim="adamw_torch",                   # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        lr_scheduler_type="cosine",            # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
        warmup_ratio=0.1,                      # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—æ¯”ç‡
        max_grad_norm=1.0,                     # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
        seed=42,                               # ã‚·ãƒ¼ãƒ‰
        data_seed=42,                          # ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒ‰
        group_by_length=True,                  # é•·ã•ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        length_column_name="length",           # é•·ã•ã‚«ãƒ©ãƒ å
        ignore_data_skip=False,                # ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒƒãƒ—ç„¡è¦–
        label_names=["labels"],                # ãƒ©ãƒ™ãƒ«å
        ddp_find_unused_parameters=False,      # DDPæœªä½¿ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç„¡è¦–
        ddp_bucket_cap_mb=25,                 # DDPãƒã‚±ãƒƒãƒˆã‚µã‚¤ã‚º
        dataloader_drop_last=False,            # æœ€å¾Œã®ãƒãƒƒãƒã‚’å‰Šé™¤ã—ãªã„
        eval_accumulation_steps=None,          # è©•ä¾¡è“„ç©ã‚¹ãƒ†ãƒƒãƒ—
        eval_delay=0,                          # è©•ä¾¡é…å»¶
        save_on_each_node=False,               # å„ãƒãƒ¼ãƒ‰ã«ä¿å­˜ã—ãªã„
        save_total_limit=None,                 # ä¿å­˜åˆ¶é™ãªã—
        save_only_model=False,                 # ãƒ¢ãƒ‡ãƒ«ã®ã¿ä¿å­˜
        use_cpu=False,                         # CPUä½¿ç”¨ã—ãªã„
        dataloader_prefetch_factor=None,       # ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒä¿‚æ•°
        dataloader_persistent_workers=False,   # æ°¸ç¶šãƒ¯ãƒ¼ã‚«ãƒ¼ç„¡åŠ¹
        dataloader_prefetch_factor_override=None,  # ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒä¿‚æ•°ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
        dataloader_pin_memory_device="",       # ãƒ”ãƒ³ãƒ¡ãƒ¢ãƒªãƒ‡ãƒã‚¤ã‚¹
        dataloader_async_init=False,           # éåŒæœŸåˆæœŸåŒ–ç„¡åŠ¹
        dataloader_async_init_timeout=0,       # éåŒæœŸåˆæœŸåŒ–ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        dataloader_async_init_batch_size=0,    # éåŒæœŸåˆæœŸåŒ–ãƒãƒƒãƒã‚µã‚¤ã‚º
        dataloader_async_init_num_workers=0,   # éåŒæœŸåˆæœŸåŒ–ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
        dataloader_async_init_pin_memory=False,  # éåŒæœŸåˆæœŸåŒ–ãƒ”ãƒ³ãƒ¡ãƒ¢ãƒªç„¡åŠ¹
        dataloader_async_init_prefetch_factor=None,  # éåŒæœŸåˆæœŸåŒ–ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒä¿‚æ•°
        dataloader_async_init_persistent_workers=False,  # éåŒæœŸåˆæœŸåŒ–æ°¸ç¶šãƒ¯ãƒ¼ã‚«ãƒ¼ç„¡åŠ¹
        dataloader_async_init_timeout_override=None,  # éåŒæœŸåˆæœŸåŒ–ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
        dataloader_async_init_batch_size_override=None,  # éåŒæœŸåˆæœŸåŒ–ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
        dataloader_async_init_num_workers_override=None,  # éåŒæœŸåˆæœŸåŒ–ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
        dataloader_async_init_pin_memory_override=None,  # éåŒæœŸåˆæœŸåŒ–ãƒ”ãƒ³ãƒ¡ãƒ¢ãƒªã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
        dataloader_async_init_prefetch_factor_override=None,  # éåŒæœŸåˆæœŸåŒ–ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒä¿‚æ•°ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
        dataloader_async_init_persistent_workers_override=None,  # éåŒæœŸåˆæœŸåŒ–æ°¸ç¶šãƒ¯ãƒ¼ã‚«ãƒ¼ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
        dataloader_async_init_timeout_override_override=None,  # éåŒæœŸåˆæœŸåŒ–ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
        dataloader_async_init_batch_size_override_override=None,  # éåŒæœŸåˆæœŸåŒ–ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
        dataloader_async_init_num_workers_override_override=None,  # éåŒæœŸåˆæœŸåŒ–ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
        dataloader_async_init_pin_memory_override_override=None,  # éåŒæœŸåˆæœŸåŒ–ãƒ”ãƒ³ãƒ¡ãƒ¢ãƒªã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
        dataloader_async_init_prefetch_factor_override_override=None,  # éåŒæœŸåˆæœŸåŒ–ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒä¿‚æ•°ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
        dataloader_async_init_persistent_workers_override_override=None,  # éåŒæœŸåˆæœŸåŒ–æ°¸ç¶šãƒ¯ãƒ¼ã‚«ãƒ¼ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
    )
    
    # ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’åˆæœŸåŒ–
    trainer = SupervisedFinetuningTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    
    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ
    print("ğŸš€ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...")
    print(f"   - ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°: {len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs}")
    print(f"   - ã‚¨ãƒãƒƒã‚¯æ•°: {training_args.num_train_epochs}")
    print(f"   - ãƒãƒƒãƒã‚µã‚¤ã‚º: {training_args.per_device_train_batch_size}")
    print(f"   - å­¦ç¿’ç‡: {training_args.learning_rate}")
    
    trainer.train()
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    print("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ä¸­...")
    trainer.save_model()
    tokenizer.save_pretrained("./supervised_finetuned_model")
    print("âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
    
    return trainer, tokenizer

def evaluate_finetuned_model(trainer, tokenizer, test_data, llm_pipeline):
    """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡"""
    print("\n=== ãƒ¢ãƒ‡ãƒ«è©•ä¾¡é–‹å§‹ ===")
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    test_finetuning_data = prepare_supervised_finetuning_data(test_data, llm_pipeline)
    
    # è©•ä¾¡çµæœ
    results = {
        "model_predictions": [],
        "llm_predictions": [],
        "ground_truth": []
    }
    
    # å„ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦äºˆæ¸¬
    for i, data in enumerate(test_finetuning_data):
        if i % 10 == 0:
            print(f"è©•ä¾¡ä¸­: {i}/{len(test_finetuning_data)}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã®äºˆæ¸¬
        inputs = tokenizer(data["prompt"], return_tensors="pt", truncation=True, max_length=512)
        
        with torch.no_grad():
            outputs = trainer.model.generate(
                **inputs,
                max_new_tokens=100,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        model_response = tokenizer.decode(outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)
        
        # çµæœã‚’è¨˜éŒ²
        results["model_predictions"].append(model_response)
        results["llm_predictions"].append(data["response"])
        results["ground_truth"].append(data["expected_score"])
    
    print("è©•ä¾¡å®Œäº†ï¼")
    return results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    # å‡ºåŠ›ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    print("\n=== ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™ ===")
    create_output_directories()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿
    print("\n=== ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ ===")
    train_data, test_data, valid_data = load_and_split_dataset()
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’åˆæœŸåŒ–
    tokenizer, model, llm_pipeline = initialize_model_and_pipeline()
    
    # æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ
    try:
        # æœ¬æ ¼çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
        trainer, tokenizer = run_supervised_finetuning(tokenizer, model, llm_pipeline, train_data, valid_data)
        print("æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚")
        
        # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã‚’å®Ÿè¡Œ
        try:
            results = evaluate_finetuned_model(trainer, tokenizer, test_data, llm_pipeline)
            print("ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚")
            print(f"è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(results['model_predictions'])}")
        except Exception as e:
            print(f"ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            
    except Exception as e:
        print(f"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        print("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±:")
        traceback.print_exc()
        print("LLMãƒ™ãƒ¼ã‚¹ã®è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

if __name__ == "__main__":
    main()