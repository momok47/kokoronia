#!/usr/bin/env python3
"""
20é …ç›®ã®ä¼šè©±å°è±¡è©•ä¾¡ã«å¯¾ã™ã‚‹äºˆæ¸¬ç²¾åº¦è¨ˆç®—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å„é …ç›®ã”ã¨ã«MAEã€RMSEã€èª¤å·®1ã§ã®æ­£è§£ç‡ã‚’è¨ˆç®—ã™ã‚‹æ©Ÿèƒ½ä»˜ã
"""

import os
import json
import logging
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI
import time
from typing import List, Dict, Any, Tuple
import numpy as np
from datetime import datetime
import pandas as pd
import re
from sklearn.metrics import mean_absolute_error, mean_squared_error
import argparse

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger(__name__)

# 20é …ç›®ã®è©•ä¾¡æŒ‡æ¨™
EVALUATION_ITEMS = [
    "è´ã„ã¦ã‚‚ã‚‰ãˆãŸã€ã‚ã‹ã£ã¦ã‚‚ã‚‰ãˆãŸã¨æ„Ÿã˜ãŸ",
    "å°Šé‡ã•ã‚ŒãŸã¨æ„Ÿã˜ãŸ",
    "æ–°ã—ã„æ°—ã¥ãã‚„ä½“é¨“ãŒã‚ã£ãŸ",
    "å¸Œæœ›ã‚„æœŸå¾…ã‚’æ„Ÿã˜ã‚‰ã‚ŒãŸ",
    "å–ã‚Šçµ„ã¿ãŸã‹ã£ãŸã“ã¨ã‚’æ‰±ãˆãŸ",
    "ä¸€ç·’ã«è€ƒãˆãªãŒã‚‰å–ã‚Šçµ„ã‚ãŸ",
    "ã‚„ã‚Šã¨ã‚Šã®ãƒªã‚ºãƒ ãŒã‚ã£ã¦ã„ãŸ",
    "å±…å¿ƒåœ°ã®ã‚ˆã„ã‚„ã‚Šã¨ã‚Šã ã£ãŸ",
    "å…¨ä½“ã¨ã—ã¦é©åˆ‡ã§ã‚ˆã‹ã£ãŸ",
    "ä»Šå›ã®ç›¸è«‡ã¯ä¾¡å€¤ãŒã‚ã£ãŸ",
    "ç›¸è«‡é–‹å§‹ã®å††æ»‘ã•",
    "ç›¸è«‡çµ‚äº†ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ï¼ˆä¸å¿…è¦ã«è´ãã™ãã¦ã„ãªã„ã‹ï¼‰ã€å††æ»‘ã•",
    "å—å®¹ãƒ»å…±æ„Ÿ",
    "è‚¯å®šãƒ»æ‰¿èª",
    "çš„ç¢ºãªè³ªå•ã«ã‚ˆã‚‹ä¼šè©±ã®ä¿ƒé€²",
    "è¦ç´„",
    "å•é¡Œã®æ˜ç¢ºåŒ–",
    "ã“ã®ç›¸è«‡ã§ã®ç›®æ¨™ã®æ˜ç¢ºåŒ–",
    "æ¬¡ã®è¡Œå‹•ã«ã¤ãªãŒã‚‹ææ¡ˆ",
    "å‹‡æ°—ã¥ã‘ãƒ»å¸Œæœ›ã®å–šèµ·"
]


class MultiItemModelEvaluator:
    """20é …ç›®ã®è©•ä¾¡äºˆæ¸¬ã«å¯¾å¿œã—ãŸãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, api_key: str):
        """
        åˆæœŸåŒ–
        
        Args:
            api_key: OpenAI APIã‚­ãƒ¼
        """
        self.client = OpenAI(api_key=api_key)
        self.script_dir = Path(__file__).resolve().parent
        self.output_dir = self.script_dir / "openai_sft_outputs"
        logger.info(f"çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š: {self.output_dir}")
        self.output_dir.mkdir(exist_ok=True)

    def _find_latest_results_file(self) -> Path:
        """æœ€æ–°ã®ãƒãƒƒãƒçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¦‹ã¤ã‘ã‚‹"""
        result_files = list(self.output_dir.glob("batch_fine_tuning_results_*.json"))
        if not result_files:
            raise FileNotFoundError(f"ãƒãƒƒãƒçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.output_dir}")
        
        latest_file = max(result_files, key=lambda x: x.stat().st_mtime)
        logger.info(f"ä½¿ç”¨ã™ã‚‹çµæœãƒ•ã‚¡ã‚¤ãƒ«: {latest_file}")
        return latest_file

    def load_test_data_and_models(self) -> Tuple[List[str], List[Dict[str, Any]]]:
        """
        æœ€æ–°ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«IDã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
        Returns:
            (ãƒ¢ãƒ‡ãƒ«IDã®ãƒªã‚¹ãƒˆ, testãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ)
        """
        latest_results_file = self._find_latest_results_file()
        
        with open(latest_results_file, 'r', encoding='utf-8') as f:
            results_data = json.load(f)

        # ãƒ¢ãƒ‡ãƒ«IDã®èª­ã¿è¾¼ã¿
        model_ids = []
        if 'batches' in results_data:
            for batch in results_data['batches']:
                if 'fine_tuned_model' in batch:
                    model_ids.append(batch['fine_tuned_model'])
        elif 'batch_results' in results_data:
            for batch in results_data['batch_results']:
                if batch.get('final_status') == 'succeeded' and 'final_model_id' in batch:
                    model_ids.append(batch['final_model_id'])
        
        if not model_ids:
            raise ValueError(f"çµæœãƒ•ã‚¡ã‚¤ãƒ« {latest_results_file.name} ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«IDã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        logger.info(f"è©•ä¾¡å¯¾è±¡ãƒ¢ãƒ‡ãƒ«æ•°: {len(model_ids)}")
        for i, model_id in enumerate(model_ids):
            logger.info(f"  ãƒ¢ãƒ‡ãƒ« {i+1}: {model_id}")

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—
        test_data_path = None
        if 'test_data_file' in results_data:
            test_data_filename = results_data['test_data_file']
            test_data_path = self.output_dir / test_data_filename
        
        if not test_data_path or not test_data_path.exists():
            test_files = list(self.output_dir.glob("test_data_*.jsonl"))
            if test_files:
                latest_test_file = max(test_files, key=lambda x: x.stat().st_mtime)
                test_data_path = latest_test_file
        
        if not test_data_path or not test_data_path.exists():
            raise FileNotFoundError(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
        
        logger.info(f"ä½¿ç”¨ã™ã‚‹testãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: {test_data_path}")

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        test_data = []
        with open(test_data_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line:
                    try:
                        test_data.append(json.loads(line))
                    except json.JSONDecodeError as e:
                        logger.warning(f"è¡Œ {line_num} ã®JSONè§£æã«å¤±æ•—: {e}")
                        continue
        
        logger.info(f"testãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(test_data)}ã‚µãƒ³ãƒ—ãƒ«")
        return model_ids, test_data

    def extract_score_from_response(self, response_text: str) -> Tuple[float, str, bool, Dict[int, float]]:
        """å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç¢ºç‡åˆ†å¸ƒã¨æœŸå¾…å€¤ã‚’æŠ½å‡º"""
        logger.debug(f"å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ: {response_text}")
        
        probability_patterns = [
            r'(\d+)ç‚¹\s*[:ï¼š]\s*(\d+(?:\.\d+)?)%',
            r'(\d+)ç‚¹\s+(\d+(?:\.\d+)?)%',
        ]
        
        probabilities = {}
        for pattern in probability_patterns:
            matches = re.findall(pattern, response_text)
            for match in matches:
                point, prob = int(match[0]), float(match[1])
                if 0 <= point <= 5 and point not in probabilities:
                    probabilities[point] = prob
        
        total_probability = sum(probabilities.values())
        
        if len(probabilities) == 6 and abs(total_probability - 100.0) < 10.0:
            expected_value = sum(p * (pr / 100.0) for p, pr in probabilities.items())
            return expected_value, "æŠ½å‡ºæˆåŠŸ", False, probabilities
        else:
            logger.warning(f"ç¢ºç‡åˆ†å¸ƒã®å½¢å¼ãŒä¸æ­£: åˆè¨ˆ={total_probability}%, æ•°={len(probabilities)}")
            return 0.0, "response error", True, {}

    def parse_correct_scores_from_sample(self, sample: Dict[str, Any]) -> Dict[str, float]:
        """
        ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰æ­£è§£ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡ºï¼ˆKokoChatãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«å¯¾å¿œï¼‰
        
        Args:
            sample: KokoChatãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒ«
            
        Returns:
            å„è©•ä¾¡é …ç›®ã®æ­£è§£ã‚¹ã‚³ã‚¢è¾æ›¸
        """
        correct_scores = {}
        
        try:
            # KokoChatãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®å ´åˆ: review_by_client_jpã‹ã‚‰æ­£è§£ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            if 'review_by_client_jp' in sample:
                review_data = sample['review_by_client_jp']
                for item in EVALUATION_ITEMS:
                    if item in review_data:
                        score_value = review_data[item]
                        if isinstance(score_value, (int, float)) and score_value != "":
                            correct_scores[item] = float(score_value)
                        else:
                            logger.warning(f"é …ç›® '{item}' ã®ã‚¹ã‚³ã‚¢ãŒç„¡åŠ¹: {score_value}")
                
                if correct_scores:
                    return correct_scores
            
            # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®messageså½¢å¼ã®å ´åˆ
            elif 'messages' in sample:
                assistant_message = sample["messages"][-1]["content"]
                
                # JSONå½¢å¼ã§å„é …ç›®ã®ã‚¹ã‚³ã‚¢ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ
                try:
                    score_data = json.loads(assistant_message)
                    if isinstance(score_data, dict):
                        for item in EVALUATION_ITEMS:
                            if item in score_data:
                                correct_scores[item] = float(score_data[item])
                        if correct_scores:
                            return correct_scores
                except json.JSONDecodeError:
                    pass
                
                # å„é …ç›®ãŒå€‹åˆ¥ã®è¡Œã§è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å ´åˆ
                lines = assistant_message.split('\n')
                for line in lines:
                    for item in EVALUATION_ITEMS:
                        if item in line:
                            # ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡ºï¼ˆä¾‹: "é …ç›®å: 3.5ç‚¹" ã®å½¢å¼ï¼‰
                            score_match = re.search(r'(\d+(?:\.\d+)?)', line)
                            if score_match:
                                correct_scores[item] = float(score_match.group(1))
                                break
            
            # ã©ã®å½¢å¼ã§ã‚‚è§£æã§ããªã‹ã£ãŸå ´åˆã®ã‚¨ãƒ©ãƒ¼
            if not correct_scores:
                logger.error(f"æ­£è§£ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚µãƒ³ãƒ—ãƒ«æ§‹é€ : {list(sample.keys())}")
                return {}
                    
        except (AttributeError, IndexError, ValueError) as e:
            logger.warning(f"æ­£è§£ãƒ‡ãƒ¼ã‚¿ã®è§£æã«å¤±æ•—: {e}")
            
        return correct_scores

    def evaluate_model_on_all_items(self, model_id: str, test_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’20é …ç›®ã™ã¹ã¦ã§è©•ä¾¡"""
        logger.info(f"ãƒ¢ãƒ‡ãƒ« {model_id} ã®20é …ç›®è©•ä¾¡ã‚’é–‹å§‹...")
        
        all_predictions = []
        
        for sample_idx, sample in enumerate(test_data):
            try:
                # ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ï¼ˆKokoChatãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«å¯¾å¿œï¼‰
                conversation_text = None
                
                if 'dialogue' in sample:
                    # KokoChatã®å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
                    dialogue_parts = []
                    for turn in sample['dialogue']:
                        role = turn.get('role', 'unknown')
                        utterance = turn.get('utterance', '')
                        if role == 'counselor':
                            dialogue_parts.append(f"ã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼: {utterance}")
                        elif role == 'client':
                            dialogue_parts.append(f"ç›¸è«‡è€…: {utterance}")
                    conversation_text = '\n'.join(dialogue_parts)
                elif 'messages' in sample:
                    # messageså½¢å¼ã®å ´åˆ
                    for msg in sample['messages']:
                        if msg['role'] == 'user':
                            conversation_text = msg['content']
                            break
                
                if not conversation_text:
                    logger.warning(f"ã‚µãƒ³ãƒ—ãƒ« {sample_idx+1}: ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    continue
                
                # æ­£è§£ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                correct_scores = self.parse_correct_scores_from_sample(sample)
                
                if not correct_scores:
                    logger.warning(f"ã‚µãƒ³ãƒ—ãƒ« {sample_idx+1}: æ­£è§£ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    continue
                
                sample_predictions = {
                    "sample_index": sample_idx,
                    "conversation_text": conversation_text,
                    "predictions": {},
                    "correct_scores": correct_scores,
                    "errors": {}
                }
                
                # å„è©•ä¾¡é …ç›®ã«ã¤ã„ã¦äºˆæ¸¬ã‚’å®Ÿè¡Œ
                for item_idx, evaluation_item in enumerate(EVALUATION_ITEMS):
                    try:
                        logger.info(f"ã‚µãƒ³ãƒ—ãƒ« {sample_idx+1}/{len(test_data)}, é …ç›® {item_idx+1}/{len(EVALUATION_ITEMS)}: {evaluation_item}")
                        
                        # ãƒ¢ãƒ‡ãƒ«ã«è³ªå•ã‚’é€ä¿¡
                        response = self.client.chat.completions.create(
                            model=model_id,
                            messages=[
                                {"role": "system", "content": "ã‚ãªãŸã¯å¿ƒç†ã‚«ã‚¦ãƒ³ã‚»ãƒªãƒ³ã‚°ã®è©•ä¾¡è€…ã§ã™ã€‚ç›¸è«‡è€…ã®æº€è¶³åº¦ã‚’0ï½5ç‚¹ã§è©•ä¾¡ã—ã€å„ç‚¹æ•°ã®ç¢ºç‡ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"},
                                {"role": "user", "content": f"""### æŒ‡ç¤º
ä»¥ä¸‹ã®å¯¾è©±ã«ã¤ã„ã¦ã€Œ{evaluation_item}ã€ã®æº€è¶³åº¦ã‚’ç›¸è«‡è€…ã®è¦–ç‚¹ã§0ï½5ç‚¹ã§è©•ä¾¡ã—ã€å„ç‚¹æ•°ã®ç¢ºç‡ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

### å¯¾è©±
{conversation_text}

### å‡ºåŠ›å½¢å¼ï¼ˆæ•°å€¤ã®ã¿ï¼‰
0ç‚¹: XX%
1ç‚¹: XX%
2ç‚¹: XX%
3ç‚¹: XX%
4ç‚¹: XX%
5ç‚¹: XX%"""}
                            ],
                            max_tokens=200,
                            temperature=0.7
                        )
                        
                        response_text = response.choices[0].message.content.strip()
                        expected_value, _, is_error, probabilities = self.extract_score_from_response(response_text)
                        
                        if not is_error:
                            sample_predictions["predictions"][evaluation_item] = expected_value
                        else:
                            sample_predictions["errors"][evaluation_item] = response_text
                            logger.warning(f"é …ç›® '{evaluation_item}' ã®äºˆæ¸¬ã«å¤±æ•—")
                        
                        time.sleep(1)  # APIåˆ¶é™å¯¾ç­–
                        
                    except Exception as e:
                        logger.error(f"é …ç›® '{evaluation_item}' ã®è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                        sample_predictions["errors"][evaluation_item] = str(e)
                
                all_predictions.append(sample_predictions)
                
            except Exception as e:
                logger.error(f"ã‚µãƒ³ãƒ—ãƒ« {sample_idx+1} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                logger.debug(f"ã‚µãƒ³ãƒ—ãƒ«æ§‹é€ : {list(sample.keys()) if isinstance(sample, dict) else type(sample)}")
                continue
        
        return {
            "model_id": model_id,
            "total_samples": len(test_data),
            "predictions": all_predictions
        }

    def calculate_metrics_per_item(self, predictions_data: Dict[str, Any]) -> Dict[str, Dict[str, float]]:
        """é …ç›®ã”ã¨ã«MAEã€RMSEã€èª¤å·®1ã§ã®æ­£è§£ç‡ã‚’è¨ˆç®—"""
        logger.info("é …ç›®ã”ã¨ã®ç²¾åº¦æŒ‡æ¨™ã‚’è¨ˆç®—ä¸­...")
        
        metrics_per_item = {}
        
        for evaluation_item in EVALUATION_ITEMS:
            predicted_scores = []
            correct_scores = []
            
            # å„ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰è©²å½“é …ç›®ã®äºˆæ¸¬å€¤ã¨æ­£è§£å€¤ã‚’åé›†
            for sample_pred in predictions_data["predictions"]:
                if evaluation_item in sample_pred["predictions"] and evaluation_item in sample_pred["correct_scores"]:
                    predicted_scores.append(sample_pred["predictions"][evaluation_item])
                    correct_scores.append(sample_pred["correct_scores"][evaluation_item])
            
            if len(predicted_scores) == 0:
                logger.warning(f"é …ç›® '{evaluation_item}' ã®æœ‰åŠ¹ãªäºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                metrics_per_item[evaluation_item] = {
                    "mae": float('nan'),
                    "rmse": float('nan'),
                    "accuracy_within_1": float('nan'),
                    "sample_count": 0
                }
                continue
            
            # MAEï¼ˆå¹³å‡çµ¶å¯¾èª¤å·®ï¼‰
            mae = mean_absolute_error(correct_scores, predicted_scores)
            
            # RMSEï¼ˆäºŒä¹—å¹³å‡å¹³æ–¹æ ¹èª¤å·®ï¼‰
            rmse = np.sqrt(mean_squared_error(correct_scores, predicted_scores))
            
            # èª¤å·®1ã§ã®æ­£è§£ç‡
            errors = np.abs(np.array(predicted_scores) - np.array(correct_scores))
            accuracy_within_1 = np.mean(errors <= 1.0) * 100  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸
            
            metrics_per_item[evaluation_item] = {
                "mae": mae,
                "rmse": rmse,
                "accuracy_within_1": accuracy_within_1,
                "sample_count": len(predicted_scores)
            }
            
            logger.info(f"é …ç›® '{evaluation_item}': MAE={mae:.3f}, RMSE={rmse:.3f}, èª¤å·®1æ­£è§£ç‡={accuracy_within_1:.1f}%")
        
        return metrics_per_item

    def load_kokorochat_data_directly(self, max_test_samples: int = None) -> List[Dict[str, Any]]:
        """KokoChatãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥èª­ã¿è¾¼ã¿"""
        from datasets import load_dataset
        
        logger.info("KokoChatãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç›´æ¥ä½¿ç”¨ã—ã¾ã™")
        logger.info("KokoroChat ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­...")
        
        # KokoChatãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿
        dataset = load_dataset("UEC-InabaLab/KokoroChat", split="train")
        
        if max_test_samples:
            dataset = dataset.select(range(min(max_test_samples, len(dataset))))
            logger.info(f"ãƒ‡ãƒãƒƒã‚°ç”¨ã«{max_test_samples}ã‚µãƒ³ãƒ—ãƒ«ã«åˆ¶é™")
        
        logger.info(f"å…ƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: {len(dataset)}")
        
        # æœ‰åŠ¹ãªã‚µãƒ³ãƒ—ãƒ«ã®ã¿ã‚’æŠ½å‡ºï¼ˆreview_by_client_jpãŒå­˜åœ¨ã™ã‚‹ã‚‚ã®ï¼‰
        valid_samples = []
        for sample in dataset:
            if 'review_by_client_jp' in sample and sample['review_by_client_jp']:
                valid_samples.append(sample)
        
        logger.info(f"æœ‰åŠ¹ãªã‚µãƒ³ãƒ—ãƒ«æ•°: {len(valid_samples)}")
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆå…¨ä½“ã®10%ã‚’ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä½¿ç”¨ï¼‰
        import random
        random.seed(42)  # å†ç¾æ€§ã®ãŸã‚
        
        test_size = int(len(valid_samples) * 0.1)
        test_data = random.sample(valid_samples, min(test_size, 150))  # æœ€å¤§150ã‚µãƒ³ãƒ—ãƒ«
        
        logger.info(f"=== ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ½å‡ºçµæœ ===")
        logger.info(f"æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿: {len(valid_samples)} ã‚µãƒ³ãƒ—ãƒ«")
        logger.info(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_data)} ã‚µãƒ³ãƒ—ãƒ« ({len(test_data)/len(valid_samples)*100:.1f}%)")
        logger.info(f"ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰: 42")
        
        return test_data

    def _find_latest_results_file(self) -> Path:
        """æœ€æ–°ã®ãƒãƒƒãƒçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¦‹ã¤ã‘ã‚‹"""
        result_files = list(self.output_dir.glob("batch_fine_tuning_results_*.json"))
        if not result_files:
            logger.warning(f"ãƒãƒƒãƒçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.output_dir}")
            return None
        
        latest_file = max(result_files, key=lambda x: x.stat().st_mtime)
        logger.info(f"ä½¿ç”¨ã™ã‚‹çµæœãƒ•ã‚¡ã‚¤ãƒ«: {latest_file}")
        return latest_file

    def load_fine_tuned_model_ids(self) -> List[str]:
        """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«IDã‚’èª­ã¿è¾¼ã‚€ï¼ˆOpenAI APIã‹ã‚‰ç›´æ¥å–å¾—ï¼‰"""
        try:
            from openai import OpenAI
            import os
            from dotenv import load_dotenv
            
            # .envãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            load_dotenv()
            api_key = os.getenv('OPENAI_API_KEY')
            
            if not api_key:
                logger.warning("OpenAI APIã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                return ["gpt-4o-mini"]
            
            client = OpenAI(api_key=api_key)
            
            # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—
            logger.info("OpenAI APIã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ä¸­...")
            jobs = client.fine_tuning.jobs.list(limit=20)
            
            model_ids = []
            for job in jobs.data:
                if job.status == 'succeeded' and hasattr(job, 'fine_tuned_model') and job.fine_tuned_model:
                    # gpt-4o-miniãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã‚’å„ªå…ˆ
                    if 'gpt-4o-mini' in job.fine_tuned_model:
                        model_ids.append(job.fine_tuned_model)
            
            # gpt-4o-miniãƒ™ãƒ¼ã‚¹ãŒãªã„å ´åˆã€ä»–ã®æˆåŠŸã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ 
            if not model_ids:
                for job in jobs.data:
                    if job.status == 'succeeded' and hasattr(job, 'fine_tuned_model') and job.fine_tuned_model:
                        model_ids.append(job.fine_tuned_model)
            
            if not model_ids:
                logger.warning("æœ‰åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                return ["gpt-4o-mini"]
            
            # æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ã®ã¿ã‚’ä½¿ç”¨ï¼ˆæœ€åˆã®1ã¤ï¼‰
            selected_model = model_ids[0]
            
            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨: {selected_model}")
            return [selected_model]
            
        except Exception as e:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            logger.info("ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            return ["gpt-4o-mini"]

    def evaluate_all_models_multi_item(self, max_test_samples: int = None):
        """å…¨ãƒ¢ãƒ‡ãƒ«ã‚’20é …ç›®ã§è©•ä¾¡ï¼ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯KokoChatãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥ä½¿ç”¨ï¼‰"""
        try:
            # KokoChatãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥èª­ã¿è¾¼ã¿
            test_data = self.load_kokorochat_data_directly(max_test_samples)
            
            # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«IDã‚’èª­ã¿è¾¼ã¿
            model_ids = self.load_fine_tuned_model_ids()
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return
        
        all_results = []
        all_metrics = {}
        
        for model_id in model_ids:
            # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®è©•ä¾¡å®Ÿè¡Œ
            predictions_data = self.evaluate_model_on_all_items(model_id, test_data)
            
            # é …ç›®ã”ã¨ã®ç²¾åº¦æŒ‡æ¨™è¨ˆç®—
            metrics_per_item = self.calculate_metrics_per_item(predictions_data)
            
            all_results.append(predictions_data)
            all_metrics[model_id] = metrics_per_item
        
        # çµæœã®ä¿å­˜
        self.save_multi_item_results(all_results, all_metrics)
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        self.print_multi_item_summary(all_metrics)

    def save_multi_item_results(self, all_results: List[Dict[str, Any]], all_metrics: Dict[str, Dict[str, Dict[str, float]]]):
        """20é …ç›®è©•ä¾¡çµæœã‚’ä¿å­˜"""
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # è©³ç´°çµæœã®ä¿å­˜
        detailed_output_path = self.output_dir / f"multi_item_detailed_results_{ts}.json"
        with open(detailed_output_path, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        logger.info(f"è©³ç´°çµæœã‚’ {detailed_output_path} ã«ä¿å­˜ã—ã¾ã—ãŸ")
        
        # ç²¾åº¦æŒ‡æ¨™ã®ä¿å­˜ï¼ˆCSVå½¢å¼ï¼‰
        metrics_rows = []
        for model_id, metrics_per_item in all_metrics.items():
            for evaluation_item, metrics in metrics_per_item.items():
                metrics_rows.append({
                    "model_id": model_id,
                    "evaluation_item": evaluation_item,
                    "mae": metrics["mae"],
                    "rmse": metrics["rmse"],
                    "accuracy_within_1": metrics["accuracy_within_1"],
                    "sample_count": metrics["sample_count"]
                })
        
        metrics_df = pd.DataFrame(metrics_rows)
        metrics_output_path = self.output_dir / f"multi_item_metrics_{ts}.csv"
        metrics_df.to_csv(metrics_output_path, index=False, encoding='utf-8-sig')
        logger.info(f"ç²¾åº¦æŒ‡æ¨™ã‚’ {metrics_output_path} ã«ä¿å­˜ã—ã¾ã—ãŸ")

    def print_multi_item_summary(self, all_metrics: Dict[str, Dict[str, Dict[str, float]]]):
        """20é …ç›®è©•ä¾¡çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        print("\n" + "="*80)
        print("ğŸ“Š 20é …ç›®è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼ ğŸ“Š")
        print("="*80)
        
        for model_id, metrics_per_item in all_metrics.items():
            print(f"\nğŸ¤– ãƒ¢ãƒ‡ãƒ«: {model_id}")
            print("-" * 60)
            
            # å„é …ç›®ã®çµæœã‚’è¡¨ç¤º
            valid_metrics = {k: v for k, v in metrics_per_item.items() if not np.isnan(v["mae"])}
            
            if not valid_metrics:
                print("âŒ æœ‰åŠ¹ãªè©•ä¾¡çµæœãŒã‚ã‚Šã¾ã›ã‚“")
                continue
            
            # å¹³å‡å€¤ã‚’è¨ˆç®—
            avg_mae = np.mean([m["mae"] for m in valid_metrics.values()])
            avg_rmse = np.mean([m["rmse"] for m in valid_metrics.values()])
            avg_accuracy = np.mean([m["accuracy_within_1"] for m in valid_metrics.values()])
            
            print(f"ğŸ“ˆ å…¨ä½“å¹³å‡:")
            print(f"   MAE (å¹³å‡çµ¶å¯¾èª¤å·®): {avg_mae:.3f}")
            print(f"   RMSE (äºŒä¹—å¹³å‡å¹³æ–¹æ ¹èª¤å·®): {avg_rmse:.3f}")
            print(f"   èª¤å·®1ã§ã®æ­£è§£ç‡: {avg_accuracy:.1f}%")
            print(f"   æœ‰åŠ¹é …ç›®æ•°: {len(valid_metrics)}/{len(EVALUATION_ITEMS)}")
            
            # æœ€è‰¯ãƒ»æœ€æ‚ªé …ç›®
            sorted_by_mae = sorted(valid_metrics.items(), key=lambda x: x[1]["mae"])
            
            print(f"\nğŸ† MAEæœ€è‰¯é …ç›® TOP3:")
            for item, metrics in sorted_by_mae[:3]:
                print(f"   {metrics['mae']:.3f} - {item}")
            
            print(f"\nâš ï¸ MAEæ”¹å–„é …ç›® TOP3:")
            for item, metrics in sorted_by_mae[-3:]:
                print(f"   {metrics['mae']:.3f} - {item}")
        
        print("\n" + "="*80)


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="20é …ç›®è©•ä¾¡ç²¾åº¦è¨ˆç®—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    parser.add_argument("--max-samples", type=int, help="è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«æ•°ã®ä¸Šé™")
    parser.add_argument("--debug", action="store_true", help="ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰")
    args = parser.parse_args()
    
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
    
    try:
        project_root = Path(__file__).resolve().parent.parent.parent.parent
        env_path = project_root / ".env"
        if env_path.exists():
            load_dotenv(env_path)
            logger.info(f".envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {env_path}")
        
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OpenAI APIã‚­ãƒ¼ãŒ.envãƒ•ã‚¡ã‚¤ãƒ«ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        
        print("ğŸš€ 20é …ç›®è©•ä¾¡ç²¾åº¦è¨ˆç®—ã‚’é–‹å§‹ã—ã¾ã™")
        print(f"ğŸ“‹ è©•ä¾¡é …ç›®æ•°: {len(EVALUATION_ITEMS)}")
        if args.max_samples:
            print(f"ğŸ“Š æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°: {args.max_samples}")
        
        evaluator = MultiItemModelEvaluator(api_key)
        evaluator.evaluate_all_models_multi_item(max_test_samples=args.max_samples)
        
        print(f"\nâœ… 20é …ç›®è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("ğŸ“ çµæœãƒ•ã‚¡ã‚¤ãƒ«ã¯ä»¥ä¸‹ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ:")
        print(f"   - è©³ç´°çµæœ: multi_item_detailed_results_*.json")
        print(f"   - ç²¾åº¦æŒ‡æ¨™: multi_item_metrics_*.csv")

    except Exception as e:
        logger.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)


if __name__ == "__main__":
    main()
