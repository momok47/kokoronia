#!/usr/bin/env python3
"""
0ã€œ6æ®µéšè©•ä¾¡ã®åˆ†å¸ƒã‚’æ£’ã‚°ãƒ©ãƒ•ã§å¯è¦–åŒ–ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ­£è§£ã€ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã€ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãªã—ãƒ¢ãƒ‡ãƒ«ã®åˆ†å¸ƒã‚’æ¯”è¼ƒ
"""

import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Any, Tuple
import logging
from datetime import datetime
import re
from collections import defaultdict

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['DejaVu Sans', 'Hiragino Sans', 'Yu Gothic', 'Meiryo', 'Takao', 'IPAexGothic', 'IPAPGothic', 'VL PGothic', 'Noto Sans CJK JP']

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger(__name__)

class ScoreDistributionVisualizer:
    """ã‚¹ã‚³ã‚¢åˆ†å¸ƒå¯è¦–åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, output_dir: Path = None):
        """
        åˆæœŸåŒ–
        
        Args:
            output_dir: çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨åŒã˜å ´æ‰€ã®openai_sft_outputsï¼‰
        """
        if output_dir is None:
            self.output_dir = Path(__file__).resolve().parent / "openai_sft_outputs"
        else:
            self.output_dir = output_dir
        
        logger.info(f"çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
        if not self.output_dir.exists():
            raise FileNotFoundError(f"çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.output_dir}")

    def load_ground_truth_distribution(self, model_id: str = None) -> Dict[int, int]:
        """æ­£è§£ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã‚’å–å¾—"""
        logger.info("æ­£è§£ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã‚’å–å¾—ä¸­...")
        
        # è©³ç´°çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ­£è§£ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        result_files = list(self.output_dir.glob("multi_item_detailed_results_*.json"))
        if not result_files:
            raise FileNotFoundError("è©³ç´°çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        ground_truth_scores = []
        
        # ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãªã—ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ­£è§£ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆã‚ˆã‚Šå¤šãã®ã‚µãƒ³ãƒ—ãƒ«ãŒã‚ã‚‹ãŸã‚ï¼‰
        target_file = None
        for result_file in sorted(result_files, key=lambda x: x.stat().st_mtime):
            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    results_data = json.load(f)
                
                # gpt-4o-miniï¼ˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãªã—ï¼‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
                for result in results_data:
                    if result.get('model_id') == 'gpt-4o-mini':
                        target_file = result_file
                        logger.info(f"ä½¿ç”¨ã™ã‚‹çµæœãƒ•ã‚¡ã‚¤ãƒ«: {target_file.name}")
                        
                        # æ­£è§£ã‚¹ã‚³ã‚¢ã‚’å–å¾—
                        for sample_pred in result['predictions']:
                            for item, correct_score in sample_pred['correct_scores'].items():
                                # 0-5ã®ç¯„å›²ã«ä¸¸ã‚ã‚‹
                                score = max(0, min(5, round(correct_score)))
                                ground_truth_scores.append(score)
                        break
                
                if target_file:
                    break
                    
            except Exception as e:
                logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ« {result_file.name} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
                continue
        
        if not target_file:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—
            latest_result_file = max(result_files, key=lambda x: x.stat().st_mtime)
            logger.info(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {latest_result_file.name}")
            
            with open(latest_result_file, 'r', encoding='utf-8') as f:
                results_data = json.load(f)
            
            if results_data:
                first_model_data = results_data[0]
                for sample_pred in first_model_data['predictions']:
                    for item, correct_score in sample_pred['correct_scores'].items():
                        # 0-5ã®ç¯„å›²ã«ä¸¸ã‚ã‚‹
                        score = max(0, min(5, round(correct_score)))
                        ground_truth_scores.append(score)
        
        if not ground_truth_scores:
            raise ValueError("æ­£è§£ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # åˆ†å¸ƒã‚’è¨ˆç®—
        distribution = {}
        for score in range(6):  # 0-5
            distribution[score] = ground_truth_scores.count(score)
        
        total_samples = len(ground_truth_scores)
        logger.info(f"æ­£è§£ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {total_samples}ã‚µãƒ³ãƒ—ãƒ«")
        logger.info(f"æ­£è§£åˆ†å¸ƒ: {distribution}")
        
        return distribution

    def load_model_predictions_distribution(self, model_id: str) -> Dict[int, int]:
        """æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬åˆ†å¸ƒã‚’å–å¾—"""
        logger.info(f"ãƒ¢ãƒ‡ãƒ« {model_id} ã®äºˆæ¸¬åˆ†å¸ƒã‚’å–å¾—ä¸­...")
        
        # è©³ç´°çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        result_files = list(self.output_dir.glob("multi_item_detailed_results_*.json"))
        if not result_files:
            raise FileNotFoundError("è©³ç´°çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # å…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ãƒ¢ãƒ‡ãƒ«IDã‚’æ¢ã™
        model_data = None
        used_file = None
        
        for result_file in sorted(result_files, key=lambda x: x.stat().st_mtime, reverse=True):
            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    results_data = json.load(f)
                
                # ãƒ¢ãƒ‡ãƒ«IDã«å¯¾å¿œã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’æ¢ã™
                for result in results_data:
                    if result.get('model_id') == model_id:
                        model_data = result
                        used_file = result_file
                        break
                
                if model_data:
                    break
                    
            except Exception as e:
                logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ« {result_file.name} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
                continue
        
        if not model_data:
            raise ValueError(f"ãƒ¢ãƒ‡ãƒ« {model_id} ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        logger.info(f"ä½¿ç”¨ã™ã‚‹çµæœãƒ•ã‚¡ã‚¤ãƒ«: {used_file.name}")
        
        try:
            # äºˆæ¸¬ã‚¹ã‚³ã‚¢ã‚’åé›†
            predicted_scores = []
            for sample_pred in model_data['predictions']:
                for item, prediction in sample_pred['predictions'].items():
                    # 0-5ã®ç¯„å›²ã«ä¸¸ã‚ã‚‹
                    score = max(0, min(5, round(prediction)))
                    predicted_scores.append(score)
            
            # åˆ†å¸ƒã‚’è¨ˆç®—
            distribution = {}
            for score in range(6):  # 0-5
                distribution[score] = predicted_scores.count(score)
            
            total_predictions = len(predicted_scores)
            logger.info(f"ãƒ¢ãƒ‡ãƒ« {model_id} ã®äºˆæ¸¬èª­ã¿è¾¼ã¿å®Œäº†: {total_predictions}äºˆæ¸¬")
            logger.info(f"äºˆæ¸¬åˆ†å¸ƒ: {distribution}")
            
            return distribution
            
        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ« {model_id} ã®äºˆæ¸¬èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def create_distribution_comparison_plot(self, 
                                         ground_truth_dist: Dict[int, int],
                                         tuned_model_dist: Dict[int, int],
                                         untuned_model_dist: Dict[int, int],
                                         output_path: Path = None):
        """åˆ†å¸ƒæ¯”è¼ƒã®æ£’ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ"""
        logger.info("åˆ†å¸ƒæ¯”è¼ƒã‚°ãƒ©ãƒ•ã‚’ä½œæˆä¸­...")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        scores = list(range(6))  # 0-5
        
        # æ­£è¦åŒ–ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ï¼‰
        total_gt = sum(ground_truth_dist.values())
        total_tuned = sum(tuned_model_dist.values())
        total_untuned = sum(untuned_model_dist.values())
        
        gt_percentages = [ground_truth_dist[score] / total_gt * 100 for score in scores]
        tuned_percentages = [tuned_model_dist[score] / total_tuned * 100 for score in scores]
        untuned_percentages = [untuned_model_dist[score] / total_untuned * 100 for score in scores]
        
        # ã‚°ãƒ©ãƒ•ã®è¨­å®š
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # ãƒãƒ¼ã®å¹…ã¨ä½ç½®
        bar_width = 0.25
        x_pos = np.arange(len(scores))
        
        # æ£’ã‚°ãƒ©ãƒ•ã‚’æç”»
        bars1 = ax.bar(x_pos - bar_width, gt_percentages, bar_width, 
                      label='æ­£è§£', color='#61C5FF', alpha=0.8)
        bars2 = ax.bar(x_pos, tuned_percentages, bar_width,
                      label='ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«', color='#FFB550', alpha=0.8)
        bars3 = ax.bar(x_pos + bar_width, untuned_percentages, bar_width,
                      label='ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãªã—ãƒ¢ãƒ‡ãƒ«', color='#FFF5AE', alpha=0.8)
        
        # ã‚°ãƒ©ãƒ•ã®è£…é£¾
        ax.set_xlabel('è©•ä¾¡ã‚¹ã‚³ã‚¢', fontsize=24, fontweight='bold')
        ax.set_ylabel('åˆ†å¸ƒ (%)', fontsize=24, fontweight='bold')
        ax.set_title('0ã€œ5æ®µéšè©•ä¾¡ã®åˆ†å¸ƒæ¯”è¼ƒ', fontsize=28, fontweight='bold', pad=20)
        ax.set_xticks(x_pos)
        ax.set_xticklabels([f'{i}ç‚¹' for i in scores], fontsize=20)
        ax.legend(fontsize=20)
        ax.grid(True, alpha=0.3)
        
        
        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆèª¿æ•´
        plt.tight_layout()
        
        # ä¿å­˜
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = self.output_dir / f"score_distribution_comparison_{timestamp}.png"
        
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        logger.info(f"åˆ†å¸ƒæ¯”è¼ƒã‚°ãƒ©ãƒ•ã‚’ä¿å­˜: {output_path}")
        
        # è¡¨ç¤º
        plt.show()
        
        return output_path



    def print_distribution_summary(self, 
                                 ground_truth_dist: Dict[int, int],
                                 tuned_model_dist: Dict[int, int],
                                 untuned_model_dist: Dict[int, int]):
        """åˆ†å¸ƒã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        print("\n" + "="*60)
        print("ğŸ“Š ã‚¹ã‚³ã‚¢åˆ†å¸ƒã‚µãƒãƒªãƒ¼ ğŸ“Š")
        print("="*60)
        
        print(f"\nğŸ¯ æ­£è§£ãƒ‡ãƒ¼ã‚¿:")
        total_gt = sum(ground_truth_dist.values())
        for score in range(6):
            count = ground_truth_dist[score]
            percentage = count / total_gt * 100
            print(f"  {score}ç‚¹: {count:3d}ä»¶ ({percentage:5.1f}%)")
        
        print(f"\nğŸ¤– ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«:")
        total_tuned = sum(tuned_model_dist.values())
        for score in range(6):
            count = tuned_model_dist[score]
            percentage = count / total_tuned * 100
            print(f"  {score}ç‚¹: {count:3d}ä»¶ ({percentage:5.1f}%)")
        
        print(f"\nğŸ”§ ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãªã—ãƒ¢ãƒ‡ãƒ«:")
        total_untuned = sum(untuned_model_dist.values())
        for score in range(6):
            count = untuned_model_dist[score]
            percentage = count / total_untuned * 100
            print(f"  {score}ç‚¹: {count:3d}ä»¶ ({percentage:5.1f}%)")
        
        print("\n" + "="*60)




    def visualize_all_distributions(self, tuned_model_id: str, untuned_model_id: str = None):
        """å…¨ã¦ã®åˆ†å¸ƒå¯è¦–åŒ–ã‚’å®Ÿè¡Œ"""
        try:
            # åˆ†å¸ƒãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            ground_truth_dist = self.load_ground_truth_distribution()
            tuned_model_dist = self.load_model_predictions_distribution(tuned_model_id)
            
            # ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãªã—ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            untuned_model_dist = None
            if untuned_model_id:
                try:
                    untuned_model_dist = self.load_model_predictions_distribution(untuned_model_id)
                except ValueError as e:
                    logger.warning(f"ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãªã—ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
                    untuned_model_dist = None
            
            # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
            self.print_distribution_summary(ground_truth_dist, tuned_model_dist, untuned_model_dist)
            
            # æ¯”è¼ƒã‚°ãƒ©ãƒ•ä½œæˆ
            comparison_path = self.create_distribution_comparison_plot(
                ground_truth_dist, tuned_model_dist, untuned_model_dist
            )
            
            print(f"\nâœ… åˆ†å¸ƒå¯è¦–åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            print(f"ğŸ“ çµæœã¯ {self.output_dir} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
            
        except Exception as e:
            logger.error(f"åˆ†å¸ƒå¯è¦–åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            raise

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    try:
        # çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š
        script_dir = Path(__file__).resolve().parent
        output_dir = script_dir / "openai_sft_outputs"
        
        if not output_dir.exists():
            raise FileNotFoundError(f"çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {output_dir}")
        
        # å¯è¦–åŒ–å®Ÿè¡Œ
        visualizer = ScoreDistributionVisualizer(output_dir)
        
        # ãƒ¢ãƒ‡ãƒ«IDã‚’æŒ‡å®šï¼ˆå®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«IDã«å¤‰æ›´ã—ã¦ãã ã•ã„ï¼‰
        tuned_model_id = "ft:gpt-4o-mini-2024-07-18:personal::CAZPNbKA"  # å®Ÿéš›ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ID
        untuned_model_id = "gpt-4o-mini"  # ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãªã—ãƒ¢ãƒ‡ãƒ«ã®ID
        
        visualizer.visualize_all_distributions(tuned_model_id, untuned_model_id)
        
    except Exception as e:
        logger.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)

if __name__ == "__main__":
    main()
