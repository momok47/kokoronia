#!/usr/bin/env python3
"""
20é …ç›®ã®ä¼šè©±å°è±¡è©•ä¾¡ã«å¯¾ã™ã‚‹äºˆæ¸¬ç²¾åº¦è¨ˆç®—ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆevaluate_models_accuracy.pyãƒ™ãƒ¼ã‚¹ï¼‰
Kokorochatãƒ‡ãƒ¼ã‚¿ã‹ã‚‰150ä»¶ã‚’æŠ½å‡ºã—ã€å„é …ç›®ã”ã¨ã«MAEã€RMSEã€èª¤å·®1ã§ã®æ­£è§£ç‡ã‚’è¨ˆç®—ã™ã‚‹æ©Ÿèƒ½ä»˜ã
"""

import os
import json
import logging
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI
import time
from typing import List, Dict, Any, Tuple, Optional
import numpy as np
from datetime import datetime
import pandas as pd
import re
from sklearn.metrics import mean_absolute_error, mean_squared_error
from datasets import load_dataset
import random
import argparse

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger(__name__)

# 20é …ç›®ã®è©•ä¾¡æŒ‡æ¨™
EVALUATION_ITEMS = [
    "è´ã„ã¦ã‚‚ã‚‰ãˆãŸã€ã‚ã‹ã£ã¦ã‚‚ã‚‰ãˆãŸã¨æ„Ÿã˜ãŸ",
    "å°Šé‡ã•ã‚ŒãŸã¨æ„Ÿã˜ãŸ",
    "æ–°ã—ã„æ°—ã¥ãã‚„ä½“é¨“ãŒã‚ã£ãŸ",
    "å¸Œæœ›ã‚„æœŸå¾…ã‚’æ„Ÿã˜ã‚‰ã‚ŒãŸ",
    "å–ã‚Šçµ„ã¿ãŸã‹ã£ãŸã“ã¨ã‚’æ‰±ãˆãŸ",
    "ä¸€ç·’ã«è€ƒãˆãªãŒã‚‰å–ã‚Šçµ„ã‚ãŸ",
    "ã‚„ã‚Šã¨ã‚Šã®ãƒªã‚ºãƒ ãŒã‚ã£ã¦ã„ãŸ",
    "å±…å¿ƒåœ°ã®ã‚ˆã„ã‚„ã‚Šã¨ã‚Šã ã£ãŸ",
    "å…¨ä½“ã¨ã—ã¦é©åˆ‡ã§ã‚ˆã‹ã£ãŸ",
    "ä»Šå›ã®ç›¸è«‡ã¯ä¾¡å€¤ãŒã‚ã£ãŸ",
    "ç›¸è«‡é–‹å§‹ã®å††æ»‘ã•",
    "ç›¸è«‡çµ‚äº†ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ï¼ˆä¸å¿…è¦ã«è´ãã™ãã¦ã„ãªã„ã‹ï¼‰ã€å††æ»‘ã•",
    "å—å®¹ãƒ»å…±æ„Ÿ",
    "è‚¯å®šãƒ»æ‰¿èª",
    "çš„ç¢ºãªè³ªå•ã«ã‚ˆã‚‹ä¼šè©±ã®ä¿ƒé€²",
    "è¦ç´„",
    "å•é¡Œã®æ˜ç¢ºåŒ–",
    "ã“ã®ç›¸è«‡ã§ã®ç›®æ¨™ã®æ˜ç¢ºåŒ–",
    "æ¬¡ã®è¡Œå‹•ã«ã¤ãªãŒã‚‹ææ¡ˆ",
    "å‹‡æ°—ã¥ã‘ãƒ»å¸Œæœ›ã®å–šèµ·"
]


class MultiItemModelAccuracyEvaluator:
    """20é …ç›®è©•ä¾¡å¯¾å¿œã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«æ­£è§£ç‡è©•ä¾¡ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, api_key: str):
        """
        åˆæœŸåŒ–
        
        Args:
            api_key: OpenAI APIã‚­ãƒ¼
        """
        self.client = OpenAI(api_key=api_key)
        # ã©ã®éšå±¤ã‹ã‚‰å®Ÿè¡Œã•ã‚Œã¦ã‚‚æ­£ã—ããƒ‘ã‚¹ã‚’æŒ‡å®šã§ãã‚‹ã‚ˆã†ã«ã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆè‡ªèº«ã®å ´æ‰€ã‚’åŸºæº–ã«ã™ã‚‹
        self.script_dir = Path(__file__).resolve().parent
        self.output_dir = self.script_dir / "openai_sft_outputs"
        logger.info(f"çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š: {self.output_dir}")
        self.output_dir.mkdir(exist_ok=True) # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒãªã‘ã‚Œã°ä½œæˆ

    def load_kokorochat_test_dataset(self, max_samples: Optional[int] = 1500, seed: int = 42) -> List[Dict]:
        """
        Kokoro Chatãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡ºã™ã‚‹ï¼ˆ1500ä»¶ã®10%=150ä»¶ï¼‰
        
        Args:
            max_samples: æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1500ï¼‰
            seed: å†ç¾æ€§ã®ãŸã‚ã®ä¹±æ•°ã‚·ãƒ¼ãƒ‰
            
        Returns:
            test_data: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        logger.info("KokoroChat ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­...")
        dataset = load_dataset("UEC-InabaLab/KokoroChat", split="train")
        
        if max_samples:
            dataset = dataset.select(range(min(max_samples, len(dataset))))
            logger.info(f"ãƒ‡ãƒãƒƒã‚°ç”¨ã«{max_samples}ã‚µãƒ³ãƒ—ãƒ«ã«åˆ¶é™")
        
        logger.info(f"å…ƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: {len(dataset)}")
        
        # æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—ï¼ˆreview_by_client_jpãŒå­˜åœ¨ã™ã‚‹ã‚‚ã®ï¼‰
        valid_indices = []
        for i, sample in enumerate(dataset):
            if 'review_by_client_jp' in sample and sample['review_by_client_jp']:
                review_data = sample['review_by_client_jp']
                # 20é …ç›®ã®è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                valid_items = sum(1 for item in EVALUATION_ITEMS if item in review_data and isinstance(review_data[item], (int, float)))
                if valid_items >= 15:  # 20é …ç›®ä¸­15é …ç›®ä»¥ä¸Šã‚ã‚Œã°æœ‰åŠ¹ã¨ã™ã‚‹
                    valid_indices.append(i)
        
        logger.info(f"æœ‰åŠ¹ãªã‚µãƒ³ãƒ—ãƒ«æ•°: {len(valid_indices)}")
        
        if len(valid_indices) == 0:
            raise ValueError("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«
        random.seed(seed)
        shuffled_indices = random.sample(valid_indices, len(valid_indices))
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡ºï¼ˆå…¨ä½“ã®10%ï¼‰
        total_size = len(shuffled_indices)
        train_size = int(total_size * 0.8)
        test_size = int(total_size * 0.1)
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
        test_indices = shuffled_indices[train_size:train_size + test_size]
        test_dataset = dataset.select(test_indices)
        
        logger.info("=== ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ½å‡ºçµæœ ===")
        logger.info(f"æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿: {total_size} ã‚µãƒ³ãƒ—ãƒ«")
        logger.info(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_dataset)} ã‚µãƒ³ãƒ—ãƒ« ({len(test_dataset)/total_size*100:.1f}%)")
        logger.info(f"ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰: {seed}")
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆå½¢å¼ã«å¤‰æ›
        test_data = self._convert_dataset_to_list(test_dataset)
        
        return test_data

    def _convert_dataset_to_list(self, dataset) -> List[Dict[str, Any]]:
        """Hugging Faceãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒªã‚¹ãƒˆå½¢å¼ã«å¤‰æ›"""
        converted_data = []
        for sample in dataset:
            # ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
            conversation_text = self._extract_conversation_text(sample["dialogue"])
            
            # æ­£è§£ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡º
            correct_scores = self._extract_correct_scores(sample)
            
            if correct_scores:
                converted_data.append({
                    "conversation_text": conversation_text,
                    "correct_scores": correct_scores,
                    "original_sample": sample
                })
        
        return converted_data

    def _extract_conversation_text(self, dialogue: List[Dict]) -> str:
        """å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        conversation_parts = []
        for turn in dialogue:
            role = "ç›¸è«‡è€…" if turn["role"] == "client" else "ã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼"
            conversation_parts.append(f"{role}: {turn['utterance']}")
        return "\n".join(conversation_parts)

    def _extract_correct_scores(self, sample: Dict[str, Any]) -> Dict[str, float]:
        """ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰20é …ç›®ã®æ­£è§£ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡º"""
        correct_scores = {}
        review_data = sample.get('review_by_client_jp', {})
        
        for item in EVALUATION_ITEMS:
            if item in review_data and isinstance(review_data[item], (int, float)):
                correct_scores[item] = float(review_data[item])
        
        return correct_scores

    def _find_latest_results_file(self) -> Path:
        """æœ€æ–°ã®ãƒãƒƒãƒçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¦‹ã¤ã‘ã‚‹"""
        result_files = list(self.output_dir.glob("batch_fine_tuning_results_*.json"))
        if not result_files:
            raise FileNotFoundError(f"ãƒãƒƒãƒçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.output_dir}")
        
        latest_file = max(result_files, key=lambda x: x.stat().st_mtime)
        logger.info(f"ä½¿ç”¨ã™ã‚‹çµæœãƒ•ã‚¡ã‚¤ãƒ«: {latest_file}")
        return latest_file
    
    def load_test_data_and_models(self, use_kokorochat: bool = True, max_samples: int = 1500, seed: int = 42) -> Tuple[List[str], List[Dict[str, Any]]]:
        """
        ãƒ¢ãƒ‡ãƒ«IDã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
        
        Args:
            use_kokorochat: Kokorochatãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥ä½¿ç”¨ã™ã‚‹ã‹
            max_samples: æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°
            seed: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
            
        Returns:
            (ãƒ¢ãƒ‡ãƒ«IDã®ãƒªã‚¹ãƒˆ, testãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ)
        """
        if use_kokorochat:
            # Kokorochatãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥ä½¿ç”¨
            logger.info("Kokorochatãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥ä½¿ç”¨ã—ã¾ã™")
            test_data = self.load_kokorochat_test_dataset(max_samples=max_samples, seed=seed)
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«IDã‚’è¨­å®šï¼ˆå®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒãªã„å ´åˆï¼‰
            model_ids = ["gpt-4o-mini"]  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«
            
            return model_ids, test_data
        else:
            # å¾“æ¥ã®æ–¹æ³•ï¼ˆãƒãƒƒãƒçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
            return self._load_from_batch_results()

    def _load_from_batch_results(self) -> Tuple[List[str], List[Dict[str, Any]]]:
        """ãƒãƒƒãƒçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«IDã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ï¼ˆå¾“æ¥ã®æ–¹æ³•ï¼‰"""
        latest_results_file = self._find_latest_results_file()
        
        with open(latest_results_file, 'r', encoding='utf-8') as f:
            results_data = json.load(f)

        # ãƒ¢ãƒ‡ãƒ«IDã®èª­ã¿è¾¼ã¿ - ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«å¿œã˜ã¦ä¿®æ­£
        model_ids = []
        if 'batches' in results_data:
            # æ–°ã—ã„å½¢å¼: batchesã‚­ãƒ¼ãŒã‚ã‚‹å ´åˆ
            for batch in results_data['batches']:
                if 'fine_tuned_model' in batch:
                    model_ids.append(batch['fine_tuned_model'])
        elif 'batch_results' in results_data:
            # å¤ã„å½¢å¼: batch_resultsã‚­ãƒ¼ãŒã‚ã‚‹å ´åˆ
            for batch in results_data['batch_results']:
                if batch.get('final_status') == 'succeeded' and 'final_model_id' in batch:
                    model_ids.append(batch['final_model_id'])
        else:
            # ãã®ä»–ã®å½¢å¼ã‚’è©¦ã™
            logger.warning(f"äºˆæœŸã—ãªã„ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã§ã™: {list(results_data.keys())}")
            # ç›´æ¥ãƒ¢ãƒ‡ãƒ«IDãŒå«ã¾ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯
            for key, value in results_data.items():
                if isinstance(value, list) and value:
                    for item in value:
                        if isinstance(item, dict):
                            if 'fine_tuned_model' in item:
                                model_ids.append(item['fine_tuned_model'])
                            elif 'final_model_id' in item:
                                model_ids.append(item['final_model_id'])
        
        if not model_ids:
            raise ValueError(f"çµæœãƒ•ã‚¡ã‚¤ãƒ« {latest_results_file.name} ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«IDã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        logger.info(f"è©•ä¾¡å¯¾è±¡ãƒ¢ãƒ‡ãƒ«æ•°: {len(model_ids)}")
        for i, model_id in enumerate(model_ids):
            logger.info(f"  ãƒ¢ãƒ‡ãƒ« {i+1}: {model_id}")

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾— - è¤‡æ•°ã®æ–¹æ³•ã‚’è©¦ã™
        test_data_path = None
        
        # æ–¹æ³•1: test_data_fileã‚­ãƒ¼ã‹ã‚‰å–å¾—
        if 'test_data_file' in results_data:
            test_data_filename = results_data['test_data_file']
            test_data_path = self.output_dir / test_data_filename
            logger.info(f"test_data_fileã‚­ãƒ¼ã‹ã‚‰å–å¾—: {test_data_filename}")
        
        # æ–¹æ³•2: ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ¨æ¸¬
        if not test_data_path or not test_data_path.exists():
            test_files = list(self.output_dir.glob("test_data_*.jsonl"))
            if test_files:
                # æœ€æ–°ã®testãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
                latest_test_file = max(test_files, key=lambda x: x.stat().st_mtime)
                test_data_path = latest_test_file
                logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ¨æ¸¬: {test_data_path.name}")
        
        # æ–¹æ³•3: ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ¨æ¸¬ï¼ˆã‚ˆã‚ŠæŸ”è»Ÿã«ï¼‰
        if not test_data_path or not test_data_path.exists():
            test_files = list(self.output_dir.glob("*test*data*.jsonl"))
            if test_files:
                # æœ€æ–°ã®testãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
                latest_test_file = max(test_files, key=lambda x: x.stat().st_mtime)
                test_data_path = latest_test_file
                logger.info(f"æŸ”è»Ÿãªæ¤œç´¢ã§ç™ºè¦‹: {test_data_path.name}")
        
        if not test_data_path or not test_data_path.exists():
            raise FileNotFoundError(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
        
        logger.info(f"ä½¿ç”¨ã™ã‚‹testãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: {test_data_path}")

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        test_data = []
        try:
            with open(test_data_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if line:  # ç©ºè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                        try:
                            test_data.append(json.loads(line))
                        except json.JSONDecodeError as e:
                            logger.warning(f"è¡Œ {line_num} ã®JSONè§£æã«å¤±æ•—: {e}")
                            continue
            
            logger.info(f"testãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(test_data)}ã‚µãƒ³ãƒ—ãƒ«")
            
            # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ç¢ºèª
            if test_data:
                sample_keys = list(test_data[0].keys())
                logger.info(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ : {sample_keys}")
                
                # messagesã‚­ãƒ¼ã®å­˜åœ¨ç¢ºèª
                if 'messages' in test_data[0]:
                    first_messages = test_data[0]['messages']
                    if first_messages:
                        logger.info(f"æœ€åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹é€ : {[msg.get('role', 'unknown') for msg in first_messages]}")
        
        except Exception as e:
            logger.error(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            raise
        
        return model_ids, test_data

    def extract_score_from_response(self, response_text: str) -> Tuple[float, str, bool, Dict[int, float]]:
        """
        å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç¢ºç‡åˆ†å¸ƒã¨æœŸå¾…å€¤ã‚’æŠ½å‡º
        """
        logger.debug(f"å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ: {response_text}")
        
        probability_patterns = [
            r'(\d+)ç‚¹\s*[:ï¼š]\s*(\d+(?:\.\d+)?)%',
            r'(\d+)ç‚¹\s+(\d+(?:\.\d+)?)%',
        ]
        
        probabilities = {}
        for pattern in probability_patterns:
            matches = re.findall(pattern, response_text)
            for match in matches:
                point, prob = int(match[0]), float(match[1])
                if 0 <= point <= 5 and point not in probabilities:
                    probabilities[point] = prob
        
        total_probability = sum(probabilities.values())
        
        if len(probabilities) == 6 and abs(total_probability - 100.0) < 10.0:
            expected_value = sum(p * (pr / 100.0) for p, pr in probabilities.items())
            return expected_value, "æŠ½å‡ºæˆåŠŸ", False, probabilities
        else:
            logger.warning(f"ç¢ºç‡åˆ†å¸ƒã®å½¢å¼ãŒä¸æ­£: åˆè¨ˆ={total_probability}%, æ•°={len(probabilities)}")
            return 0.0, "response error", True, {}

    def evaluate_model_on_all_items(self, model_id: str, test_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’20é …ç›®ã™ã¹ã¦ã§è©•ä¾¡"""
        logger.info(f"ãƒ¢ãƒ‡ãƒ« {model_id} ã®20é …ç›®è©•ä¾¡ã‚’é–‹å§‹...")
        
        all_predictions = []
        
        for sample_idx, sample in enumerate(test_data):
            try:
                # ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                conversation_text = sample.get("conversation_text", "")
                if not conversation_text:
                    logger.warning(f"ã‚µãƒ³ãƒ—ãƒ« {sample_idx+1}: ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    continue
                
                # æ­£è§£ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                correct_scores = sample.get("correct_scores", {})
                if not correct_scores:
                    logger.warning(f"ã‚µãƒ³ãƒ—ãƒ« {sample_idx+1}: æ­£è§£ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    continue
                
                sample_predictions = {
                    "sample_index": sample_idx,
                    "conversation_text": conversation_text[:200] + "...",  # è¡¨ç¤ºç”¨ã«çŸ­ç¸®
                    "predictions": {},
                    "correct_scores": correct_scores,
                    "errors": {}
                }
                
                # å„è©•ä¾¡é …ç›®ã«ã¤ã„ã¦äºˆæ¸¬ã‚’å®Ÿè¡Œ
                for item_idx, evaluation_item in enumerate(EVALUATION_ITEMS):
                    if evaluation_item not in correct_scores:
                        logger.debug(f"é …ç›® '{evaluation_item}' ã®æ­£è§£ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                        continue
                        
                    try:
                        logger.info(f"ã‚µãƒ³ãƒ—ãƒ« {sample_idx+1}/{len(test_data)}, é …ç›® {item_idx+1}/{len(EVALUATION_ITEMS)}: {evaluation_item}")
                        
                        # ãƒ¢ãƒ‡ãƒ«ã«è³ªå•ã‚’é€ä¿¡
                        response = self.client.chat.completions.create(
                            model=model_id,
                            messages=[
                                {"role": "system", "content": "ã‚ãªãŸã¯å¿ƒç†ã‚«ã‚¦ãƒ³ã‚»ãƒªãƒ³ã‚°ã®è©•ä¾¡è€…ã§ã™ã€‚ç›¸è«‡è€…ã®æº€è¶³åº¦ã‚’0ï½5ç‚¹ã§è©•ä¾¡ã—ã€å„ç‚¹æ•°ã®ç¢ºç‡ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"},
                                {"role": "user", "content": f"""### æŒ‡ç¤º
ä»¥ä¸‹ã®å¯¾è©±ã«ã¤ã„ã¦ã€Œ{evaluation_item}ã€ã®æº€è¶³åº¦ã‚’ç›¸è«‡è€…ã®è¦–ç‚¹ã§0ï½5ç‚¹ã§è©•ä¾¡ã—ã€å„ç‚¹æ•°ã®ç¢ºç‡ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

### å¯¾è©±
{conversation_text}

### å‡ºåŠ›å½¢å¼ï¼ˆæ•°å€¤ã®ã¿ï¼‰
0ç‚¹: XX%
1ç‚¹: XX%
2ç‚¹: XX%
3ç‚¹: XX%
4ç‚¹: XX%
5ç‚¹: XX%"""}
                            ],
                            max_tokens=200,
                            temperature=0.7
                        )
                        
                        response_text = response.choices[0].message.content.strip()
                        expected_value, _, is_error, probabilities = self.extract_score_from_response(response_text)
                        
                        if not is_error:
                            sample_predictions["predictions"][evaluation_item] = expected_value
                        else:
                            sample_predictions["errors"][evaluation_item] = response_text
                            logger.warning(f"é …ç›® '{evaluation_item}' ã®äºˆæ¸¬ã«å¤±æ•—")
                        
                        time.sleep(1)  # APIåˆ¶é™å¯¾ç­–
                        
                    except Exception as e:
                        logger.error(f"é …ç›® '{evaluation_item}' ã®è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                        sample_predictions["errors"][evaluation_item] = str(e)
                
                all_predictions.append(sample_predictions)
                
            except Exception as e:
                logger.error(f"ã‚µãƒ³ãƒ—ãƒ« {sample_idx+1} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        return {
            "model_id": model_id,
            "total_samples": len(test_data),
            "predictions": all_predictions
        }

    def calculate_metrics_per_item(self, predictions_data: Dict[str, Any]) -> Dict[str, Dict[str, float]]:
        """é …ç›®ã”ã¨ã«MAEã€RMSEã€èª¤å·®1ã§ã®æ­£è§£ç‡ã‚’è¨ˆç®—"""
        logger.info("é …ç›®ã”ã¨ã®ç²¾åº¦æŒ‡æ¨™ã‚’è¨ˆç®—ä¸­...")
        
        metrics_per_item = {}
        
        for evaluation_item in EVALUATION_ITEMS:
            predicted_scores = []
            correct_scores = []
            
            # å„ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰è©²å½“é …ç›®ã®äºˆæ¸¬å€¤ã¨æ­£è§£å€¤ã‚’åé›†
            for sample_pred in predictions_data["predictions"]:
                if evaluation_item in sample_pred["predictions"] and evaluation_item in sample_pred["correct_scores"]:
                    predicted_scores.append(sample_pred["predictions"][evaluation_item])
                    correct_scores.append(sample_pred["correct_scores"][evaluation_item])
            
            if len(predicted_scores) == 0:
                logger.warning(f"é …ç›® '{evaluation_item}' ã®æœ‰åŠ¹ãªäºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                metrics_per_item[evaluation_item] = {
                    "mae": float('nan'),
                    "rmse": float('nan'),
                    "accuracy_within_1": float('nan'),
                    "sample_count": 0
                }
                continue
            
            # MAEï¼ˆå¹³å‡çµ¶å¯¾èª¤å·®ï¼‰
            mae = mean_absolute_error(correct_scores, predicted_scores)
            
            # RMSEï¼ˆäºŒä¹—å¹³å‡å¹³æ–¹æ ¹èª¤å·®ï¼‰
            rmse = np.sqrt(mean_squared_error(correct_scores, predicted_scores))
            
            # èª¤å·®1ã§ã®æ­£è§£ç‡
            errors = np.abs(np.array(predicted_scores) - np.array(correct_scores))
            accuracy_within_1 = np.mean(errors <= 1.0) * 100  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸
            
            metrics_per_item[evaluation_item] = {
                "mae": mae,
                "rmse": rmse,
                "accuracy_within_1": accuracy_within_1,
                "sample_count": len(predicted_scores)
            }
            
            logger.info(f"é …ç›® '{evaluation_item}': MAE={mae:.3f}, RMSE={rmse:.3f}, èª¤å·®1æ­£è§£ç‡={accuracy_within_1:.1f}%")
        
        return metrics_per_item

    def evaluate_all_models(self, max_test_samples: int = None, use_kokorochat: bool = True, max_samples: int = 1500, seed: int = 42):
        """å…¨ãƒ¢ãƒ‡ãƒ«ã‚’20é …ç›®ã§è©•ä¾¡"""
        try:
            model_ids, test_data = self.load_test_data_and_models(
                use_kokorochat=use_kokorochat, 
                max_samples=max_samples, 
                seed=seed
            )
        except (FileNotFoundError, KeyError, IndexError) as e:
            logger.error(e)
            return
            
        if max_test_samples:
            test_data = test_data[:max_test_samples]
            logger.info(f"è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’ {max_test_samples} ã«åˆ¶é™")
        
        all_results = []
        all_metrics = {}
        
        for model_id in model_ids:
            # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®è©•ä¾¡å®Ÿè¡Œ
            predictions_data = self.evaluate_model_on_all_items(model_id, test_data)
            
            # é …ç›®ã”ã¨ã®ç²¾åº¦æŒ‡æ¨™è¨ˆç®—
            metrics_per_item = self.calculate_metrics_per_item(predictions_data)
            
            all_results.append(predictions_data)
            all_metrics[model_id] = metrics_per_item
        
        # çµæœã®ä¿å­˜
        self.save_multi_item_results(all_results, all_metrics)
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        self.print_multi_item_summary(all_metrics)

    def save_multi_item_results(self, all_results: List[Dict[str, Any]], all_metrics: Dict[str, Dict[str, Dict[str, float]]]):
        """20é …ç›®è©•ä¾¡çµæœã‚’ä¿å­˜"""
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # è©³ç´°çµæœã®ä¿å­˜
        detailed_output_path = self.output_dir / f"multi_item_detailed_results_{ts}.json"
        with open(detailed_output_path, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        logger.info(f"è©³ç´°çµæœã‚’ {detailed_output_path} ã«ä¿å­˜ã—ã¾ã—ãŸ")
        
        # ç²¾åº¦æŒ‡æ¨™ã®ä¿å­˜ï¼ˆCSVå½¢å¼ï¼‰
        metrics_rows = []
        for model_id, metrics_per_item in all_metrics.items():
            for evaluation_item, metrics in metrics_per_item.items():
                metrics_rows.append({
                    "model_id": model_id,
                    "evaluation_item": evaluation_item,
                    "mae": metrics["mae"],
                    "rmse": metrics["rmse"],
                    "accuracy_within_1": metrics["accuracy_within_1"],
                    "sample_count": metrics["sample_count"]
                })
        
        metrics_df = pd.DataFrame(metrics_rows)
        metrics_output_path = self.output_dir / f"multi_item_metrics_{ts}.csv"
        metrics_df.to_csv(metrics_output_path, index=False, encoding='utf-8-sig')
        logger.info(f"ç²¾åº¦æŒ‡æ¨™ã‚’ {metrics_output_path} ã«ä¿å­˜ã—ã¾ã—ãŸ")

    def print_multi_item_summary(self, all_metrics: Dict[str, Dict[str, Dict[str, float]]]):
        """20é …ç›®è©•ä¾¡çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        print("\n" + "="*80)
        print("ğŸ“Š 20é …ç›®è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼ ğŸ“Š")
        print("="*80)
        
        for model_id, metrics_per_item in all_metrics.items():
            print(f"\nğŸ¤– ãƒ¢ãƒ‡ãƒ«: {model_id}")
            print("-" * 60)
            
            # å„é …ç›®ã®çµæœã‚’è¡¨ç¤º
            valid_metrics = {k: v for k, v in metrics_per_item.items() if not np.isnan(v["mae"])}
            
            if not valid_metrics:
                print("âŒ æœ‰åŠ¹ãªè©•ä¾¡çµæœãŒã‚ã‚Šã¾ã›ã‚“")
                continue
            
            # å¹³å‡å€¤ã‚’è¨ˆç®—
            avg_mae = np.mean([m["mae"] for m in valid_metrics.values()])
            avg_rmse = np.mean([m["rmse"] for m in valid_metrics.values()])
            avg_accuracy = np.mean([m["accuracy_within_1"] for m in valid_metrics.values()])
            
            print(f"ğŸ“ˆ å…¨ä½“å¹³å‡:")
            print(f"   MAE (å¹³å‡çµ¶å¯¾èª¤å·®): {avg_mae:.3f}")
            print(f"   RMSE (äºŒä¹—å¹³å‡å¹³æ–¹æ ¹èª¤å·®): {avg_rmse:.3f}")
            print(f"   èª¤å·®1ã§ã®æ­£è§£ç‡: {avg_accuracy:.1f}%")
            print(f"   æœ‰åŠ¹é …ç›®æ•°: {len(valid_metrics)}/{len(EVALUATION_ITEMS)}")
            
            # æœ€è‰¯ãƒ»æœ€æ‚ªé …ç›®
            sorted_by_mae = sorted(valid_metrics.items(), key=lambda x: x[1]["mae"])
            
            print(f"\nğŸ† MAEæœ€è‰¯é …ç›® TOP3:")
            for item, metrics in sorted_by_mae[:3]:
                print(f"   {metrics['mae']:.3f} - {item}")
            
            print(f"\nâš ï¸ MAEæ”¹å–„é …ç›® TOP3:")
            for item, metrics in sorted_by_mae[-3:]:
                print(f"   {metrics['mae']:.3f} - {item}")
        
        print("\n" + "="*80)


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="20é …ç›®è©•ä¾¡ç²¾åº¦è¨ˆç®—ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆKokorochatãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰")
    parser.add_argument("--max-samples", type=int, default=1500, help="Kokorochatã‹ã‚‰æŠ½å‡ºã™ã‚‹æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1500ï¼‰")
    parser.add_argument("--max-test-samples", type=int, help="è©•ä¾¡ã«ä½¿ç”¨ã™ã‚‹ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«æ•°ã®ä¸Šé™")
    parser.add_argument("--seed", type=int, default=42, help="ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 42ï¼‰")
    parser.add_argument("--use-batch-results", action="store_true", help="ãƒãƒƒãƒçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Kokorochatãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥ä½¿ç”¨ï¼‰")
    parser.add_argument("--debug", action="store_true", help="ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰")
    args = parser.parse_args()
    
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
    
    try:
        project_root = Path(__file__).resolve().parent.parent.parent.parent
        env_path = project_root / ".env"
        if env_path.exists():
            load_dotenv(env_path)
            logger.info(f".envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {env_path}")
        
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OpenAI APIã‚­ãƒ¼ãŒ.envãƒ•ã‚¡ã‚¤ãƒ«ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        
        print("ğŸš€ 20é …ç›®è©•ä¾¡ç²¾åº¦è¨ˆç®—ã‚’é–‹å§‹ã—ã¾ã™")
        print(f"ğŸ“Š Kokorochatã‚µãƒ³ãƒ—ãƒ«æ•°: {args.max_samples}")
        print(f"ğŸ² ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰: {args.seed}")
        print(f"ğŸ“‹ è©•ä¾¡é …ç›®æ•°: {len(EVALUATION_ITEMS)}")
        if args.max_test_samples:
            print(f"ğŸ“ˆ ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«ä¸Šé™: {args.max_test_samples}")
        
        evaluator = MultiItemModelAccuracyEvaluator(api_key)
        evaluator.evaluate_all_models(
            max_test_samples=args.max_test_samples,
            use_kokorochat=not args.use_batch_results,
            max_samples=args.max_samples,
            seed=args.seed
        )
        
        print(f"\nâœ… 20é …ç›®è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("ğŸ“ çµæœãƒ•ã‚¡ã‚¤ãƒ«ã¯ä»¥ä¸‹ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ:")
        print(f"   - è©³ç´°çµæœ: multi_item_detailed_results_*.json")
        print(f"   - ç²¾åº¦æŒ‡æ¨™: multi_item_metrics_*.csv")

    except Exception as e:
        logger.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)


if __name__ == "__main__":
    main()
